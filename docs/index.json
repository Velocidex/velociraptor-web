[
{
        "uri": "/about/",
        "title": "About Velociraptor",
        "tags": [],
        "description": "Velociraptor's features, roadmap, license and motivation.",
        "content": " So what is Velociraptor? Velociraptor is a unique, advanced open-source endpoint monitoring, digital forensic and cyber response platform.\nIt was originally developed by DFIR professionals who needed a powerful and efficient way to hunt and monitor activities across fleets of endpoints for specific artefacts, in a wide range of digital forensic and cyber incident response investigations such as:\n Responding to data breaches Reconstructing attacker activities through digital forensic analysis Hunting for evidence of sophisticated adversaries Investigating malware outbreaks and other suspicious network activities Continual monitoring for suspicious user activities, such as files copies to USB devices Disclosure of confidential information outside the network Gathering endpoint data over time, for use in threat hunting and future investigations.  Velociraptor is actively being used by DFIR professionals across cases such as these and continues to grow and develop based on their feedback and ideas.\nVQL - the Velociraptor difference The most powerful feature of Velociraptor is its framework for creating highly customized artifacts which allow a user to collect, query and monitor almost any aspect of a single endpoint, groups of endpoints or an entire network.\nFor technical details on how artefacts work, check out the VQL Reference documentation.\nExample - collecting user activities from a single endpoint Here\u0026rsquo;s a simple example. Below is a VQL artefact named Windows.Registry.NTUser.Upload which is part of Velociraptor\u0026rsquo;s default artefact collection.\nThis artefact first lists all users (using another artefact named Artifact.Windows.Sys.Users) then for each user, collects their NTUSER.DAT registry hive from the endpoint, using raw NTFS access to bypass Windows file system access controls (using the upload function).\n1 LET users = SELECT Name, Directory as HomeDir 2 FROM Artifact.Windows.Sys.Users() 3 WHERE Directory 4 SELECT upload(file=\u0026quot;\\\\\\\\.\\\\\u0026quot; + HomeDir + \u0026quot;\\\\ntuser.dat\u0026quot;, 5 accessor=\u0026quot;ntfs\u0026quot;) as Upload 6 FROM users  All these artifacts and functions are documented on this site. Simply search or browse the menu to the left.\nExample - collecting ALL user Registry hives Now to extend your reach. The very same VQL artefact can be run as a hunt across multiple endpoints, to simultaneously collect all user hives across your network in one sweep.\nAll connected endpoints will immediately receive the query and carry out your request. Any endpoints not currently connected will receive the command as soon as they reconnect to the Velociraptor server. No need for repeating the hunt or scheduling multiple hunts - Velociraptor will take care of the job.\nOur design goals The design goals of Velociraptor that we\u0026rsquo;re working towards, are to be:\n Useful - each artefact and use case must return valuable information to the user Simple - the design and interface must be easy for a person to navigate and use Guided - users don\u0026rsquo;t need to be DFIR experts, since all elements should provide informative descriptions and guidance Powerful - the user should not have to perform too much additional work to achieve their objectives Quick - performance should be speedy and resource impact low, while allowing performance to be managed when needed Reliable - each feature and artefact should work as expected and be relatively free of bugs and issues  We\u0026rsquo;re still a work in progress Although Velociraptor is already being used on real-life DFIR cases, it\u0026rsquo;s still early days and is very much a work in progress.\nOur roadmap includes many exciting features and developments, including:\n Expanding the artefact library, including individual artefacts and \u0026lsquo;artefact packs\u0026rsquo; for even more powerful collection and analysis More artefact parsers to allow for analysis of artefacts and data reuse and cross-referencing directly on the server More monitoring artefacts, for real-time event detection and alerting Artefacts for OSX and Linux, since we already have clients for these Further documentation, especially within artefacts in the GUI, so users don\u0026rsquo;t have to be DFIR experts A kernel driver for Windows, providing tighter integration with operating system event monitoring Improving the user interface, including richer features and more automated reporting  Send us your feedback We welcome all ideas and suggestions on how Velociraptor could be used and improved and encourage our users to get in touch.\n"
},
{
        "uri": "/docs/getting-started/",
        "title": "Getting Started",
        "tags": [],
        "description": "",
        "content": " Quick start Just want the latest version? Sure, it\u0026rsquo;s freely available on our Github releases page.\nWe also recommend you review our Getting Started documentation, which guides you through setup and running your first hunts.\nOther setup options Velociraptor can be used in many different contexts and deployed in different ways.\n Stand alone - Most people want to start with a single client / server and check out the GUI. This installation method is covered under Standalone Deployment.\n Cloud deployment - If you deploy the server in a cloud platform such as AWS, it\u0026rsquo;s possible to use automatically minted SSL certificates and single sign-on for authentication. This is covered under Cloud Deployment.\n Triage mode - Velociraptor can also be used to simply collect artefacts from multiple endpoints simultaneously, for later analysis and preservation, as described under Triaging\n  "
},
{
        "uri": "/blog/html/2018/08/10/browsing_around_the_filesystem.html",
        "title": "Browsing around the filesystem.",
        "tags": [],
        "description": "Browsing the client's filesystem is probably the first thing\nresponders do. Both GRR and Velociraptor have a nice VFS abstraction\nthat allows users to browse files interactively. However, in order to\nmake Velociraptor much faster we made some tradeoffs and improved the\nway that the VFS is stored in the datastore.\n",
        "content": "The Virtual File System Like GRR, Velociraptor also maintains a virtual file system view (VFS) of the client's filesystem. GRR's VFS view is generated by adding a row for each file into the database. In order to refresh the view of a certain directory, GRR issues a ListDirectory request and updates the database by storing each newly discovered file in its own row.\nVelociraptor models the client's VFS as a per-directory VQL query. In order to refresh the view of a certain directory, a new VQL query is issued to the client, essentially collecting the glob information for that directory in a single VQL response table. The VQL result is then stored in a single database row. Therefore Velociraptor stores a single row per directory (as compared to GRR's single row per file approach). This leads to a huge reduction in database rows.\nThe tradeoff however, is that the Velociraptor VFS view can only show the state of the entire directory listing at a single point in time. GRR's VFS viewer can show old files (which have been removed) mixed in with current files because it can merge the output of different ListDirectory operations that occurred in different times. We decided this feature was not often useful and sometimes actually led to confusion since files that are removed from a directory are shown together with files currently present. Velociraptor therefore shows the VFS directory at the latest timestamp the entire directory was fetched.\n Recursive VFS refresh Users who are more familiar with traditional forensic tools (or GUI file managers like Windows Explorer) usually attempt to browse the client's VFS view interactively, searching for files and directories relevant to the case. However, since the VFS view is only a cached database view of the real client's file system, we need to go to the client to refresh the cache whenever we try to view a directory in the VFS which had not yet been fetched from the client.\nSince clients are not always online, some users attempt to just recursively refresh the entire VFS view (i.e. recursively list all client directories from the root). This is however, an expensive operation (This is at least as expensive as running a recursive \u0026quot;find / -ls\u0026quot; command on the commandline). Due to GRR's extensive data model and complex multi-round trip flow model, performing a recursive VFS refresh with GRR is unlikely to work in any reasonable time (typically the flow will run for a while then hang due to race conditions in the frontend).\nOn the other hand, Velociraptor issues a single VQL request as a recursive directory glob and stores the entire directory content in a single VQL response taken at an instance in time. The response is streamed back to the server. The server simply splits the response table into directory specific tables, and then stores a single VQL response table for each directory in the database.\nNote\nThe VQL glob() plugin is guaranteed to generate results in breadth first order. This means that it emits information about all files in the same directory first, before recursing into sub directories. This feature makes it simple to split the result table into directory specific sub-tables by simply watching the FullPath column and noting when its directory changes.\n  Very large VQL queries While we claimed above that Velociraptor simply issues a single VQL query and stores its result in a single database row, this was an oversimplification. If the VQL query generates too many rows, the Velociraptor client splits the response into parts (by default 10000 rows per part). This allows data to be uploaded immediately to the server and processed while the query is still executing on the client.\nConsider the VFSListDirectoryflow was issued with a glob of /**10 (i.e. refresh the entire VFS view from the root directory, recursively into a depth of 10 directories). The VQL query executed was:\nSELECT FullPath AS _FullPath, Name, Size, Mode, timestamp(epoch=Sys.Mtim.Sec) AS mtime, timestamp(epoch=Sys.Atim.Sec) AS atime, timestamp(epoch=Sys.Ctim.Sec) AS ctime FROM glob(globs='/**10')  The query was issued to a Velociraptor client running on a Chromebook. This particular system has approximately 500k files in its root filesystem, and so the response consists of 500k rows. However, as the query executes, the response is split into multiple parts, each being 10k rows, and uploaded (each part is about 3mb in total).\nTotal execution time for this query is about 4 minutes and consists of about 50 parts (around 2.5mb each). It is still an expensive query, but depending on the urgency of the case, it may well be warranted.\nIt is very convenient to just take a snapshot of the entire filesystem, especially when the client is offline. We can issue the flow and then when the client comes back online we can review all the files.\n File uploads The VFS view is just a local cache in the data store of what is really going on the client. While we can see the file in each directory we cant transfer all the file content. Velociraptor represents downloaded files differently from just listed files. Files with the floppy disk next to them represent files that we have a local cache for. We can view the Hexview or just download them.\nYou can always initiate a download of a VFS file by selecting the Download tab. Unlike GRR, Velociraptor does not keep previous versions of files - a re-download will overwrite the previous file.\n "
},
{
        "uri": "/docs/",
        "title": "Velociraptor Documentation",
        "tags": [],
        "description": "",
        "content": "Welcome to Velociraptor\u0026rsquo;s documentation.\nIf you\u0026rsquo;re new to Velociraptor, you should start by reading the Getting Started guide, which will walk you through setting up a small test deployment.\nThe User Interface guide is also worth reviewing.\nWhen you\u0026rsquo;re ready to see what Velociraptor can really do for you:\n Run some artefacts across individual endpoints Build them out into hunts Start customising the artefacts to your specific needs.  You\u0026rsquo;ll be surprised how powerful and easy it can be.\n"
},
{
        "uri": "/docs/user-interface/artifacts/client_artifacts/",
        "title": "Client Artifacts",
        "tags": [],
        "description": "",
        "content": " Now that we understand what artifacts actually are, we are ready to collect artifacts from our endpoints. We will first discuss how to collect an artifact from a single endpoint, and later discuss how to hunt for the artifacts across the entire fleet.\nOnce we searched and selected the endpoint of interest, we can switch to the \u0026ldquo;Collected Artifacts\u0026rdquo; view.\nThis page shows the artifacts previously collected on this endpoint. The page is split into a top table showing a list of collected artifacts, and a bottom overview pane showing details about each selected artifact in the table above.\nThe artifacts list table shows an overview of previously collected artifacts on this endpoint. Each artifact collection operation is termed a flow in Velociraptor. It has the following columns:\n The state of the flow. This can be a tick for completed flows, a clock for pending artifacts or a nuke for artifacts which were collected with critical errors (Artifacts may also have non critical errors so you need to check the logs as well).\n The FlowId is a unique internal ID given to each flow Velociraptor runs. You will need this ID if you need to compose VQL queries for post processing the flow.\n The Artifacts Collected column is a list of artifacts collected by this flow. It is possible to schedule multiple artifacts to be collected at the same time. This column shows each artifact by name.\n The Creation Date is when the artifact was created. The endpoint may not have been online when we issued the collection request, so it is possible that the endpoint actually collected the artifact at a later time.\n The Last Active date is when the last response arrived from the endpoint relating to this flow. The difference between this time and the creation time gives us an idea of how long the artifacts actually took to be collected on the endpoint.\n Finally we learn the user that created the collection. If this column contains a hunt id (of the form H.XXXX) then this flow was automatically created by the hunt manager.\n  The artifact details pane The bottom pane allows us to inspect the flow and the collected artifacts. It consists of several tabs, the first of which Artifact Collection tab gives high level overview of the flow. We can see how many files were uploaded, what artifacts were collected and any specific artifact parameters that were issued.\nIn particular that tab also offers a Download Results button. Clicking this button will create a zip file containing all relevant information obtained from this artifact. Specifically it contains a CSV file for each returned VQL query, as well as any files uploaded by the artifact.\nThere is no way to know currently how large the exported zip file will be, as the Zip file is built on the fly. Therefore there is no progress bar either as the file is downloaded.\n The Uploaded Files tab shows the files uploaded by the artifact\u0026rsquo;s VQL. Some artifact are simply file collectors - collecting a bunch of files for later post processing analysis.\nThe Results tab shows a table of VQL results from each source. For artifacts containing multiple sources (or if you collected multiple artifacts) the selector allows switching between them to view the result table from each. Since an artifact source is simply a VQL query, it returns a single table with columns specified by the query itself. Therefore each artifact source will produce a different table.\nThe Logs tab shows any messages logged by the endpoint while collecting the artifact. Many issues encountered by the endpoint are not considered fatal, but are nevertheless logged to the server (for example, if the endpoint attempts to open a file which is locked). You should look at the tab to assess if you are getting a complete result.\nFinally the Reports tab shows the artifact\u0026rsquo;s report. As described above, the report is a human readable document explaining the results of the artifact and performing some post processing.\nCollecting an artifact from an endpoint. We have seen how to examine older artifacts collected from the endpoint, how do we collect newer artifacts?\nClicking the plus button on the toolbar (Collect More Artifacts) presents the artifacts collection UI.\nThe UI element presents a search box for finding the desired artifacts. Velociraptor will search for the keywords in the artifact\u0026rsquo;s description field. A list of matching artifact names is presented below the search box. Clicking on each of these artifacts presents a summary of the artifact on the right hand side. The summary includes the description as well as the parameters the artifact takes and the VQL queries that will be run. The artifact can now be added to the selected artifacts box.\nControlling endpoint performance Some artifacts have to perform many operations on the endpoint. For example, image if we were looking for a particular file matching a given hash. We would have to hash all files on the endpoint and then compare the has to the required hash. This may negatively impact on the endpoint performance.\nVelociraptor offers two settings to allow you to decide how much impact you want to have on the endpoint.\nYou can set the Ops/Sec value for collecting the artifacts. This setting controls how aggressively the endpoint will collect the artifact. For artifacts that collect a lot of files or otherwise utilize heavy resources on the endpoint, it is advisable to lower this to reduce endpoint load. An operation is considered a single row or 1mb scanned, so setting this value to 20 Ops/sec would result in a yara scan of approximately 20Mb/s. Values between 1 and 50 should be set here.\nThe Maximum Time setting controls how long Velociraptor will allow the artifact query to run. Artifacts that take longer than this time (by default 10 minutes) will be canceled. Velociraptor uses this timeout to ensure artifacts do not run out of control on the endpoint. If your artifact requires examining many files, or you have lowered Ops/Sec to reduce its impact on the endpoint (but this will make it take longer), you should increase this value.\nThe File Finder Artifact Artifacts can take parameters. When you add an artifact to the \u0026ldquo;Selected Artifact\u0026rdquo; UI the Artifact\u0026rsquo;s parameters also appear in the UI for you to adjust. The parameters are should be properly filled with reasonable defaults already so you only need to adjust them sometimes.\nOne of the most useful artifacts is the File Finder:\nThe File Finder allows you to search for files in the endpoint using a number of criteria:\n The Glob is a search pattern of files\n If you want to search the contents of files you can specific a number of keywords here.\n Use Raw NTFS specifies that raw NTFS parsing should be used - this is important for locked files like registry hives.\n Upload Files specifies that matching files will be uploaded to the server\n Calculate Hash specifies that hashes will be calculated of matching files.\n It is also possible to set date ranges for file matches.\n  Inspecting the results of an artifact When an artifact runs on the endpoint, Velociraptor\u0026rsquo;s VQL engine may log messages about problems it encounters along the way.\nFor example in the above screenshot we see that some files were not opened by the query because they were locked, but the query uploaded 216 rows and 205 files to the server.\nYou should inspect the log output to ensure that the artifact is collected without errors. Although the artifact collection in this case was mostly successful (it fetched most of the browser artifacts), a few files were not able to be retrieved. This may or may not be significant to your case.\n"
},
{
        "uri": "/docs/user-interface/investigating_clients/",
        "title": "Investigating Clients",
        "tags": [],
        "description": "",
        "content": "Imagine you have received an alert about a potential suspicious activity on a particular endpoint. One of the first things you would want to do is to interactively examine the endpoint.\nVelociraptor allows you to do just that! Simply search for the client, in the GUI, and navigate its file system remotely. You can download remote files, read registry keys remotely and look at evidence stored in Volume Shadow Copies. All this in a natural, intuitive interface.\nWe will also demonstrate the FUSE interface. Velociraptor allows you to mount a remote system\u0026rsquo;s Virtual File System locally and then access the end point\u0026rsquo;s files transparently using the analysis tool of your choice.\n"
},
{
        "uri": "/docs/artifacts/linux/",
        "title": "Linux Artifacts",
        "tags": [],
        "description": "Linux Artifacts",
        "content": " Linux.Applications.Chrome.Extensions Fetch Chrome extensions.\nChrome extensions are installed into the user\u0026rsquo;s home directory. We search for manifest.json files in a known path within each system user\u0026rsquo;s home directory. We then parse the manifest file as JSON.\nMany extensions use locale packs to resolve strings like name and description. In this case we detect the default locale and load those locale files. We then resolve the extension\u0026rsquo;s name and description from there.\n   Arg Default Description     extensionGlobs /.config/google-chrome//Extensions//*/manifest.json       View Artifact Source   name: Linux.Applications.Chrome.Extensions description: | Fetch Chrome extensions. Chrome extensions are installed into the user's home directory. We search for manifest.json files in a known path within each system user's home directory. We then parse the manifest file as JSON. Many extensions use locale packs to resolve strings like name and description. In this case we detect the default locale and load those locale files. We then resolve the extension's name and description from there. parameters: - name: extensionGlobs default: /.config/google-chrome/*/Extensions/*/*/manifest.json sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | /* For each user on the system, search for extension manifests in their home directory. */ LET extension_manifests = SELECT * from foreach( row={ SELECT Uid, User, Homedir from Artifact.Linux.Sys.Users() }, query={ SELECT FullPath, Mtime, Ctime, User, Uid from glob( globs=Homedir + '/' + extensionGlobs) }) - | /* If the Manifest declares a default_locale then we load and parse the messages file. In this case the messages are actually stored in the locale file instead of the main manifest.json file. */ LET maybe_read_locale_file = SELECT * from if( condition={ select * from scope() where Manifest.default_locale }, then={ SELECT Manifest, Uid, User, Filename as LocaleFilename, ManifestFilename, parse_json(data=Data) AS LocaleManifest FROM read_file( -- Munge the filename to get the messages.json path. filenames=regex_replace( source=ManifestFilename, replace=\u0026quot;/_locales/\u0026quot; + Manifest.default_locale + \u0026quot;/messages.json\u0026quot;, re=\u0026quot;/manifest.json$\u0026quot;)) }, else={ -- Just fill in empty Locale results. SELECT Manifest, Uid, User, \u0026quot;\u0026quot; AS LocaleFilename, \u0026quot;\u0026quot; AS ManifestFilename, \u0026quot;\u0026quot; AS LocaleManifest FROM scope() }) - | LET parse_json_files = SELECT * from foreach( row={ SELECT Filename as ManifestFilename, Uid, User, parse_json(data=Data) as Manifest FROM read_file(filenames=FullPath) }, query=maybe_read_locale_file) - | LET parsed_manifest_files = SELECT * from foreach( row=extension_manifests, query=parse_json_files) - | SELECT Uid, User, /* If the manifest name contains __MSG_ then the real name is stored in the locale manifest. This condition resolves the Name column either to the main manifest or the locale manifest. */ if(condition=\u0026quot;__MSG_\u0026quot; in Manifest.name, then=get(item=LocaleManifest, member=regex_replace( source=Manifest.name, replace=\u0026quot;$1\u0026quot;, re=\u0026quot;(?:__MSG_(.+)__)\u0026quot;)).message, else=Manifest.name) as Name, if(condition=\u0026quot;__MSG_\u0026quot; in Manifest.description, then=get(item=LocaleManifest, member=regex_replace( source=Manifest.description, replace=\u0026quot;$1\u0026quot;, re=\u0026quot;(?:__MSG_(.+)__)\u0026quot;)).message, else=Manifest.description) as Description, /* Get the Identifier and Version from the manifest filename */ regex_replace( source=ManifestFilename, replace=\u0026quot;$1\u0026quot;, re=\u0026quot;(?:.+Extensions/([^/]+)/([^/]+)/manifest.json)$\u0026quot;) AS Identifier, regex_replace( source=ManifestFilename, replace=\u0026quot;$2\u0026quot;, re=\u0026quot;(?:.+Extensions/([^/]+)/([^/]+)/manifest.json)$\u0026quot;) AS Version, Manifest.author as Author, Manifest.background.persistent AS Persistent, regex_replace( source=ManifestFilename, replace=\u0026quot;$1\u0026quot;, re=\u0026quot;(.+Extensions/.+/)manifest.json$\u0026quot;) AS Path, Manifest.oauth2.scopes as Scopes, Manifest.permissions as Permissions, Manifest.key as Key FROM parsed_manifest_files    Linux.Applications.Chrome.Extensions.Upload Upload all users chrome extension.\nWe dont bother actually parsing anything here, we just grab all the extension files in user\u0026rsquo;s home directory.\n   Arg Default Description     extensionGlobs /.config/google-chrome/*/Extensions/**       View Artifact Source   name: Linux.Applications.Chrome.Extensions.Upload description: | Upload all users chrome extension. We dont bother actually parsing anything here, we just grab all the extension files in user's home directory. parameters: - name: extensionGlobs default: /.config/google-chrome/*/Extensions/** sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | /* For each user on the system, search for extension files in their home directory and upload them. */ SELECT * from foreach( row={ SELECT Uid, User, Homedir from Artifact.Linux.Sys.Users() }, query={ SELECT FullPath, Mtime, Ctime, User, Uid, upload(file=FullPath) as Upload FROM glob(globs=Homedir + '/' + extensionGlobs) })    Linux.Applications.Docker.Info Get Dockers info by connecting to its socket.\n   Arg Default Description     dockerSocket /var/run/docker.sock Docker server socket. You will normally need to be root to connect.\\n      View Artifact Source   name: Linux.Applications.Docker.Info description: Get Dockers info by connecting to its socket. parameters: - name: dockerSocket description: | Docker server socket. You will normally need to be root to connect. default: /var/run/docker.sock sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | LET data = SELECT parse_json(data=Content) as JSON FROM http_client(url=dockerSocket + \u0026quot;:unix/info\u0026quot;) - | SELECT JSON.ID as ID, JSON.Containers as Containers, JSON.ContainersRunning as ContainersRunning, JSON.ContainersPaused as ContainersPaused, JSON.ContainersStopped as ContainersStopped, JSON.Images as Images, JSON.Driver as Driver, JSON.MemoryLimit as MemoryLimit, JSON.SwapLimit as SwapLimit, JSON.KernelMemory as KernelMemory, JSON.CpuCfsPeriod as CpuCfsPeriod, JSON.CpuCfsQuota as CpuCfsQuota, JSON.CPUShares as CPUShares, JSON.CPUSet as CPUSet, JSON.IPv4Forwarding as IPv4Forwarding, JSON.BridgeNfIptables as BridgeNfIptables, JSON.BridgeNfIp6tables as BridgeNfIp6tables, JSON.OomKillDisable as OomKillDisable, JSON.LoggingDriver as LoggingDriver, JSON.CgroupDriver as CgroupDriver, JSON.KernelVersion as KernelVersion, JSON.OperatingSystem as OperatingSystem, JSON.OSType as OSType, JSON.Architecture as Architecture, JSON.NCPU as NCPU, JSON.MemTotal as MemTotal, JSON.HttpProxy as HttpProxy, JSON.HttpsProxy as HttpsProxy, JSON.NoProxy as NoProxy, JSON.Name as Name, JSON.ServerVersion as ServerVersion, JSON.DockerRootDir as DockerRootDir FROM data    Linux.Applications.Docker.Version Get Dockers version by connecting to its socket.\n   Arg Default Description     dockerSocket /var/run/docker.sock Docker server socket. You will normally need to be root to connect.\\n      View Artifact Source   name: Linux.Applications.Docker.Version description: Get Dockers version by connecting to its socket. parameters: - name: dockerSocket description: | Docker server socket. You will normally need to be root to connect. default: /var/run/docker.sock sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | LET data = SELECT parse_json(data=Content) as JSON FROM http_client(url=dockerSocket + \u0026quot;:unix/version\u0026quot;) - | SELECT JSON.Version as Version, JSON.ApiVersion as ApiVersion, JSON.MinAPIVersion as MinAPIVersion, JSON.GitCommit as GitCommit, JSON.GoVersion as GoVersion, JSON.Os as Os, JSON.Arch as Arch, JSON.KernelVersion as KernelVersion, JSON.BuildTime as BuildTime FROM data    Linux.Debian.AptSources Parse Debian apt sources.\nWe first search for *.list files which contain lines of the form\n.. code:: console\ndeb http://us.archive.ubuntu.com/ubuntu/ bionic main restricted\nFor each line we construct the cache file by spliting off the section (last component) and replacing / and \u0026ldquo; \u0026rdquo; with _.\nWe then try to open the file. If the file exists we parse some metadata from it. If not we leave those columns empty.\n   Arg Default Description     linuxAptSourcesGlobs /etc/apt/sources.list,/etc/apt/sources.list.d/*.list Globs to find apt source *.list files.   aptCacheDirectory /var/lib/apt/lists/ Location of the apt cache directory.      View Artifact Source   name: Linux.Debian.AptSources description: | Parse Debian apt sources. We first search for \\*.list files which contain lines of the form .. code:: console deb http://us.archive.ubuntu.com/ubuntu/ bionic main restricted For each line we construct the cache file by spliting off the section (last component) and replacing / and \u0026quot; \u0026quot; with _. We then try to open the file. If the file exists we parse some metadata from it. If not we leave those columns empty. reference: - https://osquery.io/schema/3.2.6#apt_sources parameters: - name: linuxAptSourcesGlobs description: Globs to find apt source *.list files. default: /etc/apt/sources.list,/etc/apt/sources.list.d/*.list - name: aptCacheDirectory description: Location of the apt cache directory. default: /var/lib/apt/lists/ sources: - precondition: SELECT OS From info() where OS = 'linux' queries: - | /* Search for files which may contain apt sources. The user can pass new globs here. */ LET files = SELECT FullPath from glob( globs=split(string=linuxAptSourcesGlobs, sep=\u0026quot;,\u0026quot;)) - | /* Read each line in the sources which is not commented. Deb lines look like: deb [arch=amd64] http://dl.google.com/linux/chrome-remote-desktop/deb/ stable main Contains URL, base_uri and components. */ LET deb_sources = SELECT * FROM parse_records_with_regex( file=files.FullPath, regex=\u0026quot;(?m)^ *(?P\u0026lt;Type\u0026gt;deb(-src)?) (?:\\\\[arch=(?P\u0026lt;Arch\u0026gt;[^\\\\]]+)\\\\] )?\u0026quot; + \u0026quot;(?P\u0026lt;URL\u0026gt;https?://(?P\u0026lt;base_uri\u0026gt;[^ ]+))\u0026quot; + \u0026quot; +(?P\u0026lt;components\u0026gt;.+)\u0026quot;) - | /* We try to get at the Release file in /var/lib/apt/ by munging the components and URL. Strip the last component off, convert / and space to _ and add _Release to get the filename. */ LET parsed_apt_lines = SELECT Arch, URL, base_uri + \u0026quot; \u0026quot; + components as Name, Type, FullPath as Source, aptCacheDirectory + regex_replace( replace=\u0026quot;_\u0026quot;, re=\u0026quot;_+\u0026quot;, source=regex_replace( replace=\u0026quot;_\u0026quot;, re=\u0026quot;[ /]\u0026quot;, source=base_uri + \u0026quot;_dists_\u0026quot; + regex_replace( source=components, replace=\u0026quot;\u0026quot;, re=\u0026quot; +[^ ]+$\u0026quot;)) + \u0026quot;_Release\u0026quot; ) as cache_file FROM deb_sources - | /* This runs if the file was found. Read the entire file into memory and parse the same record using multiple RegExps. */ LET parsed_cache_files = SELECT Name, Arch, URL, Type, Source, parse_string_with_regex( string=Record, regex=[\u0026quot;Codename: (?P\u0026lt;Release\u0026gt;[^\\\\s]+)\u0026quot;, \u0026quot;Version: (?P\u0026lt;Version\u0026gt;[^\\\\s]+)\u0026quot;, \u0026quot;Origin: (?P\u0026lt;Maintainer\u0026gt;[^\\\\s]+)\u0026quot;, \u0026quot;Architectures: (?P\u0026lt;Architectures\u0026gt;[^\\\\s]+)\u0026quot;, \u0026quot;Components: (?P\u0026lt;Components\u0026gt;[^\\\\s]+)\u0026quot;]) as Record FROM parse_records_with_regex(file=cache_file, regex=\u0026quot;(?sm)(?P\u0026lt;Record\u0026gt;.+)\u0026quot;) - | // Foreach row in the parsed cache file, collect the FileInfo too. LET add_stat_to_parsed_cache_file = SELECT * from foreach( query={ SELECT FullPath, Mtime, Ctime, Atime, Record, Type, Name, Arch, URL, Source from stat(filename=cache_file) }, row=parsed_cache_files) - | /* For each row in the parsed file, run the appropriate query depending on if the cache file exists. If the cache file is not found, we just copy the lines we parsed from the source file and fill in empty values for stat. */ LET parse_cache_or_pass = SELECT * from if( condition={ SELECT * from stat(filename=cache_file) }, then=add_stat_to_parsed_cache_file, else={ SELECT Source, Null as Mtime, Null as Ctime, Null as Atime, Type, Null as Record, Arch, URL, Name from scope() }) - | -- For each parsed apt .list file line produce some output. SELECT * from foreach( row=parsed_apt_lines, query=parse_cache_or_pass)    Linux.Debian.Packages Parse dpkg status file.\n   Arg Default Description     linuxDpkgStatus /var/lib/dpkg/status       View Artifact Source   name: Linux.Debian.Packages description: Parse dpkg status file. parameters: - name: linuxDpkgStatus default: /var/lib/dpkg/status sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | /* First pass - split file into records start with Package and end with \\n\\n. Then parse each record using multiple RegExs. */ LET packages = SELECT parse_string_with_regex( string=Record, regex=['Package:\\\\s(?P\u0026lt;Package\u0026gt;.+)', 'Installed-Size:\\\\s(?P\u0026lt;InstalledSize\u0026gt;.+)', 'Version:\\\\s(?P\u0026lt;Version\u0026gt;.+)', 'Source:\\\\s(?P\u0026lt;Source\u0026gt;.+)', 'Architecture:\\\\s(?P\u0026lt;Architecture\u0026gt;.+)']) as Record FROM parse_records_with_regex( file=linuxDpkgStatus, regex='(?sm)^(?P\u0026lt;Record\u0026gt;Package:.+?)\\\\n\\\\n') - | SELECT Record.Package as Package, atoi(string=Record.InstalledSize) as InstalledSize, Record.Version as Version, Record.Source as Source, Record.Architecture as Architecture from packages    Linux.Mounts List mounted filesystems by reading /proc/mounts\n   Arg Default Description     ProcMounts /proc/mounts       View Artifact Source   name: Linux.Mounts description: List mounted filesystems by reading /proc/mounts parameters: - name: ProcMounts default: /proc/mounts sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | SELECT Device, Mount, FSType, split(string=Opts, sep=\u0026quot;,\u0026quot;) As Options FROM parse_records_with_regex( file=ProcMounts, regex='(?m)^(?P\u0026lt;Device\u0026gt;[^ ]+) (?P\u0026lt;Mount\u0026gt;[^ ]+) (?P\u0026lt;FSType\u0026gt;[^ ]+) '+ '(?P\u0026lt;Opts\u0026gt;[^ ]+)')    Linux.Proc.Arp ARP table via /proc/net/arp.\n   Arg Default Description     ProcNetArp /proc/net/arp       View Artifact Source   name: Linux.Proc.Arp description: ARP table via /proc/net/arp. parameters: - name: ProcNetArp default: /proc/net/arp sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | SELECT * from split_records( filenames=ProcNetArp, regex='\\\\s{3,20}', first_row_is_headers=true)    Linux.Proc.Modules Module listing via /proc/modules.\n   Arg Default Description     ProcModules /proc/modules       View Artifact Source   name: Linux.Proc.Modules description: Module listing via /proc/modules. parameters: - name: ProcModules default: /proc/modules sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | SELECT Name, atoi(string=Size) As Size, atoi(string=UseCount) As UseCount, Status, Address FROM split_records( filenames=ProcModules, regex='\\\\s+', columns=['Name', 'Size', 'UseCount', 'UsedBy', 'Status', 'Address'])    Linux.Ssh.AuthorizedKeys Find and parse ssh authorized keys files.\n   Arg Default Description     sshKeyFiles .ssh/authorized_keys*       View Artifact Source   name: Linux.Ssh.AuthorizedKeys description: Find and parse ssh authorized keys files. parameters: - name: sshKeyFiles default: '.ssh/authorized_keys*' sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | // For each user on the system, search for authorized_keys files. LET authorized_keys = SELECT * from foreach( row={ SELECT Uid, User, Homedir from Artifact.Linux.Sys.Users() }, query={ SELECT FullPath, Mtime, Ctime, User, Uid from glob( globs=Homedir + '/' + sshKeyFiles) }) - | // For each authorized keys file, extract each line on a different row. // Note: This duplicates the path, user and uid on each key line. SELECT * from foreach( row=authorized_keys, query={ SELECT Uid, User, FullPath, Key from split_records( filenames=FullPath, regex=\u0026quot;\\n\u0026quot;, columns=[\u0026quot;Key\u0026quot;]) })    Linux.Ssh.KnownHosts Find and parse ssh known hosts files.\n   Arg Default Description     sshKnownHostsFiles .ssh/known_hosts*       View Artifact Source   name: Linux.Ssh.KnownHosts description: Find and parse ssh known hosts files. parameters: - name: sshKnownHostsFiles default: '.ssh/known_hosts*' sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | // For each user on the system, search for known_hosts files. LET authorized_keys = SELECT * from foreach( row={ SELECT Uid, User, Homedir from Artifact.Linux.Sys.Users() }, query={ SELECT FullPath, Mtime, Ctime, User, Uid from glob( globs=Homedir + '/' + sshKnownHostsFiles) }) - | // For each known_hosts file, extract each line on a different row. SELECT * from foreach( row=authorized_keys, query={ SELECT Uid, User, FullPath, Line from split_records( filenames=FullPath, regex=\u0026quot;\\n\u0026quot;, columns=[\u0026quot;Line\u0026quot;]) /* Ignore comment lines. */ WHERE not Line =~ \u0026quot;^[^#]+#\u0026quot; })    Linux.Sys.ACPITables Firmware ACPI functional table common metadata and content.\n   Arg Default Description     kLinuxACPIPath /sys/firmware/acpi/tables       View Artifact Source   name: Linux.Sys.ACPITables description: Firmware ACPI functional table common metadata and content. reference: - https://osquery.io/schema/3.2.6#acpi_tables parameters: - name: kLinuxACPIPath default: /sys/firmware/acpi/tables sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | LET hashes = SELECT Name, Size, hash(path=FullPath) as Hash FROM glob(globs=kLinuxACPIPath + '/*') - | SELECT Name, Size, Hash.MD5, Hash.SHA1, Hash.SHA256 from hashes    Linux.Sys.CPUTime Displays information from /proc/stat file about the time the cpu cores spent in different parts of the system.\n   Arg Default Description     procStat /proc/stat       View Artifact Source   name: Linux.Sys.CPUTime description: | Displays information from /proc/stat file about the time the cpu cores spent in different parts of the system. parameters: - name: procStat default: /proc/stat sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | LET raw = SELECT * FROM split_records( filenames=procStat, regex=' +', columns=['core', 'user', 'nice', 'system', 'idle', 'iowait', 'irq', 'softirq', 'steal', 'guest', 'guest_nice']) WHERE core =~ 'cpu.+' - | SELECT core AS Core, atoi(string=user) as User, atoi(string=nice) as Nice, atoi(string=system) as System, atoi(string=idle) as Idle, atoi(string=iowait) as IOWait, atoi(string=irq) as IRQ, atoi(string=softirq) as SoftIRQ, atoi(string=steal) as Steal, atoi(string=guest) as Guest, atoi(string=guest_nice) as GuestNice FROM raw    Linux.Sys.Crontab Displays parsed information from crontab.\n   Arg Default Description     cronTabGlob /etc/crontab,/etc/cron.d/,/var/at/tabs/,/var/spool/cron/,/var/spool/cron/crontabs/       View Artifact Source   name: Linux.Sys.Crontab description: | Displays parsed information from crontab. parameters: - name: cronTabGlob default: /etc/crontab,/etc/cron.d/**,/var/at/tabs/**,/var/spool/cron/**,/var/spool/cron/crontabs/** sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | LET raw = SELECT * FROM foreach( row={ SELECT FullPath from glob(globs=split(string=cronTabGlob, sep=\u0026quot;,\u0026quot;)) }, query={ SELECT FullPath, data, parse_string_with_regex( string=data, regex=[ /* Regex for event (Starts with @) */ \u0026quot;^(?P\u0026lt;Event\u0026gt;@[a-zA-Z]+)\\\\s+(?P\u0026lt;Command\u0026gt;.+)\u0026quot;, /* Regex for regular command. */ \u0026quot;^(?P\u0026lt;Minute\u0026gt;[^\\\\s]+)\\\\s+\u0026quot;+ \u0026quot;(?P\u0026lt;Hour\u0026gt;[^\\\\s]+)\\\\s+\u0026quot;+ \u0026quot;(?P\u0026lt;DayOfMonth\u0026gt;[^\\\\s]+)\\\\s+\u0026quot;+ \u0026quot;(?P\u0026lt;Month\u0026gt;[^\\\\s]+)\\\\s+\u0026quot;+ \u0026quot;(?P\u0026lt;DayOfWeek\u0026gt;[^\\\\s]+)\\\\s+\u0026quot;+ \u0026quot;(?P\u0026lt;Command\u0026gt;.+)$\u0026quot;]) as Record /* Read lines from the file and filter ones that start with \u0026quot;#\u0026quot; */ FROM split_records( filenames=FullPath, regex=\u0026quot;\\n\u0026quot;, columns=[\u0026quot;data\u0026quot;]) WHERE not data =~ \u0026quot;^\\\\s*#\u0026quot; }) WHERE Record.Command - | SELECT Record.Event AS Event, Record.Minute AS Minute, Record.Hour AS Hour, Record.DayOfMonth AS DayOfMonth, Record.Month AS Month, Record.DayOfWeek AS DayOfWeek, Record.Command AS Command, FullPath AS Path FROM raw    Linux.Sys.LastUserLogin Find and parse system wtmp files. This indicate when the user last logged in.\n   Arg Default Description     wtmpGlobs /var/log/wtmp*    wtmpProfile {\\n \u0026ldquo;timeval\u0026rdquo;: [8, {\\n \u0026ldquo;tv_sec\u0026rdquo;: [0, [\u0026ldquo;int\u0026rdquo;]],\\n \u0026ldquo;tv_usec\u0026rdquo;: [4, [\u0026ldquo;int\u0026rdquo;]]\\n }],\\n \u0026ldquo;exit_status\u0026rdquo;: [4, {\\n \u0026ldquo;e_exit\u0026rdquo;: [2, [\u0026ldquo;short int\u0026rdquo;]],\\n \u0026ldquo;e_termination\u0026rdquo;: [0, [\u0026ldquo;short int\u0026rdquo;]]\\n }],\\n \u0026ldquo;timezone\u0026rdquo;: [8, {\\n \u0026ldquo;tz_dsttime\u0026rdquo;: [4, [\u0026ldquo;int\u0026rdquo;]],\\n \u0026ldquo;tz_minuteswest\u0026rdquo;: [0, [\u0026ldquo;int\u0026rdquo;]]\\n }],\\n \u0026ldquo;utmp\u0026rdquo;: [384, {\\n \u0026ldquo;__glibc_reserved\u0026rdquo;: [364, [\u0026ldquo;Array\u0026rdquo;, {\\n \u0026ldquo;count\u0026rdquo;: 20,\\n \u0026ldquo;target\u0026rdquo;: \u0026ldquo;char\u0026rdquo;,\\n \u0026ldquo;target_args\u0026rdquo;: null\\n }]],\\n \u0026ldquo;ut_addr_v6\u0026rdquo;: [348, [\u0026ldquo;Array\u0026rdquo;, {\\n \u0026ldquo;count\u0026rdquo;: 4,\\n \u0026ldquo;target\u0026rdquo;: \u0026ldquo;int\u0026rdquo;,\\n \u0026ldquo;target_args\u0026rdquo;: null\\n }]],\\n \u0026ldquo;ut_exit\u0026rdquo;: [332, [\u0026ldquo;exit_status\u0026rdquo;]],\\n \u0026ldquo;ut_host\u0026rdquo;: [76, [\u0026ldquo;String\u0026rdquo;, {\\n \u0026ldquo;length\u0026rdquo;: 256\\n }]],\\n \u0026ldquo;ut_id\u0026rdquo;: [40, [\u0026ldquo;String\u0026rdquo;, {\\n \u0026ldquo;length\u0026rdquo;: 4\\n }]],\\n \u0026ldquo;ut_line\u0026rdquo;: [8, [\u0026ldquo;String\u0026rdquo;, {\\n \u0026ldquo;length\u0026rdquo;: 32\\n }]],\\n \u0026ldquo;ut_pid\u0026rdquo;: [4, [\u0026ldquo;int\u0026rdquo;]],\\n \u0026ldquo;ut_session\u0026rdquo;: [336, [\u0026ldquo;int\u0026rdquo;]],\\n \u0026ldquo;ut_tv\u0026rdquo;: [340, [\u0026ldquo;timeval\u0026rdquo;]],\\n \u0026ldquo;ut_type\u0026rdquo;: [0, [\u0026ldquo;Enumeration\u0026rdquo;, {\\n \u0026ldquo;target\u0026rdquo;: \u0026ldquo;short int\u0026rdquo;,\\n \u0026ldquo;choices\u0026rdquo;: {\\n \u0026ldquo;0\u0026rdquo;: \u0026ldquo;EMPTY\u0026rdquo;,\\n \u0026ldquo;1\u0026rdquo;: \u0026ldquo;RUN_LVL\u0026rdquo;,\\n \u0026ldquo;2\u0026rdquo;: \u0026ldquo;BOOT_TIME\u0026rdquo;,\\n \u0026ldquo;5\u0026rdquo;: \u0026ldquo;INIT_PROCESS\u0026rdquo;,\\n \u0026ldquo;6\u0026rdquo;: \u0026ldquo;LOGIN_PROCESS\u0026rdquo;,\\n \u0026ldquo;7\u0026rdquo;: \u0026ldquo;USER_PROCESS\u0026rdquo;,\\n \u0026ldquo;8\u0026rdquo;: \u0026ldquo;DEAD_PROCESS\u0026rdquo;\\n }\\n }]],\\n \u0026ldquo;ut_user\u0026rdquo;: [44, [\u0026ldquo;String\u0026rdquo;, {\\n \u0026ldquo;length\u0026rdquo;: 32\\n }]]\\n }]\\n}\\n       View Artifact Source   name: Linux.Sys.LastUserLogin description: Find and parse system wtmp files. This indicate when the user last logged in. parameters: - name: wtmpGlobs default: /var/log/wtmp* # This is automatically generated from dwarf symbols by Rekall: # gcc -c -g -o /tmp/test.o /tmp/1.c # rekall dwarfparser /tmp/test.o # And 1.c is: # #include \u0026quot;utmp.h\u0026quot; # struct utmp x; - name: wtmpProfile default: | { \u0026quot;timeval\u0026quot;: [8, { \u0026quot;tv_sec\u0026quot;: [0, [\u0026quot;int\u0026quot;]], \u0026quot;tv_usec\u0026quot;: [4, [\u0026quot;int\u0026quot;]] }], \u0026quot;exit_status\u0026quot;: [4, { \u0026quot;e_exit\u0026quot;: [2, [\u0026quot;short int\u0026quot;]], \u0026quot;e_termination\u0026quot;: [0, [\u0026quot;short int\u0026quot;]] }], \u0026quot;timezone\u0026quot;: [8, { \u0026quot;tz_dsttime\u0026quot;: [4, [\u0026quot;int\u0026quot;]], \u0026quot;tz_minuteswest\u0026quot;: [0, [\u0026quot;int\u0026quot;]] }], \u0026quot;utmp\u0026quot;: [384, { \u0026quot;__glibc_reserved\u0026quot;: [364, [\u0026quot;Array\u0026quot;, { \u0026quot;count\u0026quot;: 20, \u0026quot;target\u0026quot;: \u0026quot;char\u0026quot;, \u0026quot;target_args\u0026quot;: null }]], \u0026quot;ut_addr_v6\u0026quot;: [348, [\u0026quot;Array\u0026quot;, { \u0026quot;count\u0026quot;: 4, \u0026quot;target\u0026quot;: \u0026quot;int\u0026quot;, \u0026quot;target_args\u0026quot;: null }]], \u0026quot;ut_exit\u0026quot;: [332, [\u0026quot;exit_status\u0026quot;]], \u0026quot;ut_host\u0026quot;: [76, [\u0026quot;String\u0026quot;, { \u0026quot;length\u0026quot;: 256 }]], \u0026quot;ut_id\u0026quot;: [40, [\u0026quot;String\u0026quot;, { \u0026quot;length\u0026quot;: 4 }]], \u0026quot;ut_line\u0026quot;: [8, [\u0026quot;String\u0026quot;, { \u0026quot;length\u0026quot;: 32 }]], \u0026quot;ut_pid\u0026quot;: [4, [\u0026quot;int\u0026quot;]], \u0026quot;ut_session\u0026quot;: [336, [\u0026quot;int\u0026quot;]], \u0026quot;ut_tv\u0026quot;: [340, [\u0026quot;timeval\u0026quot;]], \u0026quot;ut_type\u0026quot;: [0, [\u0026quot;Enumeration\u0026quot;, { \u0026quot;target\u0026quot;: \u0026quot;short int\u0026quot;, \u0026quot;choices\u0026quot;: { \u0026quot;0\u0026quot;: \u0026quot;EMPTY\u0026quot;, \u0026quot;1\u0026quot;: \u0026quot;RUN_LVL\u0026quot;, \u0026quot;2\u0026quot;: \u0026quot;BOOT_TIME\u0026quot;, \u0026quot;5\u0026quot;: \u0026quot;INIT_PROCESS\u0026quot;, \u0026quot;6\u0026quot;: \u0026quot;LOGIN_PROCESS\u0026quot;, \u0026quot;7\u0026quot;: \u0026quot;USER_PROCESS\u0026quot;, \u0026quot;8\u0026quot;: \u0026quot;DEAD_PROCESS\u0026quot; } }]], \u0026quot;ut_user\u0026quot;: [44, [\u0026quot;String\u0026quot;, { \u0026quot;length\u0026quot;: 32 }]] }] } sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | SELECT * from foreach( row={ SELECT FullPath from glob(globs=split(string=wtmpGlobs, sep=\u0026quot;,\u0026quot;)) }, query={ SELECT ut_type, ut_id, ut_host.AsString as Host, ut_user.AsString as User, timestamp(epoch=ut_tv.tv_sec.AsInteger) as login_time FROM binary_parse( file=FullPath, profile=wtmpProfile, target=\u0026quot;Array\u0026quot;, args=dict(Target=\u0026quot;utmp\u0026quot;) ) })    Linux.Sys.Users Get User specific information like homedir, group etc from /etc/passwd.\n   Arg Default Description     PasswordFile /etc/passwd The location of the password file.      View Artifact Source   name: Linux.Sys.Users description: Get User specific information like homedir, group etc from /etc/passwd. parameters: - name: PasswordFile default: /etc/passwd description: The location of the password file. sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | SELECT User, Description, Uid, Gid, Homedir, Shell FROM parse_records_with_regex( file=PasswordFile, regex='(?m)^(?P\u0026lt;User\u0026gt;[^:]+):([^:]+):' + '(?P\u0026lt;Uid\u0026gt;[^:]+):(?P\u0026lt;Gid\u0026gt;[^:]+):(?P\u0026lt;Description\u0026gt;[^:]*):' + '(?P\u0026lt;Homedir\u0026gt;[^:]+):(?P\u0026lt;Shell\u0026gt;[^:\\\\s]+)')    "
},
{
        "uri": "/docs/getting-started/stand_alone/",
        "title": "Standalone Deployment",
        "tags": [],
        "description": "",
        "content": " The simplest way to deploy Velociraptor is via a self-signed, stand-alone deployment. Later we\u0026rsquo;ll see how to deploy Velociraptor in production, but this page will help you deploy a stand-alone test environment.\nOverview Before we start, it\u0026rsquo;s useful to see how a Velociraptor deployment looks at a high level:\nEndpoint systems (also called clients) connect to the server (also called the frontend). The administrator (digital forensic investigator, threat hunter, SOC analyst, IT security operations, etc.) uses their browser to connect to the Velociraptor GUI over SSL. The administrator can query endpoints to collect forensic artifacts or for continual real-time monitoring.\nThe Velociraptor binary distributed through our Github releases page contains all Velociraptor functionality in one binary, without any libraries or external dependencies. The same executable can act as a server or a client depending on command line options, making it easy to deploy and use. Just ensure you use the correct binary for the operating system on your server and endpoints - Windows, Linux or Mac (Darwin).\nA Velociraptor server will also operate with Velociraptor endpoints from any supported operating system. For example, a Velociraptor server running on Linux will support endpoints running Windows, Linux and Mac, so long as the server and endpoints have correct and corresponding configuration files - see below.\n1. Decide where to locate the Velociraptor server For testing purposes, you don\u0026rsquo;t even need two computers. You can simply run both the Velociraptor server and a client on the same computer. You can even use the same Velociraptor executable - just specify the appropriate configuration files when starting the server and client processes (see below).\nFor more interesting testing, you\u0026rsquo;ll want to setup a dedicated server and deploy to a group of endpoints. You can use virtual machines, but it\u0026rsquo;s not essential, since Velociraptor requires no libraries or external dependencies and has an extremely light installation footprint, only writing the configuration, state and history files it needs, all of which can easily be deleted after testing, along with the executable.\nHowever when setting up a longer-term deployment, the main decision is whether the Velociraptor server should be inside or outside your network. Here are some considerations:\nServer inside the network  Doesn\u0026rsquo;t require external server space nor an external domain\n Doesn\u0026rsquo;t require allowing egress from inside your network, since all client-server communications remain inside the network\n However clients are only accessible when they\u0026rsquo;re inside the network (or possibly when connecting via VPN).\n  Server outside the network  Requires external server space; we recommend a cloud-based platform such as AWS Works best with an external resolvable DNS name - dynamic DNS is useful here\n Requires allowing egress connections from inside your network to the Velociraptor server, which can be locked down to only the default service ports - TCP/8000 for client-server comms and TCP/8889 for the administrator to access the GUI\n A significant benefit is that clients will still communicate with the server when outside your network perimeter, providing excellent accessibility.\n  Inside the network, with external accessibility Another possibility is setting up the Velociraptor server outside the network, but allowing clients to connect from the Internet. This provides most of the benefits of both deployment options described above, however will require allowing ingress access and possibly NAT on your network perimeter.\n2. Configure DNS and ports as required If you want endpoints to reach the Velociraptor server across the Internet, you must provide an external DNS name for the server. If you use a dynamic external IP address you might want to use dynamic DNS as well. The DNS name will be used by the clients to connect to the frontends, so you need to ensure the port is open and DNS names resolve properly.\nThe default service ports listening on the server are as follows, although please note that these can be easily changed in the client and server config files that we\u0026rsquo;ll soon be creating:\n TCP/8000 for client-server communications TCP/8889 for access to the GUI, which is only required for users of the server TCP/8001 if you require access to the API, which is not required by default.  It might be tempting to specify an IP address here, especially if you have a static IP or this is an internal deployment. This is not recommended, since if you need to change the IP address, existing clients will continue to attempt to contact the old IP address, rendering them unreachable.\n 3. Generate the server and client configuration files Now that you\u0026rsquo;ve decided on your server location and setup any necessary networking, it\u0026rsquo;s time to create the server and client configuration files.\nVelociraptor uses a pair of configuration files to control the server and endpoints. These define settings such as the location of the server, the communication ports and the encryption keys used to secure client-server comms.\nTherefore our next step is to generate configuration files for our new deployment. The easiest way to get started is using the interactive config generator, which will build a pre-configured deployment type. These files are plain-text and you can easily edit them later to change settings if required.\nStart the interactive config generator as shown below. The various options presented by the config generator are described further below. Note these examples demonstrate configuration on a Linux server, however the commands for Windows are almost identical; only the locations of the folders you choose for the data stores will differ.\n$ velociraptor config generate -i ? Welcome to the Velociraptor configuration generator --------------------------------------------------- I will be creating a new deployment configuration for you. I will begin by identifying what type of deployment you need. [Use arrows to move, space to select, type to filter] \u0026gt; Self Signed SSL Automatically provision certificates with Lets Encrypt Authenticate users with Google OAuth SSO  Self-signed SSL certificates In a self-signed SSL deployment, communication between the user and frontend occurs over TLS with self-signed certificates.\nIn this mode of operation, the endpoints will pin the server\u0026rsquo;s self-signed certificate and will in fact refuse to communicate with a server signed via a public CA. This is actually more secure than standard PKI, because even a compromised public CA can not sign for the Velociraptor server.\nGUI communications are authenticated with basic Auth and the GUI will bind to localhost only on port 8889 by default - i.e. https://localhost:8889/\nSelecting Self Signed SSL proceeds to ask the following questions and suggests default options.\nContinue below for details about the options presented in this interactive configuration wizard.\nSelf Signed SSL Generating keys please wait.... ? Enter the frontend port to listen on. 8000 ? What is the public DNS name of the Frontend (e.g. www.example.com): www.example.com ? Path to the datastore directory. /data/velo/ ? Path to the logs directory. /data/logs/ ? Where should i write the server config file? server.config.yaml ? Where should i write the client config file? client.config.yaml ? GUI Username or email address to authorize (empty to end): mic ? Password *********  Configuring the data store Velociraptor uses two locations for data storage - you\u0026rsquo;ll notice the config generator asks where you want to put these.\n Data Store - holds details about clients and GUI users Logs - holds audit logs.  Pick locations with a reasonable amount of disk space, although this will vary according to your requirements for collecting data from endpoints and running hunts across your network.\nThese locations can also be changed by editing the configuration files. If you want to move these, you\u0026rsquo;ll need to move the folders and edit the server configuration file. Note that clients and servers hold different information in their respective data stores.\nVelociraptor does not enforce any particular data retention policies. At any time the data store can be wiped and the server restarted. If this happens, all the currently deployed clients will be automatically re-enrolled with their existing client IDs. You might want to archive any custom artifacts that you wrote however.\nSince Velociraptor uses plain text files for its configuration, it\u0026rsquo;s possible to archive the entire deployment, or simply delete older files with a scheduled or automated task, or a cron job.\nConfiguration files generated The interactive config generator will now create both a client and server configuration file, which by default are stored in the local folder and named as follows:\n server.config.yaml client.config.yaml  You can inspect these after the config generator is finished - they\u0026rsquo;re just plain text files.\nCreating GUI users The configuration process will now create some GUI users, who will be allowed to log into the admin GUI.\nNote that the accounts you create here are not related to any operating system user accounts. They are completely separate and have no security relationship, unless you choose to implement SSO via Google OAuth.\n You can always add new users to the GUI using the command velociraptor --config server.config.yaml user add MyUserName. User credentials are stored in the data store and not in the config file. If you need to change a user\u0026rsquo;s password simply add them again with the new password.\n4. Start the server Now that you\u0026rsquo;ve generated a server configuration file - server.config.yaml - it\u0026rsquo;s time to start your Velociraptor server.\nWe start the server using the frontend command like so (the -v flag causes verbose output to be shown in the terminal):\n# velociraptor --config server.config.yaml frontend -v [INFO] 2019-04-01T14:44:40+10:00 Starting Frontend. {\u0026quot;build_time\u0026quot;:\u0026quot;2019-04-01T00:25:49+10:00\u0026quot;,\u0026quot;commit\u0026quot;:\u0026quot;503b1cf\u0026quot;,\u0026quot;version\u0026quot;:\u0026quot;0.2.8\u0026quot;} [INFO] 2019-04-01T14:44:40+10:00 Loaded 99 built in artifacts [INFO] 2019-04-01T14:44:40+10:00 Loaded artifact_definitions/custom/Test.Yara.Scan.yaml [INFO] 2019-04-01T14:44:40+10:00 Launched Prometheus monitoring server on 127.0.0.1:8003 [INFO] 2019-04-01T14:44:40+10:00 Frontend is ready to handle client TLS requests at 0.0.0.0:8000 [INFO] 2019-04-01T14:44:40+10:00 Starting hunt manager. [INFO] 2019-04-01T14:44:40+10:00 Launched gRPC API server on 127.0.0.1:8001 [INFO] 2019-04-01T14:44:40+10:00 GUI is ready to handle TLS requests {\u0026quot;listenAddr\u0026quot;:\u0026quot;127.0.0.1:8889\u0026quot;} [INFO] 2019-04-01T14:44:40+10:00 Starting hunt dispatcher. [INFO] 2019-04-01T14:44:40+10:00 Starting stats collector.  The info messages will indicate which port the GUI will listen on, i.e. https://127.0.0.1:8889\nVelociraptor currently does not support multiple frontends - all clients connect to the same frontend which performs all roles (client-server connections, serving the GUI and running the API server). We have used Velociraptor with deployments of around 5,000 endpoints and it performs quite well. Eventually we plan to support horizontal scaling to multiple frontends.\n 5. Verify the GUI works Start a browser and visit your GUI URL (remember, use HTTPS and the default port is TCP/8889). In this mode, the GUI is served over TSL with a self-signed certificate. Your browser will likely report an error due to the unsigned certificate (which as mentioned above, can actually be more secure than a certificate signed by a CA). This is perfectly fine and you can click through these messages and continue to the GUI web page.\nYou will now be presented with a login screen; login as the user you created earlier during the configuration phase.\nYou can also obtain an SSL certificate from Let\u0026rsquo;s Encrypt, to avoid the browser warning being displayed. Details are provided in Cloud Deployment.\nServer setup complete Thats it! Well done.\nYou now have a Velociraptor server running and waiting for clients to connect.\nNow let\u0026rsquo;s install some clients.\n"
},
{
        "uri": "/docs/vql_reference/plugins/",
        "title": "VQL PLugins",
        "tags": [],
        "description": "",
        "content": " VQL plugins are the data sources of VQL queries. While SQL queries refer to static tables of data, VQL queries refer to plugins, which generate data rows to be filtered by the query.\nUnlike SQL, VQL plugins also receive keyword arguments. When the plugin is evaluated it simply generates a sequence of rows which are further filtered by the query.\nThis allows VQL statements to be chained naturally since plugin args may also be other queries.\nVQL plugins are not the same as VQL functions. A plugin is the subject of the VQL query - i.e. plugins always follow the FROM keyword, while functions (which return a single value instead of a sequence of rows) are only present in column specification (e.g. after SELECT) or in condition clauses (i.e. after the WHERE keyword).\n certificates Collect certificate from the system trust store.\nchain Chain the output of several queries into the same table. This plugin takes any args and chains them.\nExample The following returns the rows from the first query then the rows from the second query.\nSELECT * FROM chain( a={ SELECT ...}, b={ SELECT ...})  environ    Arg Description Type     vars Extract these variables from the environment and return them one per row list of string    The row returned will have all environment variables as columns. If the var parameter is provided, only those variables will be provided.\nexecve    Arg Description Type     argv Argv to run the command with. list of string (required)   sep The separator that will be used to split the stdout into rows. string   length Size of buffer to capture output per row. int6    This plugin launches an external command and captures its STDERR, STDOUT and return code. The command\u0026rsquo;s stdout is split using the sep parameter as required.\nThis plugin is mostly useful for running arbitrary code on the client. If you do not want to allow arbitrary code to run, you can disable this by setting the prevent_execve flag in the client\u0026rsquo;s config file. Be aware than many artifacts require running external commands to collect their output though.\nWe do not actually transfer the external program to the system automatically. If you need to run programs which are not usually installed (e.g. Sysinternal\u0026rsquo;s autoruns.exe) you will need to map them from a share (requiring direct access to the AD domain) or download them using the http_client() plugin.\nflatten    Arg Description Type     Name  string    Flatten the columns in query. If any column repeats then we repeat the entire row once for each item.\nforeach    Arg Description Type     row  StoredQuery (required)   query  StoredQuery (required)    Executes \u0026lsquo;query\u0026rsquo; once for each row in the \u0026lsquo;row\u0026rsquo; query.\nglob    Arg Description Type     globs One or more glob patterns to apply to the filesystem. list of string (required)   accessor An accessor to use. string    The glob() plugin is one of the most used plugins. It applies a glob expression in order to search for files by file name. The glob expression allows for wildcards, alternatives and character classes. Globs support both forward and backslashes as path separators. They also support quoting to delimit components.\nA glob expression consists of a sequence of components separated by path separators. If a separator is included within a component it is possible to quote the component to keep it together. For example, the windows registry contains keys with forward slash in their names. Therefore we may use these to prevent the glob from getting confused:\nHKEY_LOCAL_MACHINE\\Microsoft\\Windows\\\u0026quot;Some Key With http://www.microsoft.com/\u0026quot;\\Some Value  Glob expressions are case insensitive and may contain the following wild cards:\n The * matches one or more characters. The ? matches a single character. Alternatives are denoted by braces and comma delimited: {a,b} Recursive search is denoted by a **. By default this searches 3 directories deep. If you need to increase it you can add a depth number (e.g. **10)  By default globs do not expand environment variables. If you need to expand environment variables use the expand() function explicitly:\nglob(globs=expand(string=\u0026quot;%SystemRoot%\\System32\\Winevt\\Logs\\*\u0026quot;))  Example The following searches the raw NTFS disk for event logs.\nSELECT FullPath FROM glob( globs=\u0026quot;C:\\Windows\\System32\\Winevt\\Logs\\*.evtx\u0026quot;, accessor=\u0026quot;ntfs\u0026quot;)  http_client    Arg Description Type     url The URL to fetch string (required)   params Parameters to encode as POST or GET query strings vfilter.Any   headers A dict of headers to send. vfilter.Any   method HTTP method to use (GET, POST) string   chunk_size Read input with this chunk size and send each chunk as a row int   disable_ssl_security Disable ssl certificate verifications. bool    This plugin makes a HTTP connection using the specified method. The headers and parameters may be specified. The plugin reads the specified number of bytes per returned row.\nIf disable_ssl_security is specified we do not enforce SSL integrity. This is required to connect to self signed ssl web sites. For example many API handlers are exposed over such connections.\nWhen connecting to the Velociraptor frontend itself, even in self signed mode, we will ensure certs are properly verified. You can therefore safely export files from the Frontend\u0026rsquo;s public directory over self signed SSL. When connecting to a self signed Velociraptor frontend, we ensure the self signed certificate was issued by the Velociraptor internal CA - i.e. we pin the Frontend\u0026rsquo;s certificate in the binary.\n The http_client() plugin allows use to interact with any web services. If the web service returns a json blob, we can parse it with the parse_json() function (or parse_xml() for SOAP endpoints). Using the parameters with a POST method we may actually invoke actions from within VQL (e.g. send an SMS via an SMS gateway when a VQL event is received).So this is a very powerful plugin.\nExample The following VQL returns the client\u0026rsquo;s external IP as seen by the externalip service.\nSELECT Content as IP from http_client(url='http://www.myexternalip.com/raw')  if    Arg Description Type     condition  vfilter.Any (required)   then  vfilter.StoredQuery (required)   else  vfilter.StoredQuery    Conditional execution of query\ninfo This plugin returns a single row with information about the current system. The information includes the Hostname, Uptime, OS, Platform etc.\nThis plugin is very useful in preconditions as it restricts a query to certain OS or versions.\nSELECT OS from info() where OS = \u0026quot;windows\u0026quot;  netstat Collect network information.\nolevba    Arg Description Type     file A list of filenames to open as OLE files. list of string (required)   accessor The accessor to use. string   max_size Maximum size of file we load into memory. int64    This plugin parses the provided files as OLE documents in order to recover VB macro code. A single document can have multiple code objects, and each such code object is emitted as a row.\nparse_csv    Arg Description Type     filename CSV files to open list of string (required)   accessor The accessor to use string    Parses records from a CSV file. We expect the first row of the CSV file to contain column names. This parser specifically supports Velociraptor\u0026rsquo;s own CSV dialect and so it is perfect for post processing already existing CSV files.\nThe types of each value in each column is deduced based on Velociraptor\u0026rsquo;s standard encoding scheme. Therefore types are properly preserved when read from the CSV file.\nFor example, downloading the results of a hunt in the GUI will produce a CSV file containing artifact rows collected from all clients. We can then use the parse_csv() plugin to further filter the CSV file, or to stack using group by.\nExample The following stacks the result from a Windows.Applications.Chrome.Extensions artifact:\nSELECT count(items=User) As TotalUsers, Name FROM parse_csv(filename=\u0026quot;All Windows.Applications.Chrome.Extensions.csv\u0026quot;) Order By TotalUsers Group By Name  parse_evtx    Arg Description Type     filename A list of event log files to parse. list of string (required)   accessor The accessor to use. string    This plugin parses windows events from the Windows Event log files (EVTX).\nA windows event typically contains two columns. The EventData contains event specific structured data while the System column contains common data for all events - including the Event ID.\nYou should probably almost always filter by one or more event ids (using the System.EventID.Value field).\nExample SELECT System.TimeCreated.SystemTime as Timestamp, System.EventID.Value as EventID, EventData.ImagePath as ImagePath, EventData.ServiceName as ServiceName, EventData.ServiceType as Type, System.Security.UserID as UserSID, EventData as _EventData, System as _System FROM watch_evtx(filename=systemLogFile) WHERE EventID = 7045  parse_records_with_regex    Arg Description Type     accessor The accessor to use. string   file A list of files to parse. list of string (required)   regex A list of regex to apply to the file data. list of string (required)    Parses a file with a set of regexp and yields matches as records. The file is read into a large buffer. Then each regular expression is applied to the buffer, and all matches are emitted as rows.\nThe regular expressions are specified in the Go syntax. They are expected to contain capture variables to name the matches extracted.\nFor example, consider a HTML file with simple links. The regular expression might be:\nregex='\u0026lt;a.+?href=\u0026quot;(?P\u0026lt;Link\u0026gt;[^\u0026quot;]+?)\u0026quot;'  To produce rows with a column Link.\nThe aim of this plugin is to split the file into records which can be further parsed. For example, if the file consists of multiple records, this plugin can be used to extract each record, while parse_string_with_regex() can be used to further split each record into elements. This works better than trying to write a more complex regex which tries to capture a lot of details in one pass.\nExample Here is an example of parsing the /var/lib/dpkg/status files. These files consist of records separated by empty lines:\nPackage: ubuntu-advantage-tools Status: install ok installed Priority: important Section: misc Installed-Size: 74 Maintainer: Ubuntu Developers \u0026lt;ubuntu-devel-discuss@lists.ubuntu.com\u0026gt; Architecture: all Version: 17 Conffiles: /etc/cron.daily/ubuntu-advantage-tools 36de53e7c2d968f951b11c64be101b91 /etc/update-motd.d/80-esm 6ffbbf00021b4ea4255cff378c99c898 /etc/update-motd.d/80-livepatch 1a3172ffaa815d12b58648f117ffb67e Description: management tools for Ubuntu Advantage Ubuntu Advantage is the professional package of tooling, technology and expertise from Canonical, helping organisations around the world manage their Ubuntu deployments. . Subscribers to Ubuntu Advantage will find helpful tools for accessing services in this package. Homepage: https://buy.ubuntu.com  The following query extracts the fields in two passes. The first pass uses parse_records_with_regex() to extract records in blocks, while using parse_string_with_regex() to further break the block into fields.\nSELECT parse_string_with_regex( string=Record, regex=['Package:\\\\s(?P\u0026lt;Package\u0026gt;.+)', 'Installed-Size:\\\\s(?P\u0026lt;InstalledSize\u0026gt;.+)', 'Version:\\\\s(?P\u0026lt;Version\u0026gt;.+)', 'Source:\\\\s(?P\u0026lt;Source\u0026gt;.+)', 'Architecture:\\\\s(?P\u0026lt;Architecture\u0026gt;.+)']) as Record FROM parse_records_with_regex( file=linuxDpkgStatus, regex='(?sm)^(?P\u0026lt;Record\u0026gt;Package:.+?)\\\\n\\\\n')  partitions List all partitions\nproc_dump    Arg Description Type     pid The PID to dump out. int64 (required)    Dumps a process into a crashdump. The crashdump file can be opened with the windows debugger as normal. The plugin returns the filename of the crash dump which is a temporary file - the file will be removed when the query completes, so if you want to hold on to it, you should use the upload() plugin to upload it to the server or otherwise copy it.\nproc_yara    Arg Description Type     context How many bytes to include around each hit int   start The start offset to scan int64   end End scanning at this offset (100mb) uint64   number Stop after this many hits (1). int64   blocksize Blocksize for scanning (1mb). int64   rules Yara rules in the yara DSL. string (required)   files The list of files to scan. list of string (required)   accessor Accessor (e.g. NTFS) string   key A unique key used to cache the yara rule so it does not need to be recompiled string    This plugin uses yara\u0026rsquo;s own engine to scan process memory for the signatures.\nProcess memory access depends on having the SeDebugPrivilege which depends on how Velociraptor was started. Even when running as System, some processes are not accessible.\n pslist    Arg Description     pid A pid to list. If this is provided we are able to operate much faster by only opening a single process.    Lists running processes.\nWhen specifying the pid this operation is much faster so if you are interested in specific processes, the pid should be specified. Otherwise, the plugin returns all processes one on each row.\nread_file    Arg Description Type     filenames One or more files to open. list of string (required)   accessor An accessor to use. string   chunk length of each chunk to read from the file. int   max_length Max length of the file to read. int    This plugin reads a file in chunks and returns each chunks as a separate row.\nIt is useful when we want to report file contents for small files like configuration files etc.\nThe returned row contains the following columns: data, offset, filename\nread_key_values    Arg Description Type     globs Glob expressions to apply. list of string (required)   accessor The accessor to use (default raw_reg). string    This is a convenience plugin which applies the globs to the registry accessor to find keys. For each key the plugin then lists all the values within it, and returns a row which has the value names as columns, while the cells contain the value\u0026rsquo;s stat info (and data content available in the Data field).\nThis makes it easier to access a bunch of related values at once.\nscope The scope plugin returns the current scope as a single row.\nThe main use for this plugin is as a NOOP plugin in those cases we dont want to actually run anything.\nExample SELECT 1+1 As Two FROM scop()  sample    Arg Description Type     n Pick every n row from query. int64 (required)   query Source query. vfilter.StoredQuery (required)    Executes \u0026lsquo;query\u0026rsquo; and samples every n\u0026rsquo;th row. This is most useful on the server in order to downsample event artifact results.\nsplit_records Parses files by splitting lines into records.\n   Arg Description Type     count Only split into this many columns if possible. int   filenames Files to parse. list of string (required)   accessor The accessor to use string   regex The split regular expression (e.g. a comma) string (required)   columns If the first row is not the headers, this arg must provide a list of column names for each value. list of string   first_row_is_headers A bool indicating if we should get column names from the first row. bool    splitparser    Arg Description     filenames One or more files to parse   accessor The accessor to use for opening the file.   regex The split regular expression (e.g. a comma)   columns If the first row is not the headers, this arg must provide a list of column names for each value.   first_row_is_headers A bool indicating if we should get column names from the first row.   count Only split into this many columns if possible.    This plugin is a more generalized parser for delimited files. It is not as smart as the parse_csv() plugin but can use multiple delimiters.\nsource    Arg Description Type     mode HUNT or CLIENT mode can be empty string   day_name Only extract this day\u0026rsquo;s Monitoring logs (deprecated) string   start_time Start return events from this date (for event sources) int64   flow_id A flow ID (client or server artifacts) string   artifact The name of the artifact collection to fetch string   source An optional named source within the artifact string   client_id The client id to extract string   end_time Stop end events reach this time (event sources). int64   hunt_id Retrieve sources from this hunt (combines all results from all clients) string    Retrieve rows from an artifact\u0026rsquo;s source.\nThis plugin is mostly useful in reports. It attempts to do the right thing automatically by inferring most parameters from its execution environment.\nFor example when called within a CLIENT report context, it will automatically fill its flow id, client id etc. Typically this means that you only need to specify the source name (for multi-source artifacts).\nstat    Arg Description     filename One or more files to open.   accessor An accessor to use.    Get file information. Unlike glob() this does not support wildcards.\nupload    Arg Description Type     accessor The accessor to use string   files A list of files to upload list of string (required)    This plugin uploads the specified file to the server. If Velociraptor is run locally the file will be copied tothe --dump_dir path or added to the triage evidence container.\nThis functionality is also available using the upload() function which might be somewhat easier to use.\nusers Display information about workstation local users. This is obtained through the NetUserEnum() API.\nwmi    Arg Description Type     query The WMI query to issue. string (required)   namespace The WMI namespace to use (ROOT/CIMV2) string    This plugin issues a WMI query and returns its rows directly. The exact format of the returned row depends on the WMI query issued.\nThis plugin creates a bridge between WMI and VQL and it is a very commonly used plugin for inspecting the state of windows systems.\nyara    Arg Description Type     number Stop after this many hits (1). int64   blocksize Blocksize for scanning (1mb). int64   rules Yara rules in the yara DSL. string (required)   files The list of files to scan. list of string (required)   accessor Accessor (e.g. NTFS) string   context How many bytes to include around each hit int   start The start offset to scan int64   end End scanning at this offset (100mb) uint64   key If set use this key to cache the yara rules. string    The yara() plugin applies a signature consisting of multiple rules across files. You can read more about yara rules. The accessor is used to open the various files which allows this plugin to work across raw ntfs, zip members etc.\nScanning proceeds by reading a block from the file, then applying the yara rule on the block. This will fail if the signature is split across block boundary. You can choose the block size to be appropriate.\nNote that because we are just scanning the file data, yara plugins like the pe plugin will not work. You can emulate all the yara plugins with VQL anyway (e.g. to test for pe headers)\nTypically the yara rule does not change for the life of the query, so we cache it to avoid having to recompile it each time. The key variable can be used to uniquely identify the cache key for the rule. If the key variable is not specified, we use the rule text itself to generate the cache key. It is recommended that the key parameter be specified because it makes it more efficient.\nShorthand rules This plugin accepts yara rules in the rules parameter. But typically we only search for keywords so writing a full yara syntax rule is tedious. Therefore we provide a shorthand way to specify the keywords. For example:\nwide nocase:foo,bar,baz  When the rule is provided in the above form, the plugin will automatically generate a yara rule which matches any of the specified keywords. The specification before the : means the same thing as the yara DSL and the following combinations are supported wide, wide ascii, wide nocase, wide nocase ascii.\nBy default only the first 100mb of the file are scanned and scanning stops after one hit is found.\n "
},
{
        "uri": "/docs/user-interface/",
        "title": "User Interface",
        "tags": [],
        "description": "",
        "content": " Velociraptor is a Digital Forensic and Incident Response tool, and the Velociraptor user interface is the main tool used to create and collect artifacts from endpoints.\nThe GUI allows for:\n Interactively collecting artifacts from endpoints. Exporting this collected data for offline analysis. Adding/Removing monitoring rules from all endpoints. Adding automated response rules on the server. Adding new artifacts based on new queries.  The Dashboard When initially logging into the application, you will be presented with the Dashboard.\nThe dashboard presents pertinent information about the current deployment:\n The number of clients currently connected to the server. A graph of the server\u0026rsquo;s CPU and memory footprint. A graph of the total number of clients connected.  You can choose to view the graphs over the last 1 day, 2 days or week. You can zoom into a time range.\nThe dashboard is actually an artifact itself called Server.Monitor.Health. You can customize this artifact so the dashboard can display information relevant to your deployment.\n Navigation bar The navigation bar is displayed on the left hand side of the window. You can expand it to include descriptions by clicking on the Hamburger Icon at the top left.\nThere are two sets of options. The first dealing with the server itself, while client specific options require the client to be selected first.\nIn the following pages we will discuss each specific option and show it can be used.\n"
},
{
        "uri": "/about/features/",
        "title": "Velociraptor Features",
        "tags": [],
        "description": "Velociraptor has many features.",
        "content": " To give you a taste of what Velociraptor can do, here are some of the more interesting features:\nEasy setup and deployment  Velociraptor ships as a singe executable which has no dependencies and requires no installation routine Settings are defined by a pair of config files - one for the server and one for each endpoint All comms between endpoints and the server are encrypted The GUI supports SSL and SSO via Google Auth for strong identity management Once an endpoint is started, it\u0026rsquo;s instantly available on the server dashboard (after a browser refresh).  Endpoint operations  Quickly search for endpoints and connect to them for fast browsing and evidence collection Easily browse the contents of endpoint file systems, even bypass locked files using raw NTFS access Remotely inspect and download files of interest all through the GUI Search for files across all endpoints using glob expressions, file metadata and even Yara signatures Collect files from endpoints automatically and on demand Search and parse the Windows Registry for keys and values of interest Perform triage collection of the most common digital forensic artefacts using build-in collection templates Use the built-in library of artefacts to easily hunt for a wide range of forensic artefacts simultaneously across a whole network Acquire process memory based on various conditions for further examination by Windbg Apply Yara signatures to process memory Extend VQL with WMI to build powerful queries for interrogation and data collection An interactive shell is even available, for those unexpected times when you need to get hands-on.  Event streaming to monitor endpoint activity  Velociraptor also supports streaming event queries on endpoints themselves, meaning that data can be collected automatically from endpoints and stored on the server, for continual monitoring and real-time alerting, or for archival and investigation after the fact. Examples include:\n Operating system logging events such as privileged account activities and process execution Extended logging, for example through Sysmon integration DNS queries and responses.  Escalations can be automatically actioned on the server, upon collection of client events\n  User interface and automation  An advanced GUI which makes many simple tasks easy Server-side VQL allows for automating the server using VQL queries too, for example to launch further collection automatically when certain conditions are detected A Python API also allows for full control of the server using Python, including post processing acquired data.  Endpoint resource management  Endpoint activities can be carefully managed, for example client-side throttling allows you to run intensive operations on the endpoints at a controlled rate to minimise impact on endpoint performance.  "
},
{
        "uri": "/docs/user-interface/artifacts/client_events/",
        "title": "Client Event Artifacts",
        "tags": [],
        "description": "",
        "content": " Velociraptor\u0026rsquo;s VQL language is unique in that it allows queries to run in the background and take a long time without using CPU resources. This means that VQL is an asynchronous streaming query engine.\nVelociraptor has a number of VQL plugins which simply block, waiting for events to occur (These are termed Event plugins). When an event occurs, the query returns the event as a single row which is then streamed to the server to be recorded.\nWe are able to use this to implement monitoring rules on the endpoint as never ending VQL queries. The Velociraptor server has a list of event artifacts to run on endpoints. When a new endpoint connects, it receives this list of artifacts and begins running them in parallel - and thereby we begin monitoring the endpoint for the specific events we are interested in.\nInspecting client events Lets start by inspecting some events from the endpoint. By default, Velociraptor collects the Generic.Client.Stats artifact. This artifact samples the Velociraptor process\u0026rsquo;s memory and CPU usage footprint every 10 seconds. This artifact is useful to ensure that Velociraptor\u0026rsquo;s impact on the endpoint is acceptable.\nLet\u0026rsquo;s see what our Velociraptor client impact was on June 16. Select Client Events in the navigation bar and Generic.Client.Stats in the pulldown. You can change the date of interest in the date selector.\nThe Client Events screen renders a report which is defined within the artifact itself. This means you can customize the report, or write your own report. See Report Templates for more about customizing your own reports.\n Updating client event monitoring The Velociraptor server maintains a list of client event artifacts to monitor. You can update this list by clicking the \u0026ldquo;Update client monitoring artifacts\u0026rdquo; button.\nYou can search for client monitoring artifacts to add. Highlighting an existing artifact will display its description and may also add any potential parameters.\nIn the example above we are monitoring process execution logs via the Windows.Events.ProcessCreation artifacts. Let us add also monitor the endpoint DNS queries via the Windows.Events.DNSQueries artifact by adding it, and clicking Save Client Monitoring Artifacts.\nAs soon as you save this, clients will begin streaming these events to the server. In this case every DNS query performed on the endpoint will be recorded on the server.\nLet\u0026rsquo;s look at the DNSQueries performed by this specific endpoint on the 16th June:\nThe DNS Artifact\u0026rsquo;s report produces a table of frequency of the DNS names queried over the day. The table may be filtered (in this case we check for lookups to any microsoft domain), and sorted (by lookup count). This allows us to see if there are excessive connections to particular domains - these might indicate a C\u0026amp;C connection!\n"
},
{
        "uri": "/docs/getting-started/deploying_clients/",
        "title": "Deploying Clients",
        "tags": [],
        "description": "",
        "content": " Velociraptor endpoint agents are called clients. Clients connect to the server and wait for instructions, which mostly consist of VQL statements, then run any VQL queries and return the result to the server.\nThere are several ways to run clients, depending on your needs. Ultimately however, the same Velociraptor binary is run with the client configuration file generated in the previous setup steps, providing it with the key material and configuration.\nThis page summarizes the recommended ways to run the clients and discusses the pros and cons of each approach.\nNote that all Velociraptor binaries (for a particular operating system) are the same. There is no distinction between the client binaries and server binaries. Therefore you can run the server or the client on each supported platform. It\u0026rsquo;s simply command line options telling the binary to behave as a server or client.\nRunning clients interactively This method is most suitable for testing your deployment. In a command shell, simply run the client using the client configuration.\n$ velociraptor --config client.config.yaml client -v  The first time the client connects it will enroll. The enrolment process requires the client to reveal basic information about itself to the server.\nInstalling an MSI An MSI is a standard Windows installer package. The advantages are that most enterprise system administration tools are used to deploying software in MSI packages. Therefore you can use SCCM or Group Policy to add the MSI to the assigned software group.\nFor more information, see How to use Group Policy to remotely install software in Windows Server 2008 and in Windows Server 2003\nOfficial release MSI The recommended way to install Velociraptor as a client on Windows is via the release MSI on the Github release page.\nOne of the main benefits in using the official Velociraptor MSI is that the MSI and the executable are signed. Windows Defender aggressively quarantines unsigned binaries, so it is highly recommended that Velociraptor be signed.\n Since the Velociraptor client requires a configuration file to identify the location of the server, we can\u0026rsquo;t package the configuration file in the official release. Therefore the official MSI does not include a configuration file.\nThe official release installs the Velociraptor executable into C:\\Program Files\\Velociraptor\\ then creates a new Windows service that points to this executable and starts automatically at boot time. If an existing Velociraptor service is already installed, it will be overwritten.\nWhen the service starts, it attempts to load the configuration file from C:\\Program Files\\Velociraptor\\Velociraptor.config.yaml. Note that this is a different name to the configuration file generated by the interactive config generator, which was client.config.yaml. If your client config file is named otherwise, simply rename it to Velociraptor.config.yaml (case insensitive) and move it into the C:\\Program Files\\Velociraptor folder.\nIf that file is not found, Velociraptor will wait and retry periodically to locate the configuration file. When the file is found, the client will be started.\nSo to summarise, when installing from the official MSI package you need to:\n Assign the MSI via Group Policy.\n Copy the configuration file from a share to the Velociraptor program directory. This can be done via Group Policy Scheduled tasks or another way (see the Group Policy procedure outlined below).\n  As soon as the configuration file is copied, Velociraptor will begin communicating with the server.\nInstalling using custom MSI package The official Velociraptor MSI package installs a Windows service with a predictable name. If you want to obfuscate Velociraptor, you might want to build your own MSI package with different binary names, service name, etc.\nTo do so, follow follow the instructions here\nIf building your own MSI, you might as well just package your own configuration file in it. Then you simply assign your MSI to the Group Policy and have it installed everywhere. You may also want to sign the MSI.\nInstalling the client as a service It\u0026rsquo;s also possible to tell the executable to install itself as a service. This option is not recommended because it does not use a proper package manager, and therefore Velociraptor can not be easily uninstalled.\nNevertheless this approach is possible to do via the Group Policy scheduled tasks procedure outlined below. Simply run the following command:\n# velociraptor.exe --config client.config.yaml service install  This will copy the binary to the location specified in the configuration file under Client.windows_installer. You can also change the name of the binary and the service name if you wish.\nAgentless deployment There has been a lot of interest in \u0026ldquo;agentless hunting\u0026rdquo; especially using PowerShell.\nThere are many reasons why agentless hunting is appealing - there are already a ton of endpoint agents and yet another one may not be welcome. Sometimes we need to deploy endpoint agents as part of a DFIR engagement and we may not want to permanently install yet another agent on endpoints.\nIn the agentless deployment scenario, we simply run the binary from a network share using Group Policy settings. The downside to this approach is that the endpoint needs to be on the domain network to receive the Group Policy update (and have the network share accessible) before it can run Velociraptor.\nWhen we run in agentless mode, we are typically interested in collecting a bunch of artifacts via hunts and then exiting - the agent will not restart after a reboot. So this method is suitable for quick hunts on corporate (non roaming) assets.\nCreate a network share The first step is to create a network share with the Velociraptor binary and its configuration file. We will run the binary from the share in this example, but for more reliability you may want to copy the binary into e.g. a temp folder on the end point in case the system becomes disconnected from the domain. For quick hunts though, it should be fine.\nWe create a directory on the server. Note that in the below example, we\u0026rsquo;re creating on a Domain Controller, but we strongly recommend using another location on real deployments.\nIn this example we created a directory called C:\\Users\\Deployment and ensured that it\u0026rsquo;s read-only. We shared the directory as the name Deployment.\nWe now place the Velociraptor executable and client config file in that directory and verify that it can run the binary from the network share. The binary should be accessible via \\\\DC\\Deployment\\velociraptor.exe:\nCreate the Group Policy object Next we create the Group Policy object, which forces all domain connected machines to run the Velociraptor client. We use the Group Policy Management Console:\nSelect the OU or the entire domain and click \u0026ldquo;Create New GPO\u0026rdquo;:\nNow right click the GPO object and select \u0026ldquo;Edit\u0026rdquo;:\nWe will create a new scheduled task. Rather than schedule it at a particular time, we will select to run it immediately. This will force the command to run as soon as the endpoint updates its Group Policy settings, because we don\u0026rsquo;t want to wait for the next reboot of the endpoint.\nNext we give the task a name and a description. In order to allow Velociraptor to access raw devices (e.g. to collect memory or NTFS artifacts) we can specify that the client will run at NT_AUTHORITY\\SYSTEM privileges and run without any user being logged on.\nIt\u0026rsquo;s also worth ticking the \u0026ldquo;hidden\u0026rdquo; checkbox here to prevent a console box from appearing.\nNext click the Actions tab and add a new action. This is where we launch the Velociraptor client. The program will simply be launched from the share (i.e. \\\\DC\\Deployment\\velociraptor.exe) and we give it the arguments allowing it to read the provided configuration file (i.e. --config \\\\DC\\Deployment\\client.config.yaml client -v).\nIn the \u0026ldquo;Setting\u0026rdquo; tab we can control how long we want the client to run. For a quick hunt, this may be an hour or two depending on the network size and hunt scope. For a more comprehensive DFIR collection, be prepared to wait several hours or even days while user machines are naturally disconnected and reconnected from the network. The GPO will ensure the client is killed after the allotted time.\nOnce the GPO is installed it becomes active for all domain machines. You can now schedule any hunts you wish using the Velociraptor GUI. When a domain machine refreshes its Group Policy, it will run the client, which will enrol and immediately participate in any outstanding hunts - thus collecting and delivering its artifacts to the server.\nAfter the allotted time has passed, the client will shut down without having installed anything on the endpoint.\nYou can force a Group Policy update by running the gpupdate program. Now you can verify that Velociraptor is running:\n"
},
{
        "uri": "/docs/user-interface/investigating_clients/searching_clients/",
        "title": "Searching for clients",
        "tags": [],
        "description": "",
        "content": " Before we can interactively examine a client, we need to select it in the GUI. The best way is to search for it. The search bar is present at the top of every page making it easily accessible. Just start typing a search term and the UI will suggest matches.\nWe may search for the client by host name, label or client id. Simply click on the search bar without any search term to show some random clients. The search box features a type ahead completion, so simply start typing the hostname and Velociraptor will show some suggestions.\nAlternatively it is possible to apply regular expressions to the search term and all hosts matching will be retrieved. Prefixing the search test with host: will only search for hostnames, label: will only search labels.\nInternally each client has a unique client ID - Velociraptor uses the client Id to distinguish between hosts, rather than the hostname. This is done since a hostname is not a reliable unique indicator of an endpoint. Many systems change their hostname frequently based on DHCP settings, or even multiple machines may be assigned the same hostname due to misconfiguration. Velociraptor always uses the unique client id for the host, but will usually also show the host\u0026rsquo;s fully qualified domain name (FQDN) as well.\n The results from the search are shown as a table.\nThe table contains three columns:\n The online state of the host is shown as a color icon. A green dot indicated that the host is currently connected to the server, a yellow icon indicates the host is not currently connected but was connected less than 24 hours ago. A red icon indicates that the host has not been seen for 24 hours or more.\n The client ID of the host is shown.\n The hostname reported by the host.\n The operating system version. This indicates if the host is a Windows/Linux/OSX machine and its respective version.\n Any labels applied to the host.\n  Labels Hosts may have labels attached to them. A label is any name associated with a host. Labels are useful when we need to hunt for a well defined group of hosts, then we can restrict the hunt to one or more labels to avoid collecting unnecessary data or accessing machines we should not be.\nIt is possible to manipulate the labels via the search screen. Simply select the hosts in the GUI and then click the \u0026ldquo;add labels\u0026rdquo; button.\nAlthough it is possible to manipulate labels via the GUI, It is usually easier to use VQL queries to add or remove labels via the label() plugin.\nFor example, the following query labels all machines that the user \u0026ldquo;mike\u0026rdquo; ever logged into (Where HuntId is set to a Windows.Sys.Users artifact collector hunt ID:\nSELECT Name, label(client_id=ClientId, labels=\u0026quot;Mikes Box\u0026quot;, op=\u0026quot;set\u0026quot;) FROM hunt_results(hunt_id=HuntId, artifact=\u0026quot;Windows.Sys.Users\u0026quot;) WHERE Name =~ \u0026quot;mike\u0026quot;  Built-in Labels While one can add labels to machines using the GUI this is not practical for labeling very large numbers of client, for example belonging to a particular Active Directory Organizational Unit (OU). It is reasonable to want to quickly select those machines belonging to a particular OU.\nWe can use labels to identify machines installed by a specific group policy. For example, suppose we have a particular OU called Sales. We want to ensure that Velociraptor clients in the Sales team are specifically marked by the Sales label.\nSimply modify the client\u0026rsquo;s configuration file as obtained in the Agentless Deployment Mode to contain the Sales label:\nClient: labels: - Sales  Then we apply the Group Policy Object only on the Sales OU which will result in those clients being enrolled with the Sales label automatically.\nSelecting a client Clicking on any client in the search screen will switch the GUI to client mode. This will now present information relevant to the selected client.\nYou can easily tell which client we are dealing with as the name of the host, and the last time we connected with it are shown:\nVelociraptor maintains some basic information about the host, such as its hostname, labels, last seen IP and last seen time. This is shown in the Host View pane. Velociraptor uses this information to make it possible to search for this host and target it for further investigation. Velociraptor gathers this information during the Interrogate operation. Interrogation normally occurs when the client first enrolls, but you can interrogate any client at any time by clicking the Interrogate button.\nUltimately, interrogation simply runs a bunch of VQL queries on the endpoint. We can issue arbitrary VQL and have it appear in the VQL Drilldown page.\nTo add arbitrary VQL queries to the drill-down simply populate them in the configuration file. For example the following also collect the endpoint\u0026rsquo;s uptime:\nFlows: interrogate_additional_queries: - Name: Uptime VQL: \u0026quot;SELECT Uptime from info()\u0026quot;  The VQL Drilldown simply shows the results from running the VQL queries on the client. The time when those were run is shown, as well as the actual query run is visible by hovering over the title.\n "
},
{
        "uri": "/docs/vql_reference/event_plugins/",
        "title": "VQL Event PLugins",
        "tags": [],
        "description": "",
        "content": " VQL Event plugins are plugins which never terminate - but instead generate rows based on events. Event plugins are useful for creating client monitoring artifacts. Currently, client side monitoring artifacts are specified in the Events section of the server configuration file. When clients connect to the server, they receive a list of monitoring artifacts they are to run. The client runs all artifacts in parallel and their results are streamed to the server.\nclock    Arg Description Type     period Wait this many seconds between events. int64   ms Wait this many ms between events. int64    This plugin generates events periodically. The periodicity can be controlled either via the period or the ms parameter. Each row will be a go time.Time object. You can access its unix epoch time with the Sec column.\nExample The following will generate an event every 10 seconds.\nSELECT Sec FROM clock(period=10)  diff    Arg Description Type     query Source for cached rows. vfilter.StoredQuery (required)   key The column to use as key. string (required)   period Number of seconds between evaluation of the query. int64    The diff() plugin runs a non-event query periodically and calculates the difference between its result set from the last run.\nThis can be used to create event queries which watch for changes from simpler non-event queries.\nThe key parameter is the name of the column which is used to determine row equivalency.\nThere is only a single equivalence row specified by the key parameter, and it must be a string. If you need to watch multiple columns you need to create a new column which is the concatenation of other columns. For example format(format=\u0026quot;%s%d\u0026quot;, args=[Name, Pid])\n Example The following VQL monitors all removable drives and lists files on newly inserted drives, or files that have been added to removable drives.\nLET removable_disks = SELECT Name AS Drive, Size FROM glob(globs=\u0026quot;/*\u0026quot;, accessor=\u0026quot;file\u0026quot;) WHERE Data.Description =~ \u0026quot;Removable\u0026quot; LET file_listing = SELECT FullPath, timestamp(epoch=Mtime.Sec) As Modified, Size FROM glob(globs=Drive+\u0026quot;\\\\**\u0026quot;, accessor=\u0026quot;file\u0026quot;) LIMIT 1000 SELECT * FROM diff( query={ SELECT * FROM foreach(row=removable_disks, query=file_listing) }, key=\u0026quot;FullPath\u0026quot;, period=10) WHERE Diff = \u0026quot;added\u0026quot;  dns Monitor dns queries. This plugin opens a raw socket and monitors network traffic for DNS questions and answers.\nWhen Velociraptor attempts to open a raw socket, sometimes Windows Defender treats that as suspicious behavior and quarantines the Velociraptor binary. This can be avoided by signing the binary which signals to Windows Defender that the binary is legitimate.\nIf you do not intend to build Velociraptor from source, use the official signed Velociraptor binaries which should not trigger alerts from Windows Defender.\n fifo    Arg Description Type     query Source for cached rows. vfilter.StoredQuery (required)   max_age Maximum number of seconds to hold rows in the fifo. int64   max_rows Maximum number of rows to hold in the fifo. int64    The fifo() plugin allows for VQL queries to apply across historical data. The fifo plugin accepts another event query as parameter, then retains the last max_rows rows from it in an internal queue. Every subsequent evaluation from the query will return the full set of rows in the queue. Older rows are expired from the queue according to the max_age parameter.\nFifos are usually used to form queries that look for specific pattern of behavior. For example, a successful logon followed by failed logons. In this case the fifo retains the recent history of failed logons in its internal queue, then when a successful logon occurs we can check the recent failed ones in its queue.\nExample The following checks for 5 failed logons followed by a successful logon.\nLET failed_logon = SELECT EventData as FailedEventData, System as FailedSystem FROM watch_evtx(filename=securityLogFile) WHERE System.EventID.Value = 4625 LET last_5_events = SELECT FailedEventData, FailedSystem FROM fifo(query=failed_logon, max_rows=500, max_age=atoi(string=failedLogonTimeWindow)) LET success_logon = SELECT EventData as SuccessEventData, System as SuccessSystem FROM watch_evtx(filename=securityLogFile) WHERE System.EventID.Value = 4624 SELECT * FROM foreach( row=success_logon, query={ SELECT SuccessSystem.TimeCreated.SystemTime AS LogonTime, SuccessSystem, SuccessEventData, enumerate(items=FailedEventData) as FailedEventData, FailedSystem, count(items=SuccessSystem) as Count FROM last_5_events WHERE FailedEventData.SubjectUserName = SuccessEventData.SubjectUserName GROUP BY LogonTime }) WHERE Count \u0026gt; atoi(string=failureCount)  netstat Collect network information using the network APIs.\nwatch_csv    Arg Description Type     filename CSV files to open list of string (required)   accessor The accessor to use string    This plugin is the event version of parse_csv(). When the CSV file grows this plugin will emit the new rows.\nwatch_evtx    Arg Description Type     filename A list of event log files to parse. list of string (required)   accessor The accessor to use. string    Watch an EVTX file and stream events from it. This is the Event plugin version of parse_evtx().\nIt often takes several seconds for events to be flushed to the event log and so this plugin\u0026rsquo;s event may be delayed. For some applications this results in a race condition with the event itself - for example, files mentioned in the event may already be removed by the time the event is triggered.\n watch_monitoring    Arg Description Type     source An optional artifact named source string   client_id A list of client ids to watch. If not provided we watch all clients. list of string   artifact The event artifact name to watch string (required)    Watch clients\u0026rsquo; monitoring log. This is an event plugin. If client_id is not provided we watch the global journal which contains events from all clients.\nwmi_events    Arg Description Type     query WMI query to run. string (required)   namespace WMI namespace string (required)   wait Wait this many seconds for events and then quit. int64 (required)    This plugin sets up a WMI event listener query.\n"
},
{
        "uri": "/docs/artifacts/windows_system/",
        "title": "Windows System",
        "tags": [],
        "description": "These artifacts collect information related to the windows system itself.",
        "content": " Windows.Sys.AppcompatShims Application Compatibility shims are a way to persist malware. This table presents the AppCompat Shim information from the registry in a nice format.\n   Arg Default Description     shimKeys HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\AppCompatFlags\\InstalledSDB\\*    customKeys HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\AppCompatFlags\\Custom\\*\\*       View Artifact Source   name: Windows.Sys.AppcompatShims description: | Application Compatibility shims are a way to persist malware. This table presents the AppCompat Shim information from the registry in a nice format. reference: - http://files.brucon.org/2015/Tomczak_and_Ballenthin_Shims_for_the_Win.pdf parameters: - name: shimKeys default: \u0026gt;- HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\AppCompatFlags\\InstalledSDB\\* - name: customKeys default: \u0026gt;- HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\AppCompatFlags\\Custom\\*\\* sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | LET installed_sdb \u0026lt;= SELECT Key, Key.Name as SdbGUID, DatabasePath, DatabaseType, DatabaseDescription, -- Convert windows file time to unix epoch. (DatabaseInstallTimeStamp / 10000000) - 11644473600 AS DatabaseInstallTimeStamp FROM read_reg_key( globs=split(string=shimKeys, sep=\u0026quot;,[\\\\s]*\u0026quot;), accessor=\u0026quot;reg\u0026quot;) - | LET result = SELECT * from foreach( row={ SELECT regex_replace( source=FullPath, replace=\u0026quot;$1\u0026quot;, re=\u0026quot;^.+\\\\\\\\([^\\\\\\\\]+)\\\\\\\\[^\\\\\\\\]+$\u0026quot;) as Executable, regex_replace( source=Name, replace=\u0026quot;$1\u0026quot;, re=\u0026quot;(\\\\{[^}]+\\\\}).*$\u0026quot;) as SdbGUIDRef, Name as ExeName from glob( globs=split(string=customKeys, sep=\u0026quot;,[\\\\s]*\u0026quot;), accessor=\u0026quot;reg\u0026quot;) }, query={ SELECT Executable, DatabasePath, DatabaseType, DatabaseDescription, DatabaseInstallTimeStamp, SdbGUID FROM installed_sdb WHERE SdbGUID = SdbGUIDRef }) - | SELECT * from result    Windows.Sys.CertificateAuthorities Certificate Authorities installed in Keychains/ca-bundles.\n  View Artifact Source   name: Windows.Sys.CertificateAuthorities description: Certificate Authorities installed in Keychains/ca-bundles. sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | select Store, IsCA, Subject, encode(string=SubjectKeyId, type='hex') AS SubjectKeyId, encode(string=AuthorityKeyId, type='hex') AS AuthorityKeyId, Issuer, KeyUsageString, IsSelfSigned, SHA1, SignatureAlgorithm, PublicKeyAlgorithm, KeyStrength, NotBefore, NotAfter, HexSerialNumber from certificates()    Windows.Sys.DiskInfo Retrieve basic information about the physical disks of a system.\n  View Artifact Source   name: Windows.Sys.DiskInfo description: Retrieve basic information about the physical disks of a system. sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | SELECT Partitions, Index as DiskIndex, InterfaceType as Type, PNPDeviceID, DeviceID, Size, Manufacturer, Model, Name, SerialNumber, Description FROM wmi( query=\u0026quot;SELECT * from Win32_DiskDrive\u0026quot;, namespace=\u0026quot;ROOT\\\\CIMV2\u0026quot;)    Windows.Sys.Drivers Details for in-use Windows device drivers. This does not display installed but unused drivers.\n  View Artifact Source   name: Windows.Sys.Drivers description: Details for in-use Windows device drivers. This does not display installed but unused drivers. sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | SELECT * from wmi( query=\u0026quot;select * from Win32_PnPSignedDriver\u0026quot;, namespace=\u0026quot;ROOT\\\\CIMV2\u0026quot;)    Windows.Sys.FirewallRules List windows firewall rules.\n   Arg Default Description     regKey HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\SharedAccess\\Parameters\\FirewallPolicy\\**\\FirewallRules\\*       View Artifact Source   name: Windows.Sys.FirewallRules description: List windows firewall rules. reference: - https://social.technet.microsoft.com/Forums/azure/en-US/aaed9c6a-fb8b-4d43-8b69-9f4e0f619a8c/how-to-check-the-windows-firewall-settings-from-netsh-command?forum=winserverGP parameters: - name: regKey default: HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\SharedAccess\\Parameters\\FirewallPolicy\\**\\FirewallRules\\* sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | LET rules = SELECT Name as Value, parse_string_with_regex(string=Data, regex=[\u0026quot;Action=(?P\u0026lt;Action\u0026gt;[^|]+)\u0026quot;, \u0026quot;Active=(?P\u0026lt;Active\u0026gt;[^|]+)\u0026quot;, \u0026quot;Dir=(?P\u0026lt;Dir\u0026gt;[^|]+)\u0026quot;, \u0026quot;Protocol=(?P\u0026lt;Protocol\u0026gt;[^|]+)\u0026quot;, \u0026quot;LPort=(?P\u0026lt;LPort\u0026gt;[^|]+)\u0026quot;, \u0026quot;Name=(?P\u0026lt;Name\u0026gt;[^|]+)\u0026quot;, \u0026quot;Desc=(?P\u0026lt;Desc\u0026gt;[^|]+)\u0026quot;, \u0026quot;App=(?P\u0026lt;App\u0026gt;[^|]+)\u0026quot;]) as Record, Data, FullPath FROM glob(globs=regKey, accessor=\u0026quot;reg\u0026quot;) - | SELECT Value, Record.Action as Action, Record.Name as Name, Record.Desc as Desc, Record.App as App, Record.Action as Action, Record.Dir as Dir, if(condition=Record.Protocol = \u0026quot;6\u0026quot;, then=\u0026quot;TCP\u0026quot;, else=if(condition=Record.Protocol = \u0026quot;17\u0026quot;, then=\u0026quot;UDP\u0026quot;, else=Record.Protocol)) as Protocol, if(condition=Record.LPort = NULL, then=\u0026quot;Any\u0026quot;, else=Record.LPort) as LPort, Record.Name as Name FROM rules    Windows.Sys.Interfaces Report information about the systems interfaces. This artifact simply parses the output from ipconfig /all.\n  View Artifact Source   name: Windows.Sys.Interfaces description: | Report information about the systems interfaces. This artifact simply parses the output from ipconfig /all. sources: - precondition: SELECT OS from info() where OS = \u0026quot;windows\u0026quot; queries: - | // Run ipconfig to get all information about interfaces. LET ipconfig = SELECT * FROM execve(argv=['ipconfig', '/all']) - | // This produces a single row per interface. LET interfaces = SELECT Name, Data FROM parse_records_with_regex( file=ipconfig.Stdout, accessor='data', // This makes the data appear as a file. regex='(?s)Ethernet adapter (?P\u0026lt;Name\u0026gt;[^:]+?):\\r\\n\\r\\n(?P\u0026lt;Data\u0026gt;.+?)\\r\\n(\\r\\n|$)') - | // Now extract interesting things from each interface definition. SELECT Name, parse_string_with_regex( string=Data, regex=[ \u0026quot;Description[^:]+: (?P\u0026lt;Description\u0026gt;.+)\\r\\n\u0026quot;, \u0026quot;Physical Address[^:]+: (?P\u0026lt;MAC\u0026gt;.+)\\r\\n\u0026quot;, \u0026quot;IPv4 Address[^:]+: (?P\u0026lt;IP\u0026gt;[0-9.]+)\u0026quot;, \u0026quot;Default Gateway[^:]+: (?P\u0026lt;Gateway\u0026gt;.+)\\r\\n\u0026quot;, \u0026quot;DNS Servers[^:]+: (?P\u0026lt;DNS\u0026gt;.+)\\r\\n\u0026quot;, \u0026quot;DHCP Server[^:]+: (?P\u0026lt;DHCP\u0026gt;.+)\\r\\n\u0026quot; ] ) As Details FROM interfaces    Windows.Sys.PhysicalMemoryRanges List Windows physical memory ranges.\n   Arg Default Description     physicalMemoryKey HKEY_LOCAL_MACHINE\\HARDWARE\\RESOURCEMAP\\System Resources\\Physical Memory\\.Translated    Profile {\\n \u0026ldquo;CM_RESOURCE_LIST\u0026rdquo;: [0, {\\n \u0026ldquo;Count\u0026rdquo;: [0, [\u0026ldquo;uint32\u0026rdquo;]],\\n \u0026ldquo;List\u0026rdquo;: [4, [\u0026ldquo;CM_FULL_RESOURCE_DESCRIPTOR\u0026rdquo;]]\\n }],\\n \u0026ldquo;CM_FULL_RESOURCE_DESCRIPTOR\u0026rdquo;: [0, {\\n \u0026ldquo;PartialResourceList\u0026rdquo;: [8, [\u0026ldquo;CM_PARTIAL_RESOURCE_LIST\u0026rdquo;]]\\n }],\\n\\n \u0026ldquo;CM_PARTIAL_RESOURCE_LIST\u0026rdquo;: [0, {\\n \u0026ldquo;Version\u0026rdquo;: [0, [\u0026ldquo;uint16\u0026rdquo;]],\\n \u0026ldquo;Revision\u0026rdquo;: [2, [\u0026ldquo;uint16\u0026rdquo;]],\\n \u0026ldquo;Count\u0026rdquo;: [4, [\u0026ldquo;uint32\u0026rdquo;]],\\n \u0026ldquo;PartialDescriptors\u0026rdquo;: [8, [\u0026ldquo;Array\u0026rdquo;, {\\n \u0026ldquo;Target\u0026rdquo;: \u0026ldquo;CM_PARTIAL_RESOURCE_DESCRIPTOR\u0026rdquo;\\n }]]\\n }],\\n\\n \u0026ldquo;CM_PARTIAL_RESOURCE_DESCRIPTOR\u0026rdquo;: [20, {\\n \u0026ldquo;Type\u0026rdquo;: [0, [\u0026ldquo;char\u0026rdquo;]],\\n \u0026ldquo;ShareDisposition\u0026rdquo;: [1, [\u0026ldquo;char\u0026rdquo;]],\\n \u0026ldquo;Flags\u0026rdquo;: [2, [\u0026ldquo;uint16\u0026rdquo;]],\\n \u0026ldquo;Start\u0026rdquo;: [4, [\u0026ldquo;int64\u0026rdquo;]],\\n \u0026ldquo;Length\u0026rdquo;: [12, [\u0026ldquo;uint32\u0026rdquo;]]\\n }]\\n}\\n       View Artifact Source   name: Windows.Sys.PhysicalMemoryRanges description: List Windows physical memory ranges. reference: - https://docs.microsoft.com/en-us/windows-hardware/drivers/ddi/content/wdm/ns-wdm-_cm_resource_list parameters: - name: physicalMemoryKey default: HKEY_LOCAL_MACHINE\\HARDWARE\\RESOURCEMAP\\System Resources\\Physical Memory\\.Translated - name: Profile default: | { \u0026quot;CM_RESOURCE_LIST\u0026quot;: [0, { \u0026quot;Count\u0026quot;: [0, [\u0026quot;uint32\u0026quot;]], \u0026quot;List\u0026quot;: [4, [\u0026quot;CM_FULL_RESOURCE_DESCRIPTOR\u0026quot;]] }], \u0026quot;CM_FULL_RESOURCE_DESCRIPTOR\u0026quot;: [0, { \u0026quot;PartialResourceList\u0026quot;: [8, [\u0026quot;CM_PARTIAL_RESOURCE_LIST\u0026quot;]] }], \u0026quot;CM_PARTIAL_RESOURCE_LIST\u0026quot;: [0, { \u0026quot;Version\u0026quot;: [0, [\u0026quot;uint16\u0026quot;]], \u0026quot;Revision\u0026quot;: [2, [\u0026quot;uint16\u0026quot;]], \u0026quot;Count\u0026quot;: [4, [\u0026quot;uint32\u0026quot;]], \u0026quot;PartialDescriptors\u0026quot;: [8, [\u0026quot;Array\u0026quot;, { \u0026quot;Target\u0026quot;: \u0026quot;CM_PARTIAL_RESOURCE_DESCRIPTOR\u0026quot; }]] }], \u0026quot;CM_PARTIAL_RESOURCE_DESCRIPTOR\u0026quot;: [20, { \u0026quot;Type\u0026quot;: [0, [\u0026quot;char\u0026quot;]], \u0026quot;ShareDisposition\u0026quot;: [1, [\u0026quot;char\u0026quot;]], \u0026quot;Flags\u0026quot;: [2, [\u0026quot;uint16\u0026quot;]], \u0026quot;Start\u0026quot;: [4, [\u0026quot;int64\u0026quot;]], \u0026quot;Length\u0026quot;: [12, [\u0026quot;uint32\u0026quot;]] }] } sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | SELECT Type.AsInteger as Type, format(format=\u0026quot;%#0x\u0026quot;, args=Start.AsInteger) as Start, format(format=\u0026quot;%#0x\u0026quot;, args=Length.AsInteger) as Length FROM foreach( row={ SELECT Data FROM stat(filename=physicalMemoryKey, accessor='reg') }, query={ SELECT Type, Start, Length, Data FROM binary_parse( string=Data.value, profile=Profile, target=\u0026quot;CM_RESOURCE_LIST\u0026quot;, start=\u0026quot;List.PartialResourceList.PartialDescriptors\u0026quot;) })    Windows.Sys.Programs Represents products as they are installed by Windows Installer. A product generally correlates to one installation package on Windows. Some fields may be blank as Windows installation details are left to the discretion of the product author.\n   Arg Default Description     programKeys HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\*, HKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\*, HKEY_USERS\\*\\Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\*       View Artifact Source   name: Windows.Sys.Programs description: | Represents products as they are installed by Windows Installer. A product generally correlates to one installation package on Windows. Some fields may be blank as Windows installation details are left to the discretion of the product author. reference: - https://github.com/facebook/osquery/blob/master/specs/windows/programs.table parameters: - name: programKeys default: \u0026gt;- HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\*, HKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\*, HKEY_USERS\\*\\Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\* sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | SELECT Key.Name as Name, timestamp(epoch=Key.Mtime.Sec) AS MTime, DisplayName, DisplayVersion, InstallLocation, InstallSource, Language, Publisher, UninstallString, InstallDate FROM read_reg_key(globs=split(string=programKeys, sep=',[\\\\s]*'), accessor=\u0026quot;reg\u0026quot;)    Windows.Sys.StartupItems Applications that will be started up from the various run key locations.\n   Arg Default Description     runKeyGlobs HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run\\*, HKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Run\\, HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\Run\\* HKEY_USERS\\\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run\\, HKEY_USERS\\*\\SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Run\\, HKEY_USERS\\*\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\Run\\*\\n    startupApprovedGlobs HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\StartupApproved\\*, HKEY_USERS\\*\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\StartupApproved\\*\\n    startupFolderDirectories C:/ProgramData/Microsoft/Windows/Start Menu/Programs/Startup/, C:/Users/*/AppData/Roaming/Microsoft/Windows/StartMenu/Programs/Startup/\\n       View Artifact Source   name: Windows.Sys.StartupItems description: Applications that will be started up from the various run key locations. reference: - https://docs.microsoft.com/en-us/windows/desktop/setupapi/run-and-runonce-registry-keys parameters: - name: runKeyGlobs default: \u0026gt; HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run*\\*, HKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Run*\\*, HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\Run*\\* HKEY_USERS\\*\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run*\\*, HKEY_USERS\\*\\SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Run*\\*, HKEY_USERS\\*\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\Run*\\* - name: startupApprovedGlobs default: \u0026gt; HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\StartupApproved\\**, HKEY_USERS\\*\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\StartupApproved\\** - name: startupFolderDirectories default: \u0026gt; C:/ProgramData/Microsoft/Windows/Start Menu/Programs/Startup/**, C:/Users/*/AppData/Roaming/Microsoft/Windows/StartMenu/Programs/Startup/** sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | /* We need to search this multiple times so we materialize it into a variable (using the \u0026lt;= operator) */ LET approved \u0026lt;= SELECT Name as ApprovedName, encode(string=Data, type=\u0026quot;hex\u0026quot;) as Enabled FROM glob(globs=split( string=startupApprovedGlobs, sep=\u0026quot;[, ]+\u0026quot;), accessor=\u0026quot;reg\u0026quot;) WHERE Enabled =~ \u0026quot;^0[0-9]0+$\u0026quot; - | LET registry_runners = SELECT Name, FullPath, Data.value as Command, if( condition={ SELECT Enabled from approved WHERE Name = ApprovedName }, then=\u0026quot;enabled\u0026quot;, else=\u0026quot;disabled\u0026quot;) as Enabled FROM glob( globs=split(string=runKeyGlobs, sep=\u0026quot;[, ]+\u0026quot;), accessor=\u0026quot;reg\u0026quot;) - | LET file_runners = SELECT * FROM foreach( row={ SELECT Name, FullPath FROM glob( globs=split(string=startupFolderDirectories, sep=\u0026quot;,\\\\s*\u0026quot;)) }, query={ SELECT Name, FullPath, \u0026quot;enable\u0026quot; as Enabled, encode(string=Data, type='utf16') as Command FROM read_file(filenames=FullPath) }) - | SELECT * from chain( first=registry_runners, second=file_runners)    Windows.Sys.Users List User accounts. We combine two data sources - the output from the NetUserEnum() call and the list of SIDs in the registry.\n   Arg Default Description     remoteRegKey HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\ProfileList\\*       View Artifact Source   name: Windows.Sys.Users description: | List User accounts. We combine two data sources - the output from the NetUserEnum() call and the list of SIDs in the registry. parameters: - name: remoteRegKey default: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\ProfileList\\* sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | LET roaming_users \u0026lt;= SELECT \u0026quot;\u0026quot; as Uid, \u0026quot;\u0026quot; as Gid, lookupSID( sid=basename(path=Key.FullPath) ) as Name, Key.FullPath as Description, ProfileImagePath as Directory, basename(path=Key.FullPath) as UUID, Key.Mtime.Sec as Mtime, \u0026quot;roaming\u0026quot; as Type FROM read_reg_key(globs=remoteRegKey, accessor=\u0026quot;reg\u0026quot;) - | LET local_users \u0026lt;= select User_id as Uid, Primary_group_id as Gid, Name, Comment as Description, { SELECT Directory from roaming_users WHERE User_sid = UUID } as Directory, User_sid as UUID, 0 AS Mtime, \u0026quot;local\u0026quot; AS Type FROM users() - | LET local_users_with_mtime = SELECT Uid, Gid, Name, Description, Directory, UUID, { SELECT Mtime.Sec FROM stat(filename=expand(path=Directory)) } As Mtime, Type FROM local_users - | SELECT * from chain( q1=local_users_with_mtime, q2={ -- Only show users not already shown in the local_users above. SELECT * from roaming_users where not UUID in local_users.UUID }) reports: - type: HUNT template: | # Users Hunt Enumerating all the users on all endpoints can reveal machines which had an unexpected login activity. For example, if a user from an unrelated department is logging into an endpoint by virtue of domain credentials, this could mean their account is compromised and the attackers are laterally moving through the network. {{ define \u0026quot;users\u0026quot; }} SELECT Name, UUID, Fqdn, timestamp(epoch=Mtime) as LastMod FROM source() WHERE NOT UUID =~ \u0026quot;(-5..$|S-1-5-18|S-1-5-19|S-1-5-20)\u0026quot; {{ end }} {{ Query \u0026quot;users\u0026quot; | Table }} - type: CLIENT template: | System Users ============ {{ .Description }} The following table shows basic information about the users on this system. * Remote users also show the modification timestamp from the registry key. * Local users show the mtime of their home directory. {{ define \u0026quot;users\u0026quot; }} LET users \u0026lt;= SELECT Name, UUID, Type, timestamp(epoch=Mtime) as Mtime FROM source() {{ end }} {{ Query \u0026quot;users\u0026quot; \u0026quot;SELECT Name, UUID, Type, Mtime FROM users\u0026quot; | Table }}    Windows.System.Amcache Get information from the system\u0026rsquo;s amcache.\nThe Amcache.hve file is a registry file that stores the information of executed applications. Amcache.hve records the recent processes that were run and lists the path of the files that’s executed which can then be used to find the executed program.\nThis artifact works on Windows 10 1607 version.\nReferences: https://www.andreafortuna.org/cybersecurity/amcache-and-shimcache-in-forensic-analysis/ https://www.ssi.gouv.fr/uploads/2019/01/anssi-coriin_2019-analysis_amcache.pdf\n   Arg Default Description     amCacheGlob %SYSTEMROOT%/appcompat/Programs/Amcache.hve    amCacheRegPath /Root/InventoryApplicationFile/*       View Artifact Source   name: Windows.System.Amcache description: | Get information from the system's amcache. The Amcache.hve file is a registry file that stores the information of executed applications. Amcache.hve records the recent processes that were run and lists the path of the files that’s executed which can then be used to find the executed program. This artifact works on Windows 10 1607 version. References: https://www.andreafortuna.org/cybersecurity/amcache-and-shimcache-in-forensic-analysis/ https://www.ssi.gouv.fr/uploads/2019/01/anssi-coriin_2019-analysis_amcache.pdf parameters: - name: amCacheGlob default: \u0026quot;%SYSTEMROOT%/appcompat/Programs/Amcache.hve\u0026quot; - name: amCacheRegPath default: /Root/InventoryApplicationFile/* precondition: | SELECT OS From info() where OS = 'windows' sources: - name: InventoryApplicationFile queries: - | SELECT FileId, Key.FullPath as Key, timestamp(epoch=Key.Mtime.Sec) as LastModified, Key.Mtime.Sec as _LastModified, LowerCaseLongPath as Binary, Name, Size, ProductName, Publisher, Version, BinFileVersion FROM foreach( row={ SELECT FullPath from glob(globs=expand(path=amCacheGlob)) }, query={ SELECT * from read_reg_key( globs=url(scheme='ntfs', path=FullPath, fragment=amCacheRegPath).String, accessor='raw_reg' ) }) - name: File queries: - | SELECT * FROM foreach( row={ SELECT FullPath from glob(globs=expand(path=amCacheGlob)) }, query={ SELECT get(item=scope(), member=\u0026quot;100\u0026quot;) As ProductId, get(item=scope(), member=\u0026quot;101\u0026quot;) As SHA1, get(item=scope(), member=\u0026quot;15\u0026quot;) As FullPath, timestamp(epoch=Key.Mtime.Sec) as LastModifiedKey FROM read_reg_key( globs=url(scheme='ntfs', path=FullPath, fragment='/Root/File/*/*').String, accessor='raw_reg' ) }) reports: - type: CLIENT template: | {{define \u0026quot;recent_executions\u0026quot;}} LET recent_executions \u0026lt;= SELECT LastModified, Name, count(items=Name) As Count, int(int=_LastModified/3600) AS Hour FROM source(source=\u0026quot;InventoryApplicationFile\u0026quot;) GROUP BY Hour LIMIT 500 {{ end }} {{ define \u0026quot;timeline\u0026quot; }} SELECT LastModified, format(format=\u0026quot;%s (%d)\u0026quot;, args=[Name, Count]) As TotalCount FROM recent_executions {{ end }} The AMCache file ================ {{ .Description }} ## Execution clusters The AMCache artifact only shows us the time of first execution of a binary. We get an idea when it was installed. Typically execution artifacts are clustered in time - if an attacker copies a bunch of new tools they will all start running at about the same time. The below timeline shows a summary of execution clusters. The binaries are grouped in an hour interval. The label is the first binary name and the total number of binaries within that hour. \u0026gt; For clarity we hide the names of all other binaries, and just show the total count. {{ Query \u0026quot;recent_executions\u0026quot; \u0026quot;timeline\u0026quot; | Timeline }} Here is the same data in tabular form. {{ Query \u0026quot;timeline\u0026quot; | Table }}    Windows.System.CriticalServices This artifact returns information about any services which are considered critical.\nThe default list contains virus scanners. If the software is not installed at all, it will not be shown.\nATT\u0026amp;CK: T1089\nReferences:  https://github.com/teoseller/osquery-attck/blob/master/windows_critical_service_status.conf     Arg Default Description     lookupTable ServiceName\\nWinDefend\\nMpsSvc\\nSepMasterService\\nSAVAdminService\\nSavService\\nwscsvc\\nwuauserv\\n       View Artifact Source   name: Windows.System.CriticalServices description: | This artifact returns information about any services which are considered critical. The default list contains virus scanners. If the software is not installed at all, it will not be shown. ATT\u0026amp;CK: T1089 ### References: * https://github.com/teoseller/osquery-attck/blob/master/windows_critical_service_status.conf precondition: SELECT OS From info() where OS = 'windows' parameters: - name: lookupTable default: | ServiceName WinDefend MpsSvc SepMasterService SAVAdminService SavService wscsvc wuauserv sources: - queries: - LET lookup \u0026lt;= SELECT * FROM parse_csv(filename=lookupTable, accessor='data') - | SELECT Name, DisplayName, Created, State, { SELECT * FROM lookup WHERE Name =~ ServiceName } AS Critical FROM Artifact.Windows.System.Services() WHERE Critical AND State != \u0026quot;Running\u0026quot;    Windows.System.Pslist List processes and their running binaries.\n   Arg Default Description     processRegex .       View Artifact Source   name: Windows.System.Pslist description: | List processes and their running binaries. parameters: - name: processRegex default: . sources: - queries: - | SELECT Pid, Ppid, Name, CommandLine, Exe, hash(path=Exe) as Hash, authenticode(filename=Exe) AS Authenticode, Username, WorkingSetSize FROM pslist() WHERE Name =~ processRegex    Windows.System.SVCHost Typically a windows system will have many svchost.exe processes. Sometimes attackers name their processes svchost.exe to try to hide. Typically svchost.exe is spawned by services.exe.\nThis artifact lists all the processes named svchost.exe and their parents if the parent is not also named services.exe.\n  View Artifact Source   name: Windows.System.SVCHost description: | Typically a windows system will have many svchost.exe processes. Sometimes attackers name their processes svchost.exe to try to hide. Typically svchost.exe is spawned by services.exe. This artifact lists all the processes named svchost.exe and their parents if the parent is not also named services.exe. sources: - precondition: | SELECT OS From info() where OS = 'windows' queries: - | // Cache the pslist output in memory. LET processes \u0026lt;= SELECT * FROM pslist() - | // Get the pids of all procecesses named services.exe LET services \u0026lt;= SELECT Pid FROM processes where Name =~ \u0026quot;services.exe\u0026quot; - | // The interesting processes are those which are not spawned by services.exe LET suspicious = SELECT Pid As SVCHostPid, Ppid As SVCHostPpid, Exe as SVCHostExe, CommandLine as SVCHostCommandLine FROM processes WHERE Name =~ \u0026quot;svchost\u0026quot; AND NOT Ppid in services.Pid - | // Now for each such process we display its actual parent. SELECT * from foreach( row=suspicious, query={ SELECT SVCHostPid, SVCHostPpid, SVCHostExe, SVCHostCommandLine, Name as ParentName, Exe As ParentExe FROM processes WHERE Pid=SVCHostPpid })    Windows.System.Services List all the installed services.\n   Arg Default Description     servicesKeyGlob HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\       View Artifact Source   name: Windows.System.Services description: | List all the installed services. parameters: - name: servicesKeyGlob default: HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\ sources: - precondition: | SELECT OS From info() where OS = 'windows' queries: - | SELECT State, Name, DisplayName, Status, ProcessId as Pid, ExitCode, StartMode, PathName, ServiceType, StartName as UserAccount, { SELECT timestamp(epoch=Mtime.Sec) as Created FROM stat(filename=servicesKeyGlob + Name, accessor='reg') } AS Created, { SELECT ServiceDll FROM read_reg_key(globs=servicesKeyGlob + Name + \u0026quot;\\\\Parameters\u0026quot;) } AS ServiceDll FROM wmi(query=\u0026quot;SELECT * From Win32_service\u0026quot;, namespace=\u0026quot;root/CIMV2\u0026quot;)    Windows.System.UntrustedBinaries Windows runs a number of services and binaries as part of the operating system. Sometimes malware pretends to run as those well known names in order to hide itself in plain sight. For example, a malware service might call itself svchost.exe so it shows up in the process listing as a benign service.\nThis artifact checks that the common systems binaries are signed. If a malware replaces these files or names itself in this way their signature might not be correct.\nNote that unfortunately Microsoft does not sign all their common binaries so many will not be signed (e.g. conhost.exe).\n   Arg Default Description     processNamesRegex (?i)lsass svchost      View Artifact Source   name: Windows.System.UntrustedBinaries description: | Windows runs a number of services and binaries as part of the operating system. Sometimes malware pretends to run as those well known names in order to hide itself in plain sight. For example, a malware service might call itself svchost.exe so it shows up in the process listing as a benign service. This artifact checks that the common systems binaries are signed. If a malware replaces these files or names itself in this way their signature might not be correct. Note that unfortunately Microsoft does not sign all their common binaries so many will not be signed (e.g. conhost.exe). parameters: - name: processNamesRegex description: A regex to select running processes which we consider should be trusted. default: (?i)lsass|svchost|conhost|taskmgr|winlogon|wmiprv|dwm|csrss|velociraptor sources: - precondition: | SELECT OS From info() where OS = 'windows' queries: - | LET binaries = SELECT lowcase(string=Exe) As Binary FROM pslist() WHERE Exe =~ processNamesRegex GROUP BY Binary - | LET auth = SELECT authenticode(filename=Binary) As Authenticode FROM binaries - | SELECT Authenticode.Filename As Filename, Authenticode.IssuerName as Issuer, Authenticode.SubjectName as Subject, Authenticode.Trusted as Trusted from auth    "
},
{
        "uri": "/docs/getting-started/cloud/",
        "title": "Deploying in the cloud",
        "tags": [],
        "description": "",
        "content": " Many users choose to deploy Velociraptor in the cloud. This is a convenient and reliable option. One of the advantages of cloud deployment is the possibility of minting a proper SSL certificate using the free Letsencrypt CA. This eliminates the bad certificate browser warning seen in the Self Signed Deployment method. Velociraptor is able to use the Letsencrypt protocol to obtain and manage its own certificates (and automatically rotate them when they expire).\nIn this page we will explore how Velociraptor can be deployed in the cloud.\nGetting a domain name An SSL certificate says that the DNS name is owned by the server which presents it. Therefore SSL goes hand in hand with DNS. It is not currently possible to get a Letsencrypt certificate for an IP address.\nTherefore the first thing you need to do is to buy a DNS domain from any provider. Once there, you need to set up a DNS A Record to point at your Velociraptor server\u0026rsquo;s external IP. You can use a dynamic DNS client such as ddclient to update your DNS-\u0026gt;IP mapping dynamically. Alternative, Velociraptor directly supports updating Google Domains Dynamic DNS so this is the easiest option since it requires the least amount of configuration.\nIn this example we use Google Domains to purchase our domain, but any other domain provider would work as well.\nProvisioning a Virtual Machine Next we provision an Ubuntu VM from any cloud provider. Depending on your deployment size your VM should be large enough. An 8 or 16Gb VM should be sufficient for around 5-10k clients. Additionally we will need sufficient disk space to hold the data we will collect. We recommend to start with a modest amount of storage and then either backup data as it gets collected or increase the storage volume as needed.\nIt is essential to ensure that inbound filtering ACLs allow our virtual machine to receive connections over both ports 80 and 443. When using SSL both the client communication and the GUI are served over the same ports to benefit from SSL transport encryption. The Letsencrypt protocol requires Letsencrypt\u0026rsquo;s servers to connect to the VM on port 80 - however the GUI itself will only be served over SSL.\n When we deploy our Virtual Machine we may choose either a static IP address or allow the cloud provider to assign a dynamic IP address. We typically choose a dynamic IP address and so we need to configure Dynamic DNS.\nGoogle Domains allows us to assign a dynamic DNS entry for our domain by simply select a Dynamic DNS record:\nAfter the dynamic address is created, we can get the credentials for updating the IP address from the console. We will use these credentials in the ddclient configuration file on our server VM.\nWe will need these credential during the interactive configuration process below.\nConfigure Velociraptor to use autocert Velociraptor can issue its own certificates. Using the guided configuration wizard we may select this operation mode:\n$ ./output/velociraptor config generate -i ? Welcome to the Velociraptor configuration generator --------------------------------------------------- I will be creating a new deployment configuration for you. I will begin by identifying what type of deployment you need. [Use arrows to move, space to select, type to filter] Self Signed SSL \u0026gt; Automatically provision certificates with Lets Encrypt  By selecting this option the generated configuration file will ensure:\n The server will fetch certificates automatically from Letsencrypt\u0026rsquo;s servers when first accessed by the browser.\n Both the Frontend and GUI will be served over the standard SSL port (443).\n The GUI is therefore externally available (but protected over SSL) to the internet.\n Clients will connect to the public DNS name over SSL.\n  You must have both ports 80 and 443 publicly accessible by allowing any inbound firewall rules! Letsencrypt uses both to issue certificates. If you forgot to open port 80, Letsencrypt will fail to issue the certificate and this might result in blocking the domain name from getting an SSL certificate for several days. If you find this happened, simply change the dyndns name and start again.\n The first time you connect to the GUI or to the frontend, the server will obtain its own certificates from letsencrypt (it might take a couple of seconds to respond the first time). You should have no SSL warnings in your browser.\nConfiguring Google OAuth SSO In the previous sections we saw how to set up Velociraptor\u0026rsquo;s GUI over SSL. This is great, but we still need to create users and assign them passwords manually. The trouble with user account management is that we can not enforce 2 factor authentication, or any password policies or any of the usual enterprise requirements for user account management. It is also difficult for users to remember yet another password for a separate system, and so might make the password easily guessable.\nMost enterprise systems require an SSO mechanism to manage user accounts and passwords. Manual user account management simply does not scale!\nWe now discuss how to enable Google\u0026rsquo;s SSO authentication for Velociraptor identity management.\nOAuth Identity management Velociraptor can use Google\u0026rsquo;s oauth mechanism to verify a user\u0026rsquo;s identity. This requires a user to authenticate to Google via their usual mechanism - if their account requires 2 factor authentication, then users need to log in this way.\nOnce the user authenticates to Google, they are redirected back into the Velociraptor application with a token that allows the application to request information about the user (for example, the username or email address).\nOAuth is an authentication protocol. This means Velociraptor can be pretty confident the user is who they claim they are. This does not automatically grant them access to the application! A Velociraptor administrator must still manually grant them access before a user may log in.\n Before we can use Google for Authentication, we need to register our Velociraptor deployment as an OAuth App with Google.\nRegistering Velociraptor as an OAuth application The first step is to register Velociraptor as an OAuth app. We do this by accessing the Google cloud console at https://console.cloud.google.com . You will need to set up a cloud account first and create a cloud project - even if you do not host your server on Google\u0026rsquo;s cloud platform.\nOur ultimate goal is to obtain OAuth credentials to give our Velociraptor app, but we have to have a few things set up first. Navigate to APIs and Services in the GCP console and select Credentials and the OAuth Consent Screen tab.\nFurther down we need to provide an authorized domain\nIn order to add an Authorized domain we need to verify it. Google\u0026rsquo;s help pages explain it further:\nAuthorized domains: To protect you and your users, Google restricts your OAuth 2.0 application to using Authorized Domains. If you have verified the domain with Google, you can use any Top Private Domain as an Authorized Domain.\n In this example we assume that you purchased your domain with Google domains which makes this step easier since it is already verified.\nWe can go back to the cloud console and Create Credentials/OAuth client ID:\nNow select Web App and we must set the Authorized redirect URIs to https://\u0026lt;Your Domain Name\u0026gt;/auth/google/callback - This is the URL that successful OAuth authentication will redirect to. Velociraptor accepts this redirect and uses it to log the user on.\nIf all goes well the Google cloud console will give us a client ID and a client secret. Now we are ready to select Authenticate users with Google OAuth SSO from the guided installation wizard.\n$ velociraptor config generate -i ? Welcome to the Velociraptor configuration generator --------------------------------------------------- I will be creating a new deployment configuration for you. I will begin by identifying what type of deployment you need. Authenticate users with Google OAuth SSO Generating keys please wait.... ? What is the public DNS name of the Frontend (e.g. www.example.com): www.example.com ? Enter the Google OAuth Client ID? 1234xxxxxx.apps.googleusercontent.com ? Enter the Google OAuth Client Secret? qsadlkjhdaslkjasd ? Are you using Google Domains DynDNS? Yes ? Google Domains DynDNS Username XXXXXXXXX ? Google Domains DynDNS Password YYYYYYYYY ? Path to the datastore directory. (/tmp) ? GUI Username or email address to authorize (empty to end):mic ? GUI Username or email address to authorize (empty to end):  The DynDNS Username and Password field simply contain the values generated earlier by the Google Domains console.\nThe configuration allows you to add authorized usernames at this stage. Note that username authorization is written to the data store and so it will only work on the same machine we are deploying to. If you are creating the configuration on another machine, you will need to explicitly add users later as demonstrated below.\nNow we can start the Velociraptor frontend:\n$ velociraptor --config server.config.yaml frontend -v  Connecting using the browser goes through the familiar OAuth flow and arrives at this Velociraptor screen:\nThe OAuth flow ensures the user\u0026rsquo;s identity is correct but does not give them permission to log into Velociraptor. Note that having an OAuth enabled application on the web allows anyone with a Google identity to authenticate to the application but the user is still required to be authorized explicitly. If a user is rejected, we can see the following in the Audit logs:\n{ \u0026quot;level\u0026quot;: \u0026quot;error\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;msg\u0026quot;: \u0026quot;User rejected by GUI\u0026quot;, \u0026quot;remote\u0026quot;: \u0026quot;192.168.0.10:40570\u0026quot;, \u0026quot;time\u0026quot;: \u0026quot;2018-12-21T18:17:47+10:00\u0026quot;, \u0026quot;user\u0026quot;: \u0026quot;mike@velocidex.com\u0026quot; }  In order to authorize the user we must explicitly add them using the velociraptor admin tool:\n$ velociraptor --config ~/server.config.yaml user add mike@velocidex.com Authentication will occur via Google - therefore no password needs to be set.  Note that this time, Velociraptor does not ask for a password at all, since authentication occurs using Google\u0026rsquo;s SSO. If we hit refresh in the browser we should be able to see the Velociraptor application dashboard.\nWe can see that the logged in user is authenticated by Google, and we can also see the user\u0026rsquo;s Google avatar at the top right for some more eye candy :-).\nVelociraptor will retain its OAuth token for 24 hours. Each day users will need to re-grant OAuth credentials. Therefore revoking a user from the Google Admin console may take a full day to take effect. To remove access sooner you should use velociraptor --config server.config.yaml user lock MyUserName at the console.\n Deploying to the cloud We saw how to start the frontend manually, but in practice we would rather have it run as a service, so it can be started in case the node is rebooted.\nWe recommend that Velociraptor be deployed on an Ubuntu or Debian Linux based VM. We therefore provided an easy way to build a deb package:\n$ ./velociraptor --config ~/server.config.yaml debian server $ ls -l velociraptor_0.3.0_server.deb -rw-r--r-- 1 mic mic 12710596 Jul 7 01:35 velociraptor_0.3.0_server.deb  The produced deb package will install Velociraptor as a service and the config file we just generated. You simply need to copy the deb to the node and install it.\nYou need to ensure that directories you specify in the config file actually exist on the node. For example, we normally purchase a node with large attached storage (say 500Gb) to hold the collected artifacts. We then format the additional partition (using mkext2) and mount it on a specific directory (e.g. /data/). We then need to add the new partition to /etc/mtab to ensure it gets mounted on boot. This needs to happen before we start the velociraptor service or it will fail to start.\n You can check the installation using service velociraptor_server status.\n$ sudo dpkg -i velociraptor_0.3.0_server.deb Selecting previously unselected package velociraptor-server. (Reading database ... 384556 files and directories currently installed.) Preparing to unpack velociraptor_0.3.0_server.deb ... Unpacking velociraptor-server (0.3.0) ... Setting up velociraptor-server (0.3.0) ... Created symlink /etc/systemd/system/multi-user.target.wants/velociraptor_server.service → /etc/systemd/system/velociraptor_server.service. $ sudo service velociraptor_server status ● velociraptor_server.service - Velociraptor linux amd64 Loaded: loaded (/etc/systemd/system/velociraptor_server.service; enabled; vendor preset: enabled) Active: active (running) since Sun 2019-07-07 08:49:24 AEST; 17s ago Main PID: 2275 (velociraptor) Tasks: 13 (limit: 4915) Memory: 30.2M CGroup: /system.slice/velociraptor_server.service └─2275 /usr/local/bin/velociraptor --config /etc/velociraptor/server.config.yaml frontend Jul 07 08:49:24 mic-Inspiron systemd[1]: Started Velociraptor linux amd64.  You do not need to build the deb package on a Linux machine. If you prefer to use Windows for your day to day work, you may build the deb on your windows machine but you must specify the --binary flag to the debian server command with a path to the linux binary. You can obtain a copy of the linux binary from the Github releases page.\n "
},
{
        "uri": "/docs/artifacts/server/",
        "title": "Server Artifacts",
        "tags": [],
        "description": "These artifacts are intended to run on the server.",
        "content": " Server.Alerts.PsExec Send an email if execution of the psexec service was detected on any client. This is a server side artifact.\nNote this requires that the Windows.Event.ProcessCreation monitoring artifact be collected from clients.\n   Arg Default Description     EmailAddress admin@example.com    MessageTemplate PsExec execution detected at %v: %v for client %v\\n       View Artifact Source   name: Server.Alerts.PsExec description: | Send an email if execution of the psexec service was detected on any client. This is a server side artifact. Note this requires that the Windows.Event.ProcessCreation monitoring artifact be collected from clients. type: SERVER_EVENT parameters: - name: EmailAddress default: admin@example.com - name: MessageTemplate default: | PsExec execution detected at %v: %v for client %v sources: - queries: - | SELECT * FROM foreach( row={ SELECT * from watch_monitoring( artifact='Windows.Events.ProcessCreation') WHERE Name =~ '(?i)psexesvc' }, query={ SELECT * FROM mail( to=EmailAddress, subject='PsExec launched on host', period=60, body=format( format=MessageTemplate, args=[Timestamp, CommandLine, ClientId]) ) })    Server.Alerts.WinPmem Send an email if the pmem service has been installed on any of the endpoints.\nNote this requires that the Windows.Event.ServiceCreation monitoring artifact be collected from clients.\n   Arg Default Description     EmailAddress admin@example.com       View Artifact Source   name: Server.Alerts.WinPmem description: | Send an email if the pmem service has been installed on any of the endpoints. Note this requires that the Windows.Event.ServiceCreation monitoring artifact be collected from clients. type: SERVER_EVENT parameters: - name: EmailAddress default: admin@example.com sources: - queries: - | SELECT * FROM foreach( row={ SELECT * from watch_monitoring( artifact='Windows.Events.ServiceCreation') WHERE ServiceName =~ 'pmem' }, query={ SELECT * FROM mail( to=EmailAddress, subject='Pmem launched on host', period=60, body=format( format=\u0026quot;WinPmem execution detected at %s for client %v\u0026quot;, args=[Timestamp, ClientId] ) ) })    Server.Analysis.Triage.PowershellConsole This artifact post processes the artifact Windows.Triage.Collectors.PowershellConsoleLogs. While that artifact just uploads all the powershell console files, we sometimes want to easily see all the files in the same output table.\nThis artifact simply post processes the uploaded files and puts their content in the same table.\n   Arg Default Description     huntId        View Artifact Source   name: Server.Analysis.Triage.PowershellConsole description: | This artifact post processes the artifact Windows.Triage.Collectors.PowershellConsoleLogs. While that artifact just uploads all the powershell console files, we sometimes want to easily see all the files in the same output table. This artifact simply post processes the uploaded files and puts their content in the same table. type: SERVER parameters: - name: huntId precondition: SELECT * from server_config sources: - queries: - | LET files = SELECT ClientId, file_store(path=Flow.FlowContext.uploaded_files) as LogFiles FROM hunt_results( hunt_id=huntId, artifact='Windows.Triage.Collectors.PowershellConsoleLogs') # A lookup between client id and FQDN - | LET clients \u0026lt;= SELECT ClientId, os_info.fqdn AS FQDN from clients() - | SELECT * FROM foreach( row=files, query={ SELECT ClientId, { SELECT FQDN FROM clients where ClientId=ClientId_LU } As FQDN, Filename, Data FROM read_file(filenames=LogFiles) })    Server.Hunts.List List Hunts currently scheduled on the server.\n  View Artifact Source   name: Server.Hunts.List description: | List Hunts currently scheduled on the server. type: SERVER sources: - precondition: SELECT * from server_config queries: - | SELECT HuntId, timestamp(epoch=create_time/1000000) as Created, start_request.Args.artifacts.names as Artifact, State FROM hunts() WHERE start_request.flow_name = 'ArtifactCollector'    Server.Hunts.Results Show the results from each artifact collection hunt.\n   Arg Default Description     huntId H.d05b2482    ArtifactName Linux.Mounts       View Artifact Source   name: Server.Hunts.Results description: | Show the results from each artifact collection hunt. parameters: - name: huntId default: H.d05b2482 - name: ArtifactName default: Linux.Mounts type: SERVER sources: - precondition: SELECT * from server_config queries: - | SELECT * FROM hunt_results(hunt_id=huntId, artifact=ArtifactName)    Server.Information.Clients This artifact returns the total list of clients, their hostnames and the last times they were seen.\nWe also include a list of usernames on this machine, as gathered by the last Windows.Sys.Users artifact that was collected. Note that the list of usernames may be outdated if that artifact was not collected recently.\n  View Artifact Source   name: Server.Information.Clients description: | This artifact returns the total list of clients, their hostnames and the last times they were seen. We also include a list of usernames on this machine, as gathered by the last Windows.Sys.Users artifact that was collected. Note that the list of usernames may be outdated if that artifact was not collected recently. type: SERVER sources: - queries: - | /* Collect information about each client. */ LET client_info = SELECT client_id, os_info.fqdn as HostName, os_info.system as OS, os_info.release as Release, timestamp(epoch=last_seen_at/ 1000000).String as LastSeenAt, last_ip AS LastIP, last_seen_at AS _LastSeenAt FROM clients() ORDER BY _LastSeenAt DESC - | LET names = SELECT Name FROM Artifact.Server.Information.Users( ClientId=client_id) - | /* For each client, also list its users. */ SELECT client_id, HostName, OS, Release, LastSeenAt, LastIP, join(array=names.Name, sep=\u0026quot;,\u0026quot;) AS Users FROM client_info    Server.Information.Users List the user names and SIDs on each machine. We get this information from the last time we collected Windows.Sys.Users. If we never collected it for this machine, there will be no results.\n   Arg Default Description     ClientId None    StandardUserAccounts (-5..$ S-1-5-18      View Artifact Source   name: Server.Information.Users description: | List the user names and SIDs on each machine. We get this information from the last time we collected Windows.Sys.Users. If we never collected it for this machine, there will be no results. type: SERVER parameters: - name: ClientId default: - name: StandardUserAccounts description: Well known SIDs to hide from the output. default: \u0026quot;(-5..$|S-1-5-18|S-1-5-19|S-1-5-20)\u0026quot; sources: - queries: - | // Get the most recent collection of our user listing. LET last_user_listing = SELECT flow_id FROM flows(client_id=ClientId) WHERE context.artifacts =~'Windows.Sys.Users' ORDER BY LastActive DESC LIMIT 1 - | /* For each Windows.Sys.Users collection, extract the user names. Hide standard SIDs. */ SELECT * FROM foreach( row=last_user_listing, query={ SELECT Name, UUID from source( flow_id=flow_id, artifact='Windows.Sys.Users', client_id=ClientId) WHERE NOT UUID =~ StandardUserAccounts })    Server.Internal.ArtifactDescription   View Artifact Source   name: Server.Internal.ArtifactDescription reports: - type: INTERNAL template: | {{ $artifact := Scope \u0026quot;artifact\u0026quot; }} ## {{ $artifact.Name }} #### Type: {{ $artifact.Type }} {{ $artifact.Description }} {{ if $artifact.Parameters }} ### Parameters \u0026lt;table class=\u0026quot;table table-striped table-hover\u0026quot;\u0026gt; \u0026lt;thead\u0026gt;\u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Default\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt;\u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; {{ range $item := $artifact.Parameters }} \u0026lt;tr\u0026gt;\u0026lt;td\u0026gt; {{ $item.Name }}\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;pre\u0026gt;{{ $item.Default }}\u0026lt;/pre\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; {{ end }} \u0026lt;/tbody\u0026gt;\u0026lt;/table\u0026gt; {{ end }} {{ range $source := $artifact.Sources }} ### Source {{ $source.Name }} ```sql {{ range $query := $source.Queries -}} {{- $query -}} {{ end }} ``` {{ end }}    Server.Monitor.Health This is the main server health dashboard. It is shown on the homescreen and enabled by default on all new installs.\n   Arg Default Description     Frequency 15 Return stats every this many seconds.      View Artifact Source   name: Server.Monitor.Health description: | This is the main server health dashboard. It is shown on the homescreen and enabled by default on all new installs. type: SERVER_EVENT parameters: - name: Frequency description: Return stats every this many seconds. default: \u0026quot;15\u0026quot; sources: - name: Prometheus queries: - | LET metrics_url \u0026lt;= SELECT format(format='http://%s:%d/metrics', args=[ server_config.Monitoring.bind_address, server_config.Monitoring.bind_port]) as URL FROM scope() - | SELECT int(int=rate(x=process_cpu_seconds_total, y=Timestamp) * 100) As CPUPercent, process_resident_memory_bytes / 1000000 AS MemoryUse, process_cpu_seconds_total, client_comms_current_connections, client_comms_concurrency FROM foreach( row={ SELECT UnixNano FROM clock(period=atoi(string=Frequency)) }, query={ SELECT * FROM Artifact.Server.Monitor.VeloMetrics(MetricsURL=metrics_url.URL[0]) }) WHERE CPUPercent \u0026gt;= 0 reports: - type: SERVER_EVENT parameters: - name: Sample default: \u0026quot;4\u0026quot; template: | {{ define \u0026quot;CPU\u0026quot; }} SELECT * FROM sample( n=atoi(string=Sample), query={ SELECT _ts as Timestamp, CPUPercent, MemoryUse FROM source(source=\u0026quot;Prometheus\u0026quot;, artifact=\u0026quot;Server.Monitor.Health\u0026quot;) }) {{ end }} {{ define \u0026quot;CurrentConnections\u0026quot; }} SELECT * FROM sample( n=atoi(string=Sample), query={ SELECT _ts as Timestamp, client_comms_current_connections, client_comms_concurrency FROM source(source=\u0026quot;Prometheus\u0026quot;, artifact=\u0026quot;Server.Monitor.Health\u0026quot;) }) {{ end }} {{ $CurrentMetrics := Query \u0026quot;SELECT * FROM Artifact.Server.Monitor.VeloMetrics()\u0026quot; }} ## Server status Currently there are {{ Get $CurrentMetrics \u0026quot;0.client_comms_current_connections\u0026quot; }} clients connected. \u0026lt;span class=\u0026quot;container\u0026quot;\u0026gt; \u0026lt;span class=\u0026quot;row\u0026quot;\u0026gt; \u0026lt;span class=\u0026quot;col-sm panel\u0026quot;\u0026gt; CPU and Memory Utilization {{ Query \u0026quot;CPU\u0026quot; | LineChart \u0026quot;xaxis_mode\u0026quot; \u0026quot;time\u0026quot; \u0026quot;RSS.yaxis\u0026quot; 2 }} \u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;col-sm panel\u0026quot;\u0026gt; Currently Connected Clients {{ Query \u0026quot;CurrentConnections\u0026quot; | LineChart \u0026quot;xaxis_mode\u0026quot; \u0026quot;time\u0026quot; \u0026quot;RSS.yaxis\u0026quot; 2 }} \u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;    Server.Monitor.Shell Velociraptor can get an interactive shell on the endpoint by using the shell command. In order to use it, the user must be directly logged on the server.\nObviously being able to run arbitrary commands on the end point is a powerful feature and should be used sparingly. There is an audit trail for shell commands executed and their output available by streaming all shell commands to the \u0026ldquo;Shell\u0026rdquo; client event monitoring artifact.\nThis server event artifact centralizes all shell access from all clients into the same log file.\n  View Artifact Source   name: Server.Monitor.Shell description: | Velociraptor can get an interactive shell on the endpoint by using the shell command. In order to use it, the user must be directly logged on the server. Obviously being able to run arbitrary commands on the end point is a powerful feature and should be used sparingly. There is an audit trail for shell commands executed and their output available by streaming all shell commands to the \u0026quot;Shell\u0026quot; client evnt monitoring artifact. This server event artifact centralizes all shell access from all clients into the same log file. # Can be CLIENT, EVENT, SERVER, SERVER_EVENT type: SERVER_EVENT sources: - queries: - | SELECT * FROM watch_monitoring(artifact=\u0026quot;Shell\u0026quot;) # Reports can be MONITORING_DAILY, CLIENT reports: - type: SERVER_EVENT template: | {{ .Description }} {{ $rows := Query \u0026quot;SELECT timestamp(epoch=Timestamp) AS Timestamp, Argv, Stdout FROM source()\u0026quot; }} {{ range $row := $rows }} * On {{ Get $row \u0026quot;Timestamp\u0026quot; }} we ran {{ Get $row \u0026quot;Argv\u0026quot; }} ```text {{ Get $row \u0026quot;Stdout\u0026quot; }} ``` {{end}}    Server.Monitor.VeloMetrics Get Velociraptor server metrics.\n   Arg Default Description     MetricsURL http://localhost:8003/metrics       View Artifact Source   name: Server.Monitor.VeloMetrics description: | Get Velociraptor server metrics. parameters: - name: MetricsURL default: http://localhost:8003/metrics sources: - queries: - | LET stats = SELECT parse_string_with_regex(string=Content, regex=[ 'client_comms_concurrency (?P\u0026lt;client_comms_concurrency\u0026gt;[^\\\\s]+)', 'client_comms_current_connections (?P\u0026lt;client_comms_current_connections\u0026gt;[^\\\\s]+)', 'flow_completion (?P\u0026lt;flow_completion\u0026gt;[^\\\\s]+)', 'process_open_fds (?P\u0026lt;process_open_fds\u0026gt;[^\\\\s]+)', 'stats_client_one_day_actives{version=\u0026quot;[^\u0026quot;]+\u0026quot;} (?P\u0026lt;one_day_active\u0026gt;[^\\\\s]+)', 'stats_client_seven_day_actives{version=\u0026quot;[^\u0026quot;]+\u0026quot;} (?P\u0026lt;seven_day_active\u0026gt;[^\\\\s]+)' ]) AS Stat, { // On Windows Prometheus does not provide these so we get our own. SELECT Times.user + Times.system as CPU, MemoryInfo.RSS as RSS FROM pslist(pid=getpid()) } AS PslistStats FROM http_client(url=MetricsURL, chunk_size=50000) - | SELECT now() AS Timestamp, PslistStats.RSS AS process_resident_memory_bytes, parse_float(string=Stat.client_comms_concurrency) AS client_comms_concurrency, parse_float(string=Stat.client_comms_current_connections) AS client_comms_current_connections, parse_float(string=Stat.flow_completion) AS flow_completion, parse_float(string=Stat.process_open_fds) AS process_open_fds, PslistStats.CPU AS process_cpu_seconds_total, parse_float(string=Stat.one_day_active) AS one_day_active, parse_float(string=Stat.seven_day_active) AS seven_day_active FROM stats    Server.Monitoring.ClientCount An artifact that sends an email every hour of the current state of the deployment.\n   Arg Default Description     EmailAddress admin@example.com    CCAddress None    Subject Deployment statistics for Velociraptor    Period 3600       View Artifact Source   name: Server.Monitoring.ClientCount description: | An artifact that sends an email every hour of the current state of the deployment. type: SERVER_EVENT parameters: - name: EmailAddress default: admin@example.com - name: CCAddress default: - name: Subject default: \u0026quot;Deployment statistics for Velociraptor\u0026quot; - name: Period default: \u0026quot;3600\u0026quot; sources: - queries: - | LET metrics = SELECT * FROM Artifact.Server.Monitor.VeloMetrics() - | SELECT * FROM foreach( row={ SELECT * FROM clock(period=atoi(string=Period)) }, query={ SELECT * FROM mail( to=EmailAddress, cc=CCAddress, subject=Subject, period=60, body=format(format='Total clients currently connected %v', args=[metrics.client_comms_current_connections]) ) })    Server.Powershell.EncodedCommand It is possible to pass powershell an encoded script. This artifact decodes the scripts.\nNOTE: The client must be running the Windows.Events.ProcessCreation event artifact to retrieve process execution logs.\n  View Artifact Source   name: Server.Powershell.EncodedCommand description: | It is possible to pass powershell an encoded script. This artifact decodes the scripts. NOTE: The client must be running the Windows.Events.ProcessCreation event artifact to retrieve process execution logs. type: SERVER_EVENT sources: - queries: - | SELECT ClientId, ParentInfo, CommandLine, Timestamp, utf16( string=base64decode( string=parse_string_with_regex( string=CommandLine, regex='-encodedcommand (?P\u0026lt;Encoded\u0026gt;[^ ]+)' ).Encoded)) AS Script FROM watch_monitoring(artifact='Windows.Events.ProcessCreation') WHERE CommandLine =~ '-encodedcommand' reports: - type: SERVER_EVENT template: | Encoded Powershell ================== {{ .Description }} ## Decoded Powershell commands. {{ Query \u0026quot;SELECT ClientId, { SELECT os_info.Fqdn from clients(client_id=ClientId) } AS FQDN, Script FROM source()\u0026quot; | Table }}    "
},
{
        "uri": "/docs/user-interface/artifacts/server_artifacts/",
        "title": "Server Artifacts",
        "tags": [],
        "description": "",
        "content": " We have seen how Velociraptor\u0026rsquo;s VQL language is used to query clients for artifacts relevant to our investigation. It is also possible to run VQL queries on the server in much the same way. On the server, VQL exposes functionality making it possible to manage the server deployment, as well as post process and correlate results from the entire deployment.\nIn exactly the same way, server side VQL queries are encapsulated in Server Artifacts.\nServer Artifacts run once and produce a fixed output at an instance in time. They are not the same as Server Event Artifacts which run continuously and generate daily event logs.\n Since server artifacts do not themselves collect any information from the client, typically Server Artifacts are used to post process client collections in some way or to perform administrative tasks.\nRunning a Server Artifact To run a Server Artifact select the Server Artifact option in the side navigation bar, then click the Collect More Artifacts button.\nIn the above example we will collect the Server.Information.Clients artifact which just lists all the clients, and the users associated with them (This is an example of a server artifact which post processes the Windows.Sys.Users artifact)\nWe can now download the list of all clients, and some information about each client as obtained at this instance in time.\n"
},
{
        "uri": "/docs/use_cases/",
        "title": "Use Cases",
        "tags": [],
        "description": "",
        "content": "Since Velociraptor is a very flexible tool, it can help you in many situations. It is therefore useful to look at some of the common use cases.\n"
},
{
        "uri": "/docs/vql_reference/server/",
        "title": "VQL Server Plugins",
        "tags": [],
        "description": "",
        "content": " VQL Queries can also be run on the server. When run on the server, VQL Queries have access to server state, and so can schedule new artifact collection flows on client, manage client label, and retrieve artifact results from hunts. It is therefore possible to post process artifacts on the server in arbitrary ways.\nServer VQL queries can also be event driven which allows one to set up higher order escalations and alerting based on client side events.\nclients    Arg Description Type     search Client search string. Can have the following prefixes: \u0026lsquo;label:\u0026lsquo;, \u0026lsquo;host:\u0026rsquo; string   client_id  string    This plugin returns all clients retrieved by the search term. The search term is the same as in the GUI and may consist of:\n a plain word searches for a host name May contain wild cards (my*hostname) May contain a prefix such as host:, label:, user:  elastic    Arg Description     query A delegate query to run. Rows from this query will be inserted into elastic.   threads How many uploader threads to use.   index The name of the elastic index to use.   type The name of the elastic type to use.    This plugin uploads rows into Elastic Search. We do not define the index, so if it is not already defined, Elastic will define it as a default index.\nThis plugin is experimental.\nThis plugin is currently disabled because it seems to have an extremely large binary footprint. You can enable it and recompile the binary if you need it.\n flows    Arg Description Type     client_id  list of string (required)    Returns a list of flows launched on the client by client id.\nhunt_flows    Arg Description Type     hunt_id The hunt id to inspect. string (required)    Retrieve the flows launched by a hunt. This is useful to quickly identified which client returned results, without necessarily counting them. Flow object contains high level context about the flow execution.\nhunt_results    Arg Description Type     artifact The artifact to retrieve string (required)   source An optional source within the artifact. string   hunt_id The hunt id to read. string (required)   brief If set we return less columns. bool    This plugin returns the results from a hunt. Since hunts may collect multiple artifacts at the same time, the plugin only retrieves one artifact at the time. The returned rows have the flow_id and client_id appended (since hunt results come from multiple clients).\nThis plugin is very useful to see the results from a hunt. It always gets the most recent results so as the hunt progresses, there will be more rows returned.\nThe plugin is useful for performing stacking (using group by) or further narrowing the hunt result on demand.\nhunts Returns all the hunts scheduled in the system.\nmail    Arg Description Type     subject The subject. string   body The body of the mail. string (required)   period How long to wait before sending the next mail - help to throttle mails. int64 (required)   to Recipient of the mail list of string (required)   cc A cc for the mail list of string    This plugin sends a mail. In order to use it you must have the Mail section configured in the server\u0026rsquo;s config file.\nUsually you would use the foreach() plugin to send a mail from another event query.\nMails will not be sent more frequently than the specified period. Most mail servers implement rate limiting or spam detection so if this is set too low it is possible to overload the server. Currently we do not batch messages, rather drop them if sent too quickly.\nsearch    Arg Description Type     type The type of search (e.g. \u0026lsquo;key\u0026rsquo;) string   query The query string. string   offset Skip this many results. uint64   limit Only return limited results uint64    Search the server client\u0026rsquo;s index.\nsource    Arg Description Type     start_time Start return events from this date (for event sources) int64   mode HUNT or CLIENT mode can be empty string   flow_id A flow ID (client or server artifacts) string   hunt_id Retrieve sources from this hunt (combines all results from all clients) string   artifact The name of the artifact collection to fetch string   source An optional named source within the artifact string   client_id The client id to extract string   day_name Only extract this day\u0026rsquo;s Monitoring logs (deprecated) string   end_time Stop end events reach this time (event sources). int64    This is the main plugin to use in server VQL to fetch results. It automatically figures out what type the artifact is and where the CSV files are. The plugin can take many of these parameters from the report context so when used in a report it usually needs very few actual arguments.\nwatch_monitoring    Arg Description     client_id The client to use. If not specified we search all clients.   artifact The artifact to retrieve   date_regex A regular expression applied to the date file.    This plugin is similar to the monitoring() plugin but it is an event plugin watching the specified artifact file instead. When an event is stored in the monitoring CSV file it is also relayed to this plugin (We are essentially tailing the monitoring CSV files).\nThis allows us to implement an event query which watches for client side events and acts on them (e.g. by escalating or responding).\nIf the client id is not specified, we watch for events from all clients.\n"
},
{
        "uri": "/docs/user-interface/artifacts/",
        "title": "Velociraptor Artifacts",
        "tags": [],
        "description": "",
        "content": " Velociraptor\u0026rsquo;s main job is to collect Artifacts, but what is an Artifact?. An artifact is simply a yaml file which tells Velociraptor how to collect a set of files or information in order to answer a specific question.\nBefore we can discuss how artifacts are used within Velociraptor, we need to understand what Artifacts are and how they relate of Velociraptor.\nArtifact definitions Artifacts are supposed to be defined and tweaked by the user. Therefore they are defined using YAML in a simple human readable file format.\nBelow is an example of a typical artifact definition.\nname: Windows.Sys.Users description: | List User accounts that were logged into the machine in the past by searching for registry artifacts. What local or domain users have previously logged into an endpoint? parameters: - name: remoteRegKey default: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\ProfileList\\* sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - SELECT \u0026quot;\u0026quot; as Uid, \u0026quot;\u0026quot; as Gid, lookupSID( sid=basename(path=Key.FullPath) ) as Name, Key.FullPath as Description, ProfileImagePath as Directory, basename(path=Key.FullPath) as UUID, Key.Mtime.Sec as Mtime, \u0026quot;roaming\u0026quot; as Type FROM read_reg_key(globs=remoteRegKey, accessor=\u0026quot;reg\u0026quot;) reports: - type: CLIENT template: | Users that logged in previously. =============================== {{ .Description }} The following table shows basic information about the users on this system. {{ Query \u0026quot;users\u0026quot; \u0026quot;SELECT Name, UUID, Type, Mtime FROM source()\u0026quot; | Table }}  We can see the main sections:\n The name of the artifact is a dot separated string used to identify the Artifact in the UI. We typically name the artifact using a hierarchical category based naming scheme.\n The description section contains a human readable description of the purpose of this artifact, how it works and when to use it. The description section is searchable in the GUI so you should provide enough context there to assist a user in selecting this artifact.\n The parameters section is a list of parameters provided to the artifact. When the user selects this artifact in the GUI, they are also given the option to tweak these parameters. Parameters may also specify a default value and a helpful description to help users set the correct value.\n The sources section contains a list of evidence sources. Each source specifies a series of VQL queries. The queries may retrieve specific information or files.\n A precondition is a VQL query which must be satisfied before the source is collected. This is typically used to limit an artifact to a specific operating system or version.\n The queries section is a list of VQL queries, executed one at the time, which produce a single result set (i.e. a table with specified columns and rows). Typically the queries section consists of a list of LET VQL statements followed by a single SELECT.\n  Finally the reports section specifies a set of report templates to be used to analyze the results collected from the artifact. You can read more about report templates.\n  At a high level, an artifact answers a specific question. As an investigator we ask questions relevant to our case, and the artifact maps these questions to a mechanical collection providing sufficient evidence to cast light on our question. The Velociraptor GUI allows one to search artifacts by their description section.\n Reports The artifact contains a report that helps the user make sense of the collected evidence and provides simple post processing capabilities. Reports are templates that are evaluated on the artifact\u0026rsquo;s results and produce simple markdown, graphs, tables and other primitives. Reports may issue VQL statements to further analyze the collected data and therefore may produce any output.\nThe report presents a human readable post processing on the collected artifact - collating and correlating evidence from multiple sources in order to answer the high level question posed by the artifact.\nThe purpose of the artifact is to encapsulate expert knowledge into the artifact to both document and guide investigators through the investigation process. Even experienced investigators can benefit from artifacts, since they do not need to worry about forgetting to collect a particular source, or wrongly interpreting some of its finding.\nIn the above example, the high level question is What domain users have logged into this endpoint?. To answer this question we extract registry artifacts created whenever a user gains an interactive logon session to a machine. The report helps us to understand what the registry artifacts actually mean. We can see the report can run further VQL queries to highlight or post process the results, perhaps drawing our attention to particularly interesting findings.\nArtifact Types Velociraptor uses VQL for many different purposes. Since Artifacts are a nice way to package VQL queries, there are a number of different types of artifacts depending on the specific VQL contained within them.\nFor a full reference of VQL see VQL Reference, but for now we just need to distinguish between two main types of VQL queries:\n A Collection Query is a query which runs once and collects a table of results, then terminates.\n An Event Query is a query which runs forever, waiting for some events to occur. When the event occurs, the query will emit one or more rows and continue waiting. Output from Event Queries is streamed for as long as the query continues running.\n  Therefore we have 4 types of artifacts:\n A client collection artifact encapsulates VQL queries primarily designed to run on the endpoint and return a table of results. These are typically used to capture some piece of information from the host - for example, the list of installed programs, the presence of a registry key etc.\n A client event artifact encapsulates Event Queries that are running on the client, streaming rows to the server. These are typically used to monitor for specific events on the client. For example, watching the event log for a new event of interest.\n A server collection artifact is an artifact that contains a Collection Query that is designed to run on the server. Typically these artifacts are used to perform some post processing on the server or provide server state information.\n A server event artifact is an artifact containing event queries permanently running on the server. These are typically used to watch the entire Velociraptor deployment for specific conditions. For example, a server event artifact might monitor process execution logs from all clients and automatically decode encoded powershell command lines, alerting on suspicious occurrences.\n  "
},
{
        "uri": "/docs/user-interface/investigating_clients/virtual_filesystem/",
        "title": "The Virtual File System",
        "tags": [],
        "description": "",
        "content": " If we had to investigate a machine interactively we would probably start off by using Windows Explorer or a similar tool to navigate the endpoint\u0026rsquo;s filesystem directly. It is convenient and intuitive!\nVelociraptor provides a similar feature - the client\u0026rsquo;s Virtual File System. This feature mirrors some of the endpoint\u0026rsquo;s files and directories on the endpoint on the server and allows user to navigate through those interactively.\nAfter searching for a client, and selecting it you will see the option Virtual Filesystem available in the side navigation bar.\nThe interface is divided into three main parts:\n The left side shows a tree view of the Virtual File System and its directories. The top pane shows the file listings contained within each directory. The bottom pane shows information about files selected in the file listing pane.  File operations Selecting a directory in the tree view will populate the file listing pane with cached information stored on the server\u0026rsquo;s. You can see the time when that listing was actually taken from the endpoint at the top of the table.\nSince we can only show the information we have cached on the server, we may not have data for a directory on the end point we have never navigated to previously.\nTo refresh the server\u0026rsquo;s cache you can click the \u0026ldquo;Refresh this directory\u0026rdquo; button. This will schedule a directory listing on the client and refresh the server\u0026rsquo;s cache.\nIt is also possible to refresh directories recursively by clicking the \u0026ldquo;Recursively Refresh directory\u0026rdquo; button.\nIt may be tempting to just recursively refresh the entire endpoint\u0026rsquo;s filesystem but can take a long time and download a lot of data. Nevertheless it may be convenient sometimes. Note that queries have a 10 minute timeout by default so it may not completely cover the entire filesystem in this time either.\n Downloading files Sometimes we can see a file in the file listing pane and want to view it. Since the GUI only shows information cached on the server, the file contents are not immediately available to us.\nClicking on the \u0026ldquo;Collect From Client\u0026rdquo; button will schedule a file collection from the endpoint (if the client is currently connected the file will be download immediately).\nYou may now download the file to your computer by clicking the download button. Alternately you can view a hexdump or text dump of the file using the relevant tabs.\nFilesystem Accessors Many VQL plugins operate on files. However how we read files on the endpoint can vary - depending on what we actually mean by file. For example, Velociraptor is able to read files parsed from the NTFS parser, compressed files within Zip archives, or even files downloaded from a URL. VQL specifies the way a file is read via an accessor (essentially a file access driver).\nThe Virtual File System make a number of common accessors available for navigation, by specifying them at the top level of the tree view:\n The file accessor uses the normal OS filesystem APIs to access files and directories.\n The limitations with this method is that some files are locked if they are in use and we are not able to read them. For example, the registry hives or the page file.  The ntfs accessor uses Velociraptor\u0026rsquo;s built in NTFS parser to extract directory information and file contents.\n This bypasses the normal file locking mechanism and allows us to download and read locked files like registry hives.  The registry accessor uses the OS APIs to view the registry as a filesystem. You can use this to navigate the endpoint\u0026rsquo;s registry hives interactively.\n Since registry values are typically small, Velociraptor also gets the values in the directory listing as well, so it is not usually necessary to download files from the registry hive.   Automation While it is intuitive to interactively examine an endpoint using the Virtual File System we typically need something a bit more automated.\nVelociraptor uses Artifacts to encapsulate and automate endpoint analysis. You can read more about Client Artifacts.\n"
},
{
        "uri": "/docs/user-interface/artifacts/server_events/",
        "title": "Server Event Artifacts",
        "tags": [],
        "description": "",
        "content": " We have seen how client event VQL queries are run on endpoints and stream client side events to the server. In a similar way, event queries can run on the server to watch for events server side.\nTypically server event artifacts monitor the server for the occurrence of some conditions. When these conditions are detected, the artifact will emit a row.\nSelecting Server Events in the UI shows a similar screen to the Client Events selection, except that we are now looking at server monitoring artifacts.\nIn the above example, we see the Server.Powershell.EncodedCommands artifact. This artifact monitors the process execution logs from all clients (as collected by the Windows.Events.ProcessCreation client monitoring artifact) and when a Powershell process with encoded scripts are run, it simply decodes the script.\nPowershell is a common tool used by many adversaries for lateral movement. Typically attackers encode their malicious scripts so they can be sent over remote connections. This makes it difficult to see what the command will do when looking at a plain process execution log. The Server.Powershell.EncodedCommands simply decodes these for further inspection.\n Updating server event monitoring The Velociraptor server maintains a list of server event artifacts to monitor. You can update this list by clicking the \u0026ldquo;Update server monitoring artifacts\u0026rdquo; button.\nYou can search for server monitoring artifacts to add. Highlighting an existing artifact will display its description and you may also add any potential parameters.\nIn the example above we are adding the Admin.System.CompressUploads artifact which compresses uploads as soon as they are collected from endpoints in order to keep server disk usage to a minimum.\nThe Admin.System.CompressUploads artifact is an example of a server event monitoring artifact specifically designed to post process a specific type of collected files. In this case we watch for the collection flow to complete, then simply compress all files uploaded. However one can think of similar post processing scenarios such as parsing file of a particular type, or uploading the files to virus reputation sites (e.g. Virus Total). Your imagination is really the limit of what can be done using this mechanism.\n "
},
{
        "uri": "/docs/vql_reference/functions/",
        "title": "VQL Functions",
        "tags": [],
        "description": "",
        "content": " VQL Functions operate on value to return other values. Functions are useful to transform values obtained from a VQL row.\nVQL plugins are not the same as VQL functions. A plugin is the subject of the VQL query - i.e. plugins always follow the FROM keyword, while functions (which return a single value instead of a sequence of rows) are only present in column specification (e.g. after SELECT) or in condition clauses (i.e. after the WHERE keyword).\n array This function accepts arbitrary arguments and creates an array by flattening the arguments. For example array(a=1, b=2) will return [1, 2].\nYou can use this to flatten a subquery as well:\nSELECT array(a1={ SELECT User FROM Artifact.Windows.System.Users() }) as Users FROM scope()  Will return a single row with Users being an array of names.\natoi    Arg Description Type     string A string to convert to int string (required)    Converts a string to an integer.\nauthenticode    Arg Description Type     filename The filename to parse. string (required)    Uses the Windows API to extract and verify the file\u0026rsquo;s authenticode signature. Since we use the windows API this can only work with the \u0026ldquo;file\u0026rdquo; accessor.\nbase64decode    Arg Description Type     string A string to decode string (required)    Decodes a base64 encoded string.\nbase64encode    Arg Description Type     string A string to encode string (required)    Encodes a binary string to base64.\nbasename    Arg Description Type     path Extract directory name of path string (required)    Splits the path on separator and return the basename.\nbinary_parse    Arg Description Type     offset Start parsing from this offset. int64   string The string to parse. string (required)   profile The profile to use. string   iterator An iterator to begin with. string   target The target type to fetch. string    Parse a binary string with profile based parser.\nThis plugin extract binary data from strings. It works by applying a profile to the binary string and generating an object from that. Profiles use the same syntax as Rekall or Volatility. For example a profile might be:\n{ \u0026quot;StructName\u0026quot;: [10, { \u0026quot;field1\u0026quot;: [2, [\u0026quot;unsigned int\u0026quot;]], \u0026quot;field2\u0026quot;: [6, [\u0026quot;unsigned long long\u0026quot;]], }] }  The profile is compiled and overlayed on top of the offset specified, then the object is emitted with its required fields.\nExample: velociraptor query 'select binary_parse(profile=profile, string=\u0026quot;hello world\u0026quot;, target=\u0026quot;X\u0026quot;, offset=2) as Item from scope()' --env profile='{\u0026quot;X\u0026quot;:[10,{\u0026quot;field1\u0026quot;:[0,[\u0026quot;unsigned short\u0026quot;]]}]}' [ { \u0026quot;Item\u0026quot;: { \u0026quot;field1\u0026quot;: \u0026quot;27756\u0026quot; } } ]  collect Launch an artifact collection against a client.\n   Arg Description Type     client_id The client id to schedule a collection on string (required)   artifacts A list of artifacts to collect list of string (required)   env Parameters to apply to the artifacts vfilter.Any    compress    Arg Description Type     path A VFS path to compress list of string (required)    Compress a file in the server\u0026rsquo;s FileStore. A compressed file is archived so it takes less space. It is still possible to see the file and read it but not to seek within it.\ncount Counts the items.\n   Arg Description Type     items  vfilter.Any (required)    dict Construct a dict from arbitrary keyword args. The dictionary can be referenced later by VQL expressions.\ndirname    Arg Description     path The path to use    Splits the path on separator and return the directory name.\nencode    Arg Description Type     string  vfilter.Any (required)   type  string (required)    Encodes a string as as different type. Currently supported types include \u0026lsquo;hex\u0026rsquo;, \u0026lsquo;base64\u0026rsquo;.\nenviron    Arg Description Type     var Extract the var from the environment. string (required)    Returns the value of the environment variable specified.\nexpand    Arg Description Type     path A path with environment escapes string (required)    This function expands environment variables into the path. It is normally needed after using registry values of type REG_EXPAND_SZ as they typically contain environment strings. Velociraptor does not automatically expand such values since environment variables typically depend on the specific user account which reads the registry value (different user accounts can have different environment variables).\nfile_store    Arg Description Type     path A VFS path to convert list of string (required)    Resolves file store paths into full filesystem paths. This function is only available on the server. It can be used to find the backing file behind a filestore path so it can be passed on to an external program.\nVelociraptor uses the concept of a Virtual File System to manage the information about clients etc. The VFS path is a path into the file store. Of course ultimately (at least in the current implementation) the file store is storing files on disk, but the disk filename is not necessarily the same as the VFS path (for example non representable characters are escaped).\nYou can use the file_store() function to return the real file path on disk. This probably only makes sense for VQL queries running on the server which can independently open the file.\nIn future the file store may be abstracted (e.g. files may not be locally stored at all) and this function may stop working.\nfilter    Arg Description Type     list A list of items too filter list of string (required)   regex A regex to test each item list of string (required)    Returns another array filtered by the regular expression.\nformat    Arg Description Type     format Format string to use string (required)   args An array of elements to apply into the format string. Any    Format one or more items according to a format string. The format string is interpreted using the standard golang fmt package.\nThe function returns a string.\nget    Arg Description Type     item  vfilter.Any (required)   member  string (required)    Gets the member field from item. This is useful to index an item from an array. For example:\nExample select get(item=[dict(foo=3), 2, 3, 4], member='0.foo') AS Foo from scope() [ { \u0026quot;Foo\u0026quot;: 3 } ]  getpid Returns Velociraptor\u0026rsquo;s own pid.\ngrep    Arg Description Type     path path to open. string (required)   accessor An accessor to use. string   keywords Keywords to search for. list of string (required)   context Extract this many bytes as context around hits. int    Search a file for keywords.\nhash    Arg Description Type     path Path to open and hash. string (required)   accessor The accessor to use string    This function calculates the MD5, SHA1 and SHA256 hashes of the file.\nhumanize    Arg Description Type     bytes Format bytes with units int64    Formats a byte count in human readable way (e.g. Mb, Gb etc).\nif    Arg Description     condition A condition (either a value or a subquery)   then a value if the condition is true   else a value if the condition is false    This function evaluates a condition. Note that the values used in the then or else clause are evaluated lazily. They may be expressions that involve stored queries (i.e. queries stored using the LET keyword). These queries will not be evaluated if they are not needed.\nThis allows a query to cheaply branch. For example, if a parameter is given, then perform hash or upload to the server. See the Windows.Search.FileFinder for an example of how if() is used.\nint    Arg Description Type     int The integer to round vfilter.Any    Truncate a float to an integer.\nip    Arg Description Type     netaddr4_le A network order IPv4 address (as little endian). int64   netaddr4_be A network order IPv4 address (as big endian). int64    Converts an ip address encoded in various ways. If the IP address is encoded as 32 bit integer we can use netaddr4_le or netaddr4_be to print it in a human readable way.\nThis currently does not support IPv6 addresses. Those are usually encoded as an array of 8 bytes which makes it easy to format using the format() function:\n format(format=\u0026quot;%x:%x:%x:%x:%x:%x:%x:%x\u0026quot;, value)  join    Arg Description Type     sep The separator string   array The array to join list of string (required)    Joins the array into a string separated by the sep character.\nlabel    Arg Description Type     client_id Client ID to label. string (required)   labels A list of labels to apply list of string (required)   op An operation on the labels (add, remove) string    Add the labels to the client. If op is \u0026lsquo;remove\u0026rsquo; then remove these labels.\nThis function only works when run on the server.\nlowcase    Arg Description Type     string A string to lower string (required)    Converts a string to lower case.\nmax    Arg Description Type     items  vfilter.Any (required)    This finds the smallest number in the aggregate. It is only meaningful in a group by query.\nExample The following query lists all the processes and shows the largest bash pid of all bash processes.\nSELECT Name, max(items=Pid) as LargestPid from pslist() Where Name =~ 'bash' group by Name  min    Arg Description Type     items  vfilter.Any (required)    This finds the smallest number in the aggregate. It is only meaningful in a group by query.\nExample The following query lists all the processes and shows the smallest bash pid of all bash processes.\nSELECT Name, min(items=Pid) as SmallestPid from pslist() Where Name =~ 'bash' group by Name  now Returns the current time as seconds since the unix epoch.\nparse_float    Arg Description Type     string A string to convert to int string (required)    Convert a string to a float.\nparse_json    Arg Description Type     data Json encoded string. string (required)    This function parses a json string into a dict.\nNote that when VQL dereferences fields in a dict it returns a Null for those fields that do not exist. Thus there is no error in actually accessing missing fields, the column will just return nil.\nparse_json_array    Arg Description Type     data Json encoded string. string (required)    This function is similar to parse_json() but works for a JSON list instead of an object.\nparse_pe    Arg Description Type     file The PE file to open. string (required)   accessor The accessor to use. string    Parse a PE file.\nparse_string_with_regex    Arg Description Type     string A string to parse. string (required)   regex The regex to apply. list of string (required)    Parse a string with a set of regex and extract fields. Returns a dict with fields populated from all regex capture variables.\nparse_xml    Arg Description Type     file XML file to open. string (required)   accessor The accessor to use string    This function parses the xml file into a dict like object which can then be queried.\nrate    Arg Description Type     x The X float float64 (required)   y The Y float float64 (required)    Calculates the rate (derivative) between two quantities. For example if a monitoring plugin returns an absolute value sampled in time (e.g. bytes transferred sampled every second) then the rate() plugin can calculate the average bytes/sec.\nThis function works by remembering the values of x and y from the previous row and applying the current rows values.\nregex_replace    Arg Description Type     source The source string to replace. string (required)   replace The substitute string. string (required)   re A regex to apply string (required)    Search and replace a string with a regexp. Note you can use $1 to replace the capture string.\nscope return the scope as a dict.\nsplit    Arg Description     string The string to split   sep A regex to serve as a separator.    Splits a string into an array based on a regexp separator.\ntempfile    Arg Description Type     data Data to write in the tempfile. list of string (required)   extension An extension to place in the tempfile. string    Create a temporary file and write some data into it. The file will be removed when the query completes.\ntimestamp    Arg Description Type     epoch  int64   winfiletime  int64    Convert seconds from epoch into a string.\nupcase    Arg Description Type     string A string to lower string (required)    Converts a string to upper case\nupload    Arg Description Type     accessor The accessor to use string   file The file to upload string (required)   name The name of the file that should be stored on the server string    This function uploads the specified file to the server. If Velociraptor is run locally the file will be copied to the --dump_dir path or added to the triage evidence container.\nurl    Arg Description Type     scheme The scheme to use string   host The host component string   path The path component string   fragment The fragment string   parse A url to parse string    This function parses or constructs URLs. A URL may be constructed from scratch by providing all the components or it may be parsed from an existing URL.\nThe returned object is a golang URL and can be serialized again using its String method.\nThis function is important when constructing parameters for certain accessors which receive a URL. For example the zip accessor requires its file names to consist of URLs. The Zip accessor interprets the URL in the following way:\n The scheme is the delegate accessor to use. The path is the delegate accessor\u0026rsquo;s filename The fragment is used by the zip accessor to retrieve the zip member itself.  In this case it is critical to properly escape each level - it is not possible in the general case to simply append strings. You need to use the url() function to build the proper url.\nutf16    Arg Description Type     string A string to decode string (required)    Converts a UTF16 encoded string to a normal utf8 string.\n"
},
{
        "uri": "/docs/vql_reference/",
        "title": "VQL Reference",
        "tags": [],
        "description": "",
        "content": " The Velociraptor Query language (VQL) is an expressive language designed for querying endpoint state. It was developed as a way to flexibly adapt new IOCs on endpoints without needing to rebuild or deploy new endpoints.\nVQL is simple to use and simple to understand. Although it draws its inspiration from SQL, VQL does not support complex operations such as join. Unlike SQL, which can only query static tables of data, VQL queries plugins which are provided parameters. This allows VQL plugins to customize their output based on arguments provided to them.\nThe basic structure of a VQL query is:\nSELECT Column1, Column2, Column3 FROM plugin(arg=1) WHERE Column1 = \u0026quot;X\u0026quot;  In the above we term the clause between the SELECT and FROM clause the \u0026ldquo;Column Specification\u0026rdquo;. The clause between the FROM and WHERE is termed the plugin clause while the terms after the WHERE are termed the filter clause.\nColumn Specification The column specification is a comma delimited list of expressions which may consist of operations or VQL functions. The expressions operate on each row returned from the plugin. Each expression specifies a single Column to add to the transformed row. Alternatively the Column Specification may consist of \u0026ldquo;*\u0026rdquo; indicating no transformation shall be applied to the rows returned from the plugin.\nA column expression may also use the keyword AS to define an alias for the expression (i.e. define a new name for the column).\nFor example the following will return a row with a column names \u0026ldquo;Now\u0026rdquo; containing the string representation of the current time:\nSELECT timestamp(epoch=now()) AS Now FROM scope()  Plugin The plugin clause specifies a VQL plugin to run. VQL plugins are the data source for the VQL query and generate a sequence of rows. They also take keyword args.\nPlugins must take keyword args (i.e. keyword=value). Positional args are currently not supported. You will receive a syntax error if no keyword is provided.\n The names of the args are defined by the plugin itself which also defines which arg is required and which are simply optional. The references below explain the meaning of each arg for the different plugins.\nArgs also have a type and must receive the correct type. For example, providing a string to an arg which requires an integer will result in that arg being ignored. The VQL statement will not be aborted though! Such a syntax error will simply result in the plugin returning no rows and a log message generated.\n"
},
{
        "uri": "/docs/artifacts/triage/",
        "title": "Windows Triage Artifacts",
        "tags": [],
        "description": "Triage artifacts simply collect various files as quickly as possible.",
        "content": " Windows.Triage.Collectors.Amcache    Arg Default Description     triageTable Type,Accessor,Glob\\nAmcache,ntfs,C:\\Windows\\AppCompat\\Programs\\Amcache.hve\\nAmcache transaction files,ntfs,C:\\Windows\\AppCompat\\Programs\\Amcache.hve.LOG*\\n       View Artifact Source   name: Windows.Triage.Collectors.Amcache precondition: SELECT OS From info() where OS = 'windows' parameters: - name: triageTable default: | Type,Accessor,Glob Amcache,ntfs,C:\\Windows\\AppCompat\\Programs\\Amcache.hve Amcache transaction files,ntfs,C:\\Windows\\AppCompat\\Programs\\Amcache.hve.LOG* sources: - queries: - SELECT * FROM Artifact.Triage.Collection.UploadTable(triageTable=triageTable)    Windows.Triage.Collectors.BCD Boot Configuration Files.\n  View Artifact Source   name: Windows.Triage.Collectors.BCD description: | Boot Configuration Files. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;BCD\u0026quot;, path=[ \u0026quot;C:\\\\Boot\\\\BCD\u0026quot;, \u0026quot;C:\\\\Boot\\\\BCD.LOG*\u0026quot; ]) } )    Windows.Triage.Collectors.Chrome Collect Chrome related artifacts.\n   Arg Default Description     triageTable Type,Accessor,Glob\\nChrome Bookmarks,file,C:\\Documents and Settings\\\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Bookmarks\\nChrome Bookmarks,file,C:\\Users\\\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Bookmarks\\nChrome Bookmarks,file,C:\\Documents and Settings\\\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Bookmarks\\nChrome Bookmarks,file,C:\\Users\\\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Bookmarks\\nChrome Cookies,file,C:\\Documents and Settings\\\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Cookies\\nChrome Cookies,file,C:\\Users\\\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Cookies\\nChrome Current Session,ntfs,C:\\Documents and Settings\\\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Current Session\\nChrome Current Session,ntfs,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Current Session\\nChrome Current Tab,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Current Tab\\nChrome Current Tab,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Current Tab\\nChrome Favicons,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Favicons\\nChrome Favicons,file,C:\\Users\\\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Favicons\\nChrome History,file,C:\\Documents and Settings\\\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\History\\nChrome History,file,C:\\Users\\\\AppData\\Local\\Google\\Chrome\\User Data\\*\\History\\nChrome Last Session,file,C:\\Documents and Settings\\\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Last Session\\nChrome Last Session,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Last Session\\nChrome Last Tabs,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Last Tabs\\nChrome Last Tabs,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Last Tabs\\nChrome Preferences,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Preferences\\nChrome Preferences,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Preferences\\nChrome Shortcuts,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Shortcuts\\nChrome Shortcuts,file,C:\\Users\\\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Shortcuts\\nChrome Top Sites,file,C:\\Documents and Settings\\\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Top Sites\\nChrome Top Sites,file,C:\\Users\\\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Top Sites\\nChrome Visited Links,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Visited Links\\nChrome Visited Links,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Visited Links\\nChrome Web Data,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Web Data\\nChrome Web Data,file,C:\\Users\\\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Web Data\\n       View Artifact Source   name: Windows.Triage.Collectors.Chrome description: | Collect Chrome related artifacts. precondition: SELECT OS From info() where OS = 'windows' parameters: - name: triageTable default: | Type,Accessor,Glob Chrome Bookmarks,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Bookmarks* Chrome Bookmarks,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Bookmarks* Chrome Bookmarks,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Bookmarks* Chrome Bookmarks,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Bookmarks* Chrome Cookies,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Cookies* Chrome Cookies,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Cookies* Chrome Current Session,ntfs,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Current Session Chrome Current Session,ntfs,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Current Session Chrome Current Tab,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Current Tab Chrome Current Tab,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Current Tab Chrome Favicons,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Favicons* Chrome Favicons,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Favicons* Chrome History,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\History* Chrome History,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\History* Chrome Last Session,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Last Session Chrome Last Session,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Last Session Chrome Last Tabs,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Last Tabs Chrome Last Tabs,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Last Tabs Chrome Preferences,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Preferences Chrome Preferences,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Preferences Chrome Shortcuts,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Shortcuts* Chrome Shortcuts,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Shortcuts* Chrome Top Sites,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Top Sites* Chrome Top Sites,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Top Sites Chrome Visited Links,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Visited Links Chrome Visited Links,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Visited Links Chrome Web Data,file,C:\\Documents and Settings\\*\\Local Settings\\Application Data\\Google\\Chrome\\User Data\\*\\Web Data* Chrome Web Data,file,C:\\Users\\*\\AppData\\Local\\Google\\Chrome\\User Data\\*\\Web Data* sources: - queries: - SELECT * FROM Artifact.Triage.Collection.UploadTable(triageTable=triageTable)    Windows.Triage.Collectors.Edge Collect Edge related artifacts.\n   Arg Default Description     triageTable Type,Accessor,Glob\\nEdge folder,ntfs,C:\\Users\\\\AppData\\Local\\Packages\\Microsoft.MicrosoftEdge_\\*\\nWebcacheV01.dat,ntfs,C:\\Users\\*\\AppData\\Local\\Microsoft\\Windows\\WebCache\\*\\n       View Artifact Source   name: Windows.Triage.Collectors.Edge description: | Collect Edge related artifacts. precondition: SELECT OS From info() where OS = 'windows' parameters: - name: triageTable default: | Type,Accessor,Glob Edge folder,ntfs,C:\\Users\\*\\AppData\\Local\\Packages\\Microsoft.MicrosoftEdge_*\\** WebcacheV01.dat,ntfs,C:\\Users\\*\\AppData\\Local\\Microsoft\\Windows\\WebCache\\** sources: - queries: - SELECT * FROM Artifact.Triage.Collection.UploadTable(triageTable=triageTable)    Windows.Triage.Collectors.EventLogs Collect event log files.\n   Arg Default Description     EventLogGlobs C:\\Windows\\system32\\config\\*.evt,C:\\Windows\\system32\\winevt\\logs\\*.evtx       View Artifact Source   name: Windows.Triage.Collectors.EventLogs description: | Collect event log files. parameters: - name: EventLogGlobs default: C:\\Windows\\system32\\config\\*.evt,C:\\Windows\\system32\\winevt\\logs\\*.evtx precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;EventLogs\u0026quot;, path=split(string=EventLogGlobs, sep=\u0026quot;,\u0026quot;))    Windows.Triage.Collectors.EventTraceLogs Collect event trace log files.\n  View Artifact Source   name: Windows.Triage.Collectors.EventTraceLogs description: | Collect event trace log files. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;WDI Trace Logs\u0026quot;, path=[ \u0026quot;C:\\\\Windows\\\\System32\\\\WDI\\\\LogFiles\\\\*.etl*\u0026quot;, \u0026quot;C:\\\\Windows\\\\System32\\\\WDI\\\\{*\u0026quot; ]) }, a2={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;WMI Trace Logs\u0026quot;, path=\u0026quot;C:\\\\Windows\\\\System32\\\\LogFiles\\\\WMI\\\\*\u0026quot;) }, a3={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;SleepStudy Trace Logs\u0026quot;, path=\u0026quot;C:\\\\Windows\\\\System32\\\\SleepStudy*\u0026quot;) }, a4={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Energy-NTKL Trace Logs\u0026quot;, path=\u0026quot;C:\\\\ProgramData\\\\Microsoft\\\\Windows\\\\PowerEfficiency Diagnostics\\\\energy-ntkl.etl\u0026quot;) } )    Windows.Triage.Collectors.EvidenceOfExecution   View Artifact Source   name: Windows.Triage.Collectors.EvidenceOfExecution precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Prefetch\u0026quot;, path=\u0026quot;C:\\\\Windows\\\\prefetch\\\\*.pf\u0026quot;) }, a2={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;RecentFileCache\u0026quot;, path=\u0026quot;C:\\\\Windows\\\\AppCompat\\\\Programs\\\\RecentFileCache.bcf\u0026quot;) } )    Windows.Triage.Collectors.Firefox Collect Firefox related artifacts.\n   Arg Default Description     baseLocations C:\\Users\\*\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\*\\,C:\\Documents and Settings\\*\\Application Data\\Mozilla\\Firefox\\Profiles\\*\\ Globs for different possible locations of firefox profiles.      View Artifact Source   name: Windows.Triage.Collectors.Firefox description: | Collect Firefox related artifacts. precondition: SELECT OS From info() where OS = 'windows' parameters: - name: baseLocations description: Globs for different possible locations of firefox profiles. default: C:\\Users\\*\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\*\\,C:\\Documents and Settings\\*\\Application Data\\Mozilla\\Firefox\\Profiles\\*\\ sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Places\u0026quot;, path=split(string=baseLocations, sep=\u0026quot;,\u0026quot;) + \u0026quot;places.sqlite*\u0026quot;) }, a2={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Downloads\u0026quot;, path=split(string=baseLocations, sep=\u0026quot;,\u0026quot;) + \u0026quot;downloads.sqlite*\u0026quot;) }, a3={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Form history\u0026quot;, path=split(string=baseLocations, sep=\u0026quot;,\u0026quot;) + \u0026quot;formhistory.sqlite*\u0026quot;) }, a4={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Cookies\u0026quot;, path=split(string=baseLocations, sep=\u0026quot;,\u0026quot;) + \u0026quot;cookies.sqlite*\u0026quot;) }, a5={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Signons\u0026quot;, path=split(string=baseLocations, sep=\u0026quot;,\u0026quot;) + \u0026quot;signons.sqlite*\u0026quot;) }, a6={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Webappstore\u0026quot;, path=split(string=baseLocations, sep=\u0026quot;,\u0026quot;) + \u0026quot;webappstore.sqlite*\u0026quot;) }, a7={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Favicons\u0026quot;, path=split(string=baseLocations, sep=\u0026quot;,\u0026quot;) + \u0026quot;favicons.sqlite*\u0026quot;) }, a8={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Addons\u0026quot;, path=split(string=baseLocations, sep=\u0026quot;,\u0026quot;) + \u0026quot;addons.sqlite*\u0026quot;) }, a9={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Search\u0026quot;, path=split(string=baseLocations, sep=\u0026quot;,\u0026quot;) + \u0026quot;search.sqlite*\u0026quot;) } )    Windows.Triage.Collectors.InternetExplorer Collect Firefox related artifacts.\n   Arg Default Description     triageTable Type,Accessor,Glob\\nIndex.dat History,file,C:\\Documents and Settings\\\\Local Settings\\History\\History.IE5\\index.dat\\nIndex.dat History,file,C:\\Documents and Settings\\*\\Local Settings\\History\\History.IE5\\*\\index.dat\\nIndex.dat temp internet files,file,C:\\Documents and Settings\\*\\Local Settings\\Temporary Internet Files\\Content.IE5\\index.dat\\nIndex.dat cookies,file,C:\\Documents and Settings\\*\\Cookies\\index.dat\\nIndex.dat UserData,file,C:\\Documents and Settings\\*\\Application Data\\Microsoft\\Internet Explorer\\UserData\\index.dat\\nIndex.dat Office XP,file,C:\\Documents and Settings\\*\\Application Data\\Microsoft\\Office\\Recent\\index.dat\\nIndex.dat Office,file,C:\\Users\\*\\AppData\\Roaming\\Microsoft\\Office\\Recent\\index.dat\\nLocal Internet Explorer folder,ntfs,C:\\Users\\*\\AppData\\Local\\Microsoft\\Internet Explorer\\*\\nRoaming Internet Explorer folder,file,C:\\Users\\\\AppData\\Roaming\\Microsoft\\Internet Explorer\\*\\nIE 9\u0026frasl;10 History,file,C:\\Users\\\\AppData\\Local\\Microsoft\\Windows\\History\\*\\nIE 9\u0026frasl;10 Cache,file,C:\\Users\\\\AppData\\Local\\Microsoft\\Windows\\Temporary Internet Files\\*\\nIE 9\u0026frasl;10 Cookies,file,C:\\Users\\\\AppData\\Local\\Microsoft\\Windows\\Cookies\\*\\nIE 9\u0026frasl;10 Download History,file,C:\\Users\\\\AppData\\Local\\Microsoft\\Windows\\IEDownloadHistory\\*\\nIE 11 Metadata,ntfs,C:\\Users\\\\AppData\\Local\\Microsoft\\Windows\\WebCache\\*\\nIE 11 Cache,ntfs,C:\\Users\\\\AppData\\Local\\Microsoft\\Windows\\INetCache\\*\\nIE 11 Cookies,file,C:\\Users\\\\AppData\\Local\\Microsoft\\Windows\\INetCookies\\*\\n       View Artifact Source   name: Windows.Triage.Collectors.InternetExplorer description: | Collect Firefox related artifacts. precondition: SELECT OS From info() where OS = 'windows' parameters: - name: triageTable default: | Type,Accessor,Glob Index.dat History,file,C:\\Documents and Settings\\*\\Local Settings\\History\\History.IE5\\index.dat Index.dat History,file,C:\\Documents and Settings\\*\\Local Settings\\History\\History.IE5\\*\\index.dat Index.dat temp internet files,file,C:\\Documents and Settings\\*\\Local Settings\\Temporary Internet Files\\Content.IE5\\index.dat Index.dat cookies,file,C:\\Documents and Settings\\*\\Cookies\\index.dat Index.dat UserData,file,C:\\Documents and Settings\\*\\Application Data\\Microsoft\\Internet Explorer\\UserData\\index.dat Index.dat Office XP,file,C:\\Documents and Settings\\*\\Application Data\\Microsoft\\Office\\Recent\\index.dat Index.dat Office,file,C:\\Users\\*\\AppData\\Roaming\\Microsoft\\Office\\Recent\\index.dat Local Internet Explorer folder,ntfs,C:\\Users\\*\\AppData\\Local\\Microsoft\\Internet Explorer\\** Roaming Internet Explorer folder,file,C:\\Users\\*\\AppData\\Roaming\\Microsoft\\Internet Explorer\\** IE 9/10 History,file,C:\\Users\\*\\AppData\\Local\\Microsoft\\Windows\\History\\** IE 9/10 Cache,file,C:\\Users\\*\\AppData\\Local\\Microsoft\\Windows\\Temporary Internet Files\\** IE 9/10 Cookies,file,C:\\Users\\*\\AppData\\Local\\Microsoft\\Windows\\Cookies\\** IE 9/10 Download History,file,C:\\Users\\*\\AppData\\Local\\Microsoft\\Windows\\IEDownloadHistory\\** IE 11 Metadata,ntfs,C:\\Users\\*\\AppData\\Local\\Microsoft\\Windows\\WebCache\\** IE 11 Cache,ntfs,C:\\Users\\*\\AppData\\Local\\Microsoft\\Windows\\INetCache\\** IE 11 Cookies,file,C:\\Users\\*\\AppData\\Local\\Microsoft\\Windows\\INetCookies\\** sources: - queries: - SELECT * FROM Artifact.Triage.Collection.UploadTable(triageTable=triageTable)    Windows.Triage.Collectors.Jabber Jabber.\n  View Artifact Source   name: Windows.Triage.Collectors.Jabber description: | Jabber. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Cisco Jabber Database\u0026quot;, accessor=\u0026quot;ntfs\u0026quot;, path=[ \u0026quot;C:\\\\Users\\\\*\\\\AppData\\\\Local\\\\Cisco\\\\Unified Communications\\\\Jabber\\\\CSF\\\\History\\\\*.db\u0026quot; ]) } )    Windows.Triage.Collectors.LnkFiles Lnk files and jump lists.\n  View Artifact Source   name: Windows.Triage.Collectors.LnkFiles description: | Lnk files and jump lists. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Lnk files from Recent\u0026quot;, path=[ \u0026quot;C:\\\\Users\\\\*\\\\AppData\\\\Roaming\\\\Microsoft\\\\Windows\\\\Recent\\\\**\u0026quot;, \u0026quot;C:\\\\Users\\\\*\\\\AppData\\\\Roaming\\\\Microsoft\\\\Office\\\\Recent\\\\**\u0026quot;, \u0026quot;C:\\\\Documents and Settings\\\\*\\\\Recent\\\\**\u0026quot; ]) }, a2={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Desktop lnk files\u0026quot;, path=[ \u0026quot;C:\\\\Documents and Settings\\\\*\\\\Desktop\\\\*.lnk\u0026quot;, \u0026quot;C:\\\\Users\\\\*\\\\Desktop\\\\*.lnk\u0026quot; ]) }, a3={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Restore point lnk files XP\u0026quot;, path=\u0026quot;C:\\\\System Volume Information\\\\_restore*\\\\RP*\\\\*.lnk\u0026quot;) } )    Windows.Triage.Collectors.NTFSMetadata   View Artifact Source   name: Windows.Triage.Collectors.NTFSMetadata precondition: SELECT OS From info() where OS = 'windows' reference: - https://github.com/EricZimmerman/KapeFiles sources: - name: NTFS Metadata Files queries: - | SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;NTFS Metadata Files\u0026quot;, accessor=\u0026quot;ntfs\u0026quot;, path=[ \u0026quot;C:\\\\$MFT\u0026quot;, \u0026quot;C:\\\\$LogFile\u0026quot;, \u0026quot;C:\\\\$Extend\\\\$UsnJrnl:$J\u0026quot;, \u0026quot;C:\\\\$Extend\\\\$UsnJrnl:$Max\u0026quot;, \u0026quot;C:\\\\$Secure:$SDS\u0026quot;, \u0026quot;C:\\\\$Boot\u0026quot;, \u0026quot;C:\\\\$Extend\\\\$RmMetadata\\\\$TxfLog\\\\$Tops:$T\u0026quot; ])    Windows.Triage.Collectors.OutlookPST Outlook PST and OST files.\n  View Artifact Source   name: Windows.Triage.Collectors.OutlookPST description: | Outlook PST and OST files. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;PST\u0026quot;, path=[ \u0026quot;C:\\\\Documents and Settings\\\\*\\\\Local Settings\\\\Application Data\\\\Microsoft\\\\Outlook\\\\*.pst\u0026quot;, \u0026quot;C:\\\\Users\\\\*\\\\AppData\\\\Local\\\\Microsoft\\\\Outlook\\\\*.pst\u0026quot; ]) }, a2={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;OST\u0026quot;, path=[ \u0026quot;C:\\\\Documents and Settings\\\\*\\\\Local Settings\\\\Application Data\\\\Microsoft\\\\Outlook\\\\*.ost\u0026quot;, \u0026quot;C:\\\\Users\\\\*\\\\AppData\\\\Local\\\\Microsoft\\\\Outlook\\\\*.ost\u0026quot; ]) } )    Windows.Triage.Collectors.PowershellConsoleLogs PowerShell Console Log File.\n  View Artifact Source   name: Windows.Triage.Collectors.PowershellConsoleLogs description: | PowerShell Console Log File. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;PowerShell Console Log\u0026quot;, path=\u0026quot;C:\\\\users\\\\*\\\\Appdata\\\\Roaming\\\\Microsoft\\\\Windows\\\\PowerShell\\\\PSReadline\\\\ConsoleHost_history.txt\u0026quot;) } )    Windows.Triage.Collectors.RecycleBin Collect contents of Recycle Bin.\n  View Artifact Source   name: Windows.Triage.Collectors.RecycleBin description: | Collect contents of Recycle Bin. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Recycle.Bin\u0026quot;, path=[ \u0026quot;C:\\\\$Recycle.Bin\\\\**\u0026quot;, \u0026quot;C:\\\\RECYCLER\\\\**\u0026quot; ]) } )    Windows.Triage.Collectors.RegistryHives System and user related Registry hives.\n   Arg Default Description     triageTable Type,Accessor,Glob\\nntuser.dat registry hive,ntfs,C:\\Documents and Settings\\\\ntuser.dat\\nntuser.dat registry hive,ntfs,C:\\Users\\*\\ntuser.dat\\nntuser.dat registry transaction files,ntfs,C:\\Documents and Settings\\*\\ntuser.dat.LOG\\nntuser.dat registry transaction files,ntfs,C:\\Users\\\\ntuser.dat.LOG\\nUsrClass.dat registry hive,ntfs,C:\\Users\\\\AppData\\Local\\Microsoft\\Windows\\UsrClass.dat\\nUsrClass.dat registry transaction files,ntfs,C:\\Users\\*\\AppData\\Local\\Microsoft\\Windows\\UsrClass.dat.LOG\\nSAM registry transaction files,ntfs,C:\\Windows\\System32\\config\\SAM.LOG\\nSECURITY registry transaction files,ntfs,C:\\Windows\\System32\\config\\SECURITY.LOG\\nSYSTEM registry transaction files,ntfs,C:\\Windows\\System32\\config\\SYSTEM.LOG\\nSAM registry hive,ntfs,C:\\Windows\\System32\\config\\SAM\\nSECURITY registry hive,ntfs,C:\\Windows\\System32\\config\\SECURITY\\nSOFTWARE registry hive,ntfs,C:\\Windows\\System32\\config\\SOFTWARE\\nSYSTEM registry hive,ntfs,C:\\Windows\\System32\\config\\SYSTEM\\nRegBack registry transaction files,ntfs,C:\\Windows\\System32\\config\\RegBack\\*.LOG\\nSAM registry hive (RegBack),ntfs,C:\\Windows\\System32\\config\\RegBack\\SAM\\nSECURITY registry hive (RegBack),ntfs,C:\\Windows\\System32\\config\\RegBack\\SECURITY\\nSOFTWARE registry hive (RegBack),ntfs,C:\\Windows\\System32\\config\\RegBack\\SOFTWARE\\nSYSTEM registry hive (RegBack),ntfs,C:\\Windows\\System32\\config\\RegBack\\SYSTEM\\nSystem Profile registry hive,ntfs,C:\\Windows\\System32\\config\\systemprofile\\ntuser.dat\\nSystem Profile registry transaction files,ntfs,C:\\Windows\\System32\\config\\systemprofile\\ntuser.dat.LOG\\nLocal Service registry hive,ntfs,C:\\Windows\\ServiceProfiles\\LocalService\\ntuser.dat\\nLocal Service registry transaction files,ntfs,C:\\Windows\\ServiceProfiles\\LocalService\\ntuser.dat.LOG\\nNetwork Service registry hive,ntfs,C:\\Windows\\ServiceProfiles\\NetworkService\\ntuser.dat\\nNetwork Service registry transaction files,ntfs,C:\\Windows\\ServiceProfiles\\NetworkService\\ntuser.dat.LOG\\nSystem Restore Points Registry Hives (XP),ntfs,C:\\System Volume Information\\_restore\\RP\\snapshot\\REGISTRY\\n       View Artifact Source   name: Windows.Triage.Collectors.RegistryHives description: | System and user related Registry hives. precondition: SELECT OS From info() where OS = 'windows' reference: - https://github.com/EricZimmerman/KapeFiles parameters: - name: triageTable default: | Type,Accessor,Glob ntuser.dat registry hive,ntfs,C:\\Documents and Settings\\*\\ntuser.dat ntuser.dat registry hive,ntfs,C:\\Users\\*\\ntuser.dat ntuser.dat registry transaction files,ntfs,C:\\Documents and Settings\\*\\ntuser.dat.LOG* ntuser.dat registry transaction files,ntfs,C:\\Users\\*\\ntuser.dat.LOG* UsrClass.dat registry hive,ntfs,C:\\Users\\*\\AppData\\Local\\Microsoft\\Windows\\UsrClass.dat UsrClass.dat registry transaction files,ntfs,C:\\Users\\*\\AppData\\Local\\Microsoft\\Windows\\UsrClass.dat.LOG* SAM registry transaction files,ntfs,C:\\Windows\\System32\\config\\SAM.LOG* SECURITY registry transaction files,ntfs,C:\\Windows\\System32\\config\\SECURITY.LOG* SYSTEM registry transaction files,ntfs,C:\\Windows\\System32\\config\\SYSTEM.LOG* SAM registry hive,ntfs,C:\\Windows\\System32\\config\\SAM SECURITY registry hive,ntfs,C:\\Windows\\System32\\config\\SECURITY SOFTWARE registry hive,ntfs,C:\\Windows\\System32\\config\\SOFTWARE SYSTEM registry hive,ntfs,C:\\Windows\\System32\\config\\SYSTEM RegBack registry transaction files,ntfs,C:\\Windows\\System32\\config\\RegBack\\*.LOG* SAM registry hive (RegBack),ntfs,C:\\Windows\\System32\\config\\RegBack\\SAM SECURITY registry hive (RegBack),ntfs,C:\\Windows\\System32\\config\\RegBack\\SECURITY SOFTWARE registry hive (RegBack),ntfs,C:\\Windows\\System32\\config\\RegBack\\SOFTWARE SYSTEM registry hive (RegBack),ntfs,C:\\Windows\\System32\\config\\RegBack\\SYSTEM System Profile registry hive,ntfs,C:\\Windows\\System32\\config\\systemprofile\\ntuser.dat System Profile registry transaction files,ntfs,C:\\Windows\\System32\\config\\systemprofile\\ntuser.dat.LOG* Local Service registry hive,ntfs,C:\\Windows\\ServiceProfiles\\LocalService\\ntuser.dat Local Service registry transaction files,ntfs,C:\\Windows\\ServiceProfiles\\LocalService\\ntuser.dat.LOG* Network Service registry hive,ntfs,C:\\Windows\\ServiceProfiles\\NetworkService\\ntuser.dat Network Service registry transaction files,ntfs,C:\\Windows\\ServiceProfiles\\NetworkService\\ntuser.dat.LOG* System Restore Points Registry Hives (XP),ntfs,C:\\System Volume Information\\_restore*\\RP*\\snapshot\\_REGISTRY_* sources: - queries: - SELECT * FROM Artifact.Triage.Collection.UploadTable( triageTable=triageTable)    Windows.Triage.Collectors.SRUM System Resource Usage Monitor (SRUM) Data.\n  View Artifact Source   name: Windows.Triage.Collectors.SRUM description: | System Resource Usage Monitor (SRUM) Data. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;SRUM\u0026quot;, path=\u0026quot;C:\\\\Windows\\\\System32\\\\SRU\\\\**\u0026quot;) } )    Windows.Triage.Collectors.ScheduledTasks Scheduled tasks (*.job and XML).\n  View Artifact Source   name: Windows.Triage.Collectors.ScheduledTasks description: | Scheduled tasks (*.job and XML). precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;at .job\u0026quot;, path=\u0026quot;C:\\\\Windows\\\\Tasks\\\\*.job\u0026quot;) }, a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;at SchedLgU.txt\u0026quot;, path=\u0026quot;C:\\\\Windows\\\\SchedLgU.txt\u0026quot;) }, a2={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;XML\u0026quot;, path=\u0026quot;C:\\\\Windows\\\\system32\\\\Tasks\\\\**\u0026quot;) } )    Windows.Triage.Collectors.Skype Skype.\n  View Artifact Source   name: Windows.Triage.Collectors.Skype description: | Skype. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;main.db\u0026quot;, path=[ \u0026quot;C:\\\\Users\\\\*\\\\AppData\\\\Local\\\\Packages\\\\Microsoft.SkypeApp_*\\\\LocalState\\\\*\\\\main.db\u0026quot;, \u0026quot;C:\\\\Documents and Settings\\\\*\\\\Application Data\\\\Skype\\\\*\\\\main.db\u0026quot;, \u0026quot;C:\\\\Users\\\\*\\\\AppData\\\\Roaming\\\\Skype\\\\*\\\\main.db\u0026quot; ]) } )    Windows.Triage.Collectors.StartupInfo StartupInfo XML Files.\n  View Artifact Source   name: Windows.Triage.Collectors.StartupInfo description: | StartupInfo XML Files. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;StartupInfo XML Files\u0026quot;, path=[ \u0026quot;C:\\\\Windows\\\\System32\\\\WDI\\\\LogFiles\\\\StartupInfo\\\\*.xml\u0026quot; ]) } )    Windows.Triage.Collectors.TeraCopy TeraCopy log history.\n  View Artifact Source   name: Windows.Triage.Collectors.TeraCopy description: | TeraCopy log history. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;TeraCopy\u0026quot;, path=[ \u0026quot;C:\\\\Users\\\\*\\\\AppData\\\\Roaming\\\\TeraCopy\u0026quot; ]) } )    Windows.Triage.Collectors.ThumbDB Thumbcache DB.\n  View Artifact Source   name: Windows.Triage.Collectors.ThumbDB description: | Thumbcache DB. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Thumbcache DB\u0026quot;, path=[ \u0026quot;C:\\\\Users\\\\*\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Explorer\\\\thumbcache_*.db\u0026quot; ]) } )    Windows.Triage.Collectors.USBDeviceLogs USB devices log files.\n  View Artifact Source   name: Windows.Triage.Collectors.USBDeviceLogs description: | USB devices log files. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Setupapi.log\u0026quot;, path=[ \u0026quot;C:\\\\Windows\\\\setupapi.log\u0026quot;, \u0026quot;C:\\\\Windows\\\\inf\\\\setupapi.dev.log\u0026quot; ]) } )    Windows.Triage.Collectors.WBEM Web-Based Enterprise Management (WBEM).\n  View Artifact Source   name: Windows.Triage.Collectors.WBEM description: | Web-Based Enterprise Management (WBEM). precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;WBEM\u0026quot;, path=[ \u0026quot;C:\\\\Windows\\\\System32\\\\wbem\\\\Repository\u0026quot; ]) } )    Windows.Triage.Collectors.WindowsFirewall Windows Firewall Logs.\n  View Artifact Source   name: Windows.Triage.Collectors.WindowsFirewall description: | Windows Firewall Logs. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;Windows Firewall Logs\u0026quot;, path=\u0026quot;C:\\\\Windows\\\\System32\\\\LogFiles\\\\Firewall\\\\pfirewall.*\u0026quot;) } )    Windows.Triage.Collectors.WindowsIndex Windows Index Search.\n  View Artifact Source   name: Windows.Triage.Collectors.WindowsIndex description: | Windows Index Search. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Triage.Collection.Upload( type=\u0026quot;WindowsIndexSearch\u0026quot;, path=\u0026quot;C:\\\\programdata\\\\microsoft\\\\search\\\\data\\\\applications\\\\windows\\\\Windows.edb\u0026quot;) } )    Windows.Triage.ProcessMemory Dump process memory and upload to the server\n   Arg Default Description     processRegex notepad       View Artifact Source   name: Windows.Triage.ProcessMemory description: | Dump process memory and upload to the server precondition: SELECT OS From info() where OS = 'windows' parameters: - name: processRegex default: notepad sources: - queries: - | LET processes = SELECT Name as ProcessName, CommandLine, Pid FROM pslist() WHERE Name =~ processRegex - | SELECT * FROM foreach( row=processes, query={ SELECT ProcessName, CommandLine, Pid, FullPath, upload(file=FullPath) as CrashDump FROM proc_dump(pid=Pid) })    Windows.Triage.WebBrowsers A high level artifact for selecting all browser related artifacts.\n  View Artifact Source   name: Windows.Triage.WebBrowsers description: | A high level artifact for selecting all browser related artifacts. precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM chain( a1={ SELECT * FROM Artifact.Windows.Triage.Collectors.Chrome() }, a2={ SELECT * FROM Artifact.Windows.Triage.Collectors.Firefox() }, a3={ SELECT * FROM Artifact.Windows.Triage.Collectors.Edge() }, a4={ SELECT * FROM Artifact.Windows.Triage.Collectors.InternetExplorer() } )    Triage.Collection.Upload A Generic uploader used by triaging artifacts.\n   Arg Default Description     path  This is the glob of the files we use.   type  The type of files these are.   accessor file       View Artifact Source   name: Triage.Collection.Upload description: | A Generic uploader used by triaging artifacts. parameters: - name: path description: This is the glob of the files we use. - name: type description: The type of files these are. - name: accessor default: file sources: - queries: - | LET results = SELECT FullPath, Size, timestamp(epoch=Mtime.Sec) As Modified, type AS Type, { SELECT * FROM upload(files=FullPath, accessor=accessor) } AS FileDetails FROM glob(globs=path, accessor=accessor) WHERE NOT IsDir - | SELECT FullPath, Size, Modified, Type, FileDetails.Path AS ZipPath, FileDetails.Md5 as Md5, FileDetails.Sha256 as SHA256 FROM results    Triage.Collection.UploadTable A Generic uploader used by triaging artifacts. This is similar to Triage.Collection.Upload but uses a CSV table to drive it.\n   Arg Default Description     triageTable Type,Accessor,Glob\\n A CSV table controlling upload. Must have the headers: Type, Accessor, Glob.      View Artifact Source   name: Triage.Collection.UploadTable description: | A Generic uploader used by triaging artifacts. This is similar to `Triage.Collection.Upload` but uses a CSV table to drive it. parameters: - name: triageTable description: \u0026quot;A CSV table controlling upload. Must have the headers: Type, Accessor, Glob.\u0026quot; default: | Type,Accessor,Glob sources: - queries: - | LET results = SELECT FullPath, Size, timestamp(epoch=Mtime.Sec) As Modified, Type, { SELECT * FROM upload(files=FullPath, accessor=Accessor) } AS FileDetails FROM glob(globs=split(string=Glob, sep=\u0026quot;,\u0026quot;), accessor=Accessor) WHERE NOT IsDir - | SELECT * FROM foreach( row={ SELECT * FROM parse_csv(filename=triageTable, accessor='data') }, query={ SELECT FullPath, Size, Modified, Type, FileDetails.Path AS ZipPath, FileDetails.Md5 as Md5, FileDetails.Sha256 as SHA256 FROM results })    Windows.Forensics.Bam The Background Activity Moderator (BAM) is a Windows service that Controls activity of background applications. This service exists in Windows 10 only after Fall Creators update – version 1709.\nIt provides full path of the executable file that was run on the system and last execution date/time\n   Arg Default Description     bamKeys HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\bam\\UserSettings\\*\\*,HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\bam\\State\\UserSettings\\*\\*       View Artifact Source   name: Windows.Forensics.Bam description: | The Background Activity Moderator (BAM) is a Windows service that Controls activity of background applications. This service exists in Windows 10 only after Fall Creators update – version 1709. It provides full path of the executable file that was run on the system and last execution date/time reference: - https://www.andreafortuna.org/dfir/forensic-artifacts-evidences-of-program-execution-on-windows-systems/ parameters: - name: bamKeys default: HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\bam\\UserSettings\\*\\*,HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\bam\\State\\UserSettings\\*\\* sources: - precondition: SELECT OS from info() where OS = \u0026quot;windows\u0026quot; queries: - | LET users \u0026lt;= SELECT Name, UUID FROM Artifact.Windows.Sys.Users() - | SELECT basename(path=dirname(path=FullPath)) as SID, { SELECT Name FROM users WHERE UUID = basename(path=dirname(path=FullPath)) } As UserName, Name as Binary, timestamp(winfiletime=binary_parse( string=Data.value, target=\u0026quot;int64\u0026quot;).AsInteger) as Bam_time FROM glob(globs=split(string=bamKeys, sep=\u0026quot;,\u0026quot;), accessor=\u0026quot;reg\u0026quot;) WHERE Data.type = \u0026quot;BINARY\u0026quot;    Windows.Forensics.FilenameSearch Did a specific file exist on this machine in the past or does it still exist on this machine?\nThis common question comes up frequently in cases of IP theft, discovery and other matters. One way to answer this question is to search the $MFT file for any references to the specific filename. If the filename is fairly unique then a positive hit on that name generally means the file was present.\nSimply determining that a filename existed on an endpoint in the past is significant for some investigations.\nThis artifact applies a YARA search for a set of filenames of interest on the $MFT file. For any hit, the artifact then identified the MFT entry where the hit was found and attempts to resolve that to an actual filename.\n   Arg Default Description     yaraRule wide nocase:my secret file.txt    Device \\\\.\\c:       View Artifact Source   name: Windows.Forensics.FilenameSearch description: | Did a specific file exist on this machine in the past or does it still exist on this machine? This common question comes up frequently in cases of IP theft, discovery and other matters. One way to answer this question is to search the $MFT file for any references to the specific filename. If the filename is fairly unique then a positive hit on that name generally means the file was present. Simply determining that a filename existed on an endpoint in the past is significant for some investigations. This artifact applies a YARA search for a set of filenames of interest on the $MFT file. For any hit, the artifact then identified the MFT entry where the hit was found and attempts to resolve that to an actual filename. parameters: - name: yaraRule default: wide nocase:my secret file.txt - name: Device default: \u0026quot;\\\\\\\\.\\\\c:\u0026quot; sources: - queries: - | SELECT String.Offset AS Offset, String.HexData AS HexData, parse_ntfs(device=Device, mft=String.Offset / 1024) AS MFT FROM yara( rules=yaraRule, files=Device + \u0026quot;/$MFT\u0026quot;, end=10000000000, number_of_hits=1000, accessor=\u0026quot;ntfs\u0026quot;)    Windows.Forensics.Prefetch Windows keeps a cache of prefetch files. When an executable is run, the system records properties about the executable to make it faster to run next time. By parsing this information we are able to determine when binaries are run in the past. On Windows10 we can see the last 8 execution times.\n   Arg Default Description     prefetchGlobs C:\\Windows\\Prefetch\\*.pf       View Artifact Source   name: Windows.Forensics.Prefetch description: | Windows keeps a cache of prefetch files. When an executable is run, the system records properties about the executable to make it faster to run next time. By parsing this information we are able to determine when binaries are run in the past. On Windows10 we can see the last 8 execution times. reference: - https://www.forensicswiki.org/wiki/Prefetch parameters: - name: prefetchGlobs default: C:\\Windows\\Prefetch\\*.pf precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM foreach( row={ SELECT * FROM glob(globs=prefetchGlobs) }, query={ SELECT Name AS PrefetchFileName, Executable, FileSize, LastRunTimes, LastRunTimes.Unix AS LastExecutionTS, RunCount FROM prefetch(filename=FullPath) })    Windows.Forensics.RecentApps GUI Program execution launched on the Win10 system is tracked in the RecentApps key\n   Arg Default Description     UserFilter  If specified we filter by this user ID.   ExecutionTimeAfter  If specified only show executions after this time.   RecentAppsKey Software\\Microsoft\\Windows\\CurrentVersion\\Search\\RecentApps\\*    UserHomes C:\\Users\\*\\NTUSER.DAT       View Artifact Source   name: Windows.Forensics.RecentApps description: | GUI Program execution launched on the Win10 system is tracked in the RecentApps key reference: - https://www.sans.org/security-resources/posters/windows-forensics-evidence-of/75/download precondition: SELECT OS From info() where OS = 'windows' parameters: - name: UserFilter default: \u0026quot;\u0026quot; description: If specified we filter by this user ID. - name: ExecutionTimeAfter default: \u0026quot;\u0026quot; type: timestamp description: If specified only show executions after this time. - name: RecentAppsKey default: Software\\Microsoft\\Windows\\CurrentVersion\\Search\\RecentApps\\* - name: UserHomes default: C:\\Users\\*\\NTUSER.DAT sources: - queries: - LET TMP = SELECT * FROM foreach( row={ SELECT FullPath FROM glob(globs=UserHomes) }, query={ SELECT AppId, AppPath, LaunchCount, timestamp(winfiletime=LastAccessedTime) AS LastExecution, timestamp(winfiletime=LastAccessedTime).Unix AS LastExecutionTS, parse_string_with_regex( string=Key.FullPath, regex=\u0026quot;/Users/(?P\u0026lt;User\u0026gt;[^/]+)/ntuser.dat\u0026quot;).User AS User FROM read_reg_key( globs=url(scheme=\u0026quot;ntfs\u0026quot;, path=FullPath, fragment=RecentAppsKey).String, accessor=\u0026quot;raw_reg\u0026quot;) }) - LET A1 = SELECT * FROM if( condition=UserFilter, then={ SELECT * FROM TMP WHERE User =~ UserFilter }, else=TMP) - SELECT * FROM if( condition=ExecutionTimeAfter, then={ SELECT * FROM A1 WHERE LastExecutionTS \u0026gt; ExecutionTimeAfter }, else=A1)    Windows.Forensics.Timeline Win10 records recently used applications and files in a “timeline” accessible via the “WIN+TAB” key. The data is recorded in a SQLite database.\n   Arg Default Description     UserFilter  If specified we filter by this user ID.   ExecutionTimeAfter  If specified only show executions after this time.   Win10TimelineGlob C:\\Users\\\\AppData\\Local\\ConnectedDevicesPlatform\\L.\\ActivitiesCache.db       View Artifact Source   name: Windows.Forensics.Timeline description: | Win10 records recently used applications and files in a “timeline” accessible via the “WIN+TAB” key. The data is recorded in a SQLite database. parameters: - name: UserFilter default: \u0026quot;\u0026quot; description: If specified we filter by this user ID. - name: ExecutionTimeAfter default: \u0026quot;\u0026quot; type: timestamp description: If specified only show executions after this time. - name: Win10TimelineGlob default: C:\\Users\\*\\AppData\\Local\\ConnectedDevicesPlatform\\L.*\\ActivitiesCache.db precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - LET timeline = SELECT * FROM foreach( row={ SELECT FullPath FROM glob(globs=Win10TimelineGlob) }, query={ SELECT AppId, FullPath, LastModifiedTime FROM sqlite(file=FullPath, query=\u0026quot;SELECT * FROM Activity\u0026quot;) }) - LET TMP = SELECT get( item=parse_json_array(data=AppId).application, member=\u0026quot;0\u0026quot;) AS Application, parse_string_with_regex( string=FullPath, regex=\u0026quot;\\\\\\\\L.(?P\u0026lt;User\u0026gt;[^\\\\\\\\]+)\\\\\\\\\u0026quot;).User AS User, LastModifiedTime, LastModifiedTime.Unix as LastExecutionTS FROM timeline - LET A1 = SELECT * FROM if( condition=UserFilter, then={ SELECT * FROM TMP WHERE User =~ UserFilter }, else=TMP) - SELECT * FROM if( condition=ExecutionTimeAfter, then={ SELECT * FROM A1 WHERE LastExecutionTS \u0026gt; ExecutionTimeAfter }, else=A1)    "
},
{
        "uri": "/blog/",
        "title": "Velociraptor Blog",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/docs/artifacts/",
        "title": "Artifact Reference",
        "tags": [],
        "description": "",
        "content": " Velociraptor uses VQL for many different purposes. Since Artifacts are a nice way to package VQL queries, there are a number of different types of artifacts. This page documents some of the more common artifact types and where they are used.\nClient Artifacts Client artifacts encapsulate VQL queries that run on the client. The artifact contains a number of Sources - each extracting a single table of data.\nClient artifacts are collected from the each client at a time, or using a hunt, collected from a number of clients at the same time.\nServer Artifacts Velociraptor can also run VQL queries within the server process itself. When running in the server, there are a number of plugins available providing access to hunts, flows and their results.\nTherefore server artifacts are useful for post processing the raw data collected from client artifacts.\n"
},
{
        "uri": "/blog/calendar/",
        "title": "Calendar",
        "tags": [],
        "description": "",
        "content": "  2018      October   MonTueWedThuFriSatSun 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31        November    MonTueWedThuFriSatSun    1 2 3 4 5 6 7 8 91 10 11 12 131 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30          December    MonTueWedThuFriSatSun      1 2 3 4 5 6 7 8 91 101 111 12 13 14 15 16 17 18 19 20 21 221 231 24 25 26 27 28 29 30 31    \n  2019      January   MonTueWedThuFriSatSun  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31         February    MonTueWedThuFriSatSun     11 2 3 4 5 6 7 8 93 101 11 12 13 141 15 16 17 18 19 20 21 22 23 24 25 26 27 28         March    MonTueWedThuFriSatSun     1 21 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31     April   MonTueWedThuFriSatSun 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30       May   MonTueWedThuFriSatSun   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31          June   MonTueWedThuFriSatSun      1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30     July   MonTueWedThuFriSatSun 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31        August    MonTueWedThuFriSatSun    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 261 27 28 29 30 31           September   MonTueWedThuFriSatSun       1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30      October   MonTueWedThuFriSatSun  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31       \n"
},
{
        "uri": "/docs/user-interface/artifacts/add_artifacts/",
        "title": "Customizing Artifacts",
        "tags": [],
        "description": "",
        "content": " Velociraptor comes with a wide selection of built in artifacts, but the real power of Velociraptor\u0026rsquo;s query language lies in the ability of users to customize and develop their own artifacts - flexibly responding to their own needs. This is really what sets Velociraptor apart from other tools: It is an open platform putting the user in charge of designing their own perfect response and detection system.\nAs described previously, Artifacts are simply text files in YAML format which encapsulate VQL queries. Artifacts are readable and editable by end users, and are supposed to be reused and adapted as seen fit.\nWhen thinking about writing a new artifact, you should check out any similar artifacts that already exist. It is way easier to adapt an existing artifact which does something similar to what you need.\n Viewing existing artifacts The Customizing Artifacts screen, reachable from the side bar menu, offers a search interface to the Velociraptor artifacts database. Velociraptor comes built in with a large number of useful artifacts (you can contribute your own artifacts to the project by sending us a pull request ).\nSince there are many artifacts in the repository it is easier to search for what you need. Simply type your search string in the search bar and the UI will suggest artifacts for you to look more closely into. The search terms are applied across both the artifact\u0026rsquo;s name and the artifact\u0026rsquo;s description.\nWhen selecting an artifact to view, the right pane will be filled with information about it. Each artifact should provide some information about what it is supposed to look for, any parameters it might require and their default values, as well as the actual VQL that will be run.\nNote that artifacts have a Type which indicates in which context a particular artifact can be used. Currently artifacts can be:\n CLIENT: Will run on the client once and return a single collection bundle. CLIENT_EVENT: Will run continuously on the client as an event monitor. SERVER: Will run on the server once and return a single collection bundle. SERVER_EVENT: Will run continuously on the server as an event monitor.  In the above example we see the Server.Monitoring.ClientCount artifact. This is a server event monitoring artifact which sends an email every hour with the current state of the deployment (total number of clients enrolled). This artifact is useful in order to keep network administrators informed of the state of a Velociraptor deployment rollout, without needing to provide them actual logons to the Velociraptor GUI.\nEditing an existing artifact. Once an artifact is viewed, it is possible to edit it in order to customize a new artifact. Velociraptor comes with a large library of existing artifacts and chances are that some of them will already be similar to what you need. Therefore it makes sense to start by editing an existing artifact.\nClicking the Edit an artifact button will edit the currently selected artifact by bringing up an editor window.\nAll custom artifacts must have the prefix Custom to their name. This distinguishes them from the built in artifacts. This means that at any time you may choose to run the customized version or the original built in artifact as you see fit. In future Velociraptor releases, built in artifacts may be updated but since these are not the same as the customized artifacts there is no conflict.\nSaving the artifact will validate it and ensure the VQL syntax is correct. This is not sufficient though, so you should test your new artifact thoroughly. Custom artifacts appear in the relevant search screen (depending on their declared artifact type).\nAdding a brand new artifact Clicking the Add an Artifact button pre-fills a new artifact with a template. The template contains comments to help you build the artifact.\nWhen you become more comfortable with VQL and Velociraptor artifacts creating a new artifact only takes minutes. This provides you with great hunting power - from idea to implementation and execution of a hunt may only take a few minutes!\nA note about security: Being able to add new artifacts gives a user absolute control over clients and the server. This is because a user can add arbitrary VQL to either a Client Artifact (so it can run on the client with system level access) or to a Server Artifact (which has access to all clients). Currently Velociraptor has a 2 tier security model (Readonly users can not add artifacts, and full users can do anything). This might change in future but until then you need to be aware that Velociraptor admins are extremely privileged.\n "
},
{
        "uri": "/docs/vql_reference/templates/",
        "title": "Report Templates",
        "tags": [],
        "description": "",
        "content": " Velociraptor collects artifacts from end points but sometimes the significance of the collected information requires user interpretation. To assist in interpreting the collected information, Velociraptor can generate a Report.\nArtifact writers use the report to guide the artifact user in interpreting the results and assessing the significance of these results.\nThe artifact file therefore contains a report section which allows the artifact writer to explain the significance of the findings.\nReport templates The report is simply a template that gets evaluated in the context of the collected artifact. The templating language is the same as Go\u0026rsquo;s text/template and produces markdown format.\nReport types It is possible to create multiple reports in each artifact to be used in different contexts. For example, an artifact that collects installed software might be run individually on one endpoint, or as part of a hunt. For the hunt we might want to count total machines with a certain software installed.\nTherefore the following report types may be defined:\n CLIENT - this report applies to an artifact collected from a single endpoint. HUNT - The report applies when viewing the results of a hunt.  If the report type is left empty, the type is inferred from the artifact type.\nReport Plugins Reports may call specialized functions to customize the data shown.\nThe Query plugin. This is the most useful report plugin in Velociraptor reporting. It allows you to run arbitrary VQL statements and produces a result set with columns and rows.\nThere are two ways to run a query, inline and using a template. Due to limitations in the Go\u0026rsquo;s templating language, expansion braces may not contain line breaks. This makes it hard to properly format long VQL statements.\nWe can therefore define a template by name and simply call in as part of the Query plugin. Here is an example from Windows.Sys.Users artifact:\n {{ define \u0026quot;users\u0026quot; }} LET users \u0026lt;= SELECT Name, UUID, Type, timestamp(epoch=Mtime) as Mtime FROM source() {{ end }} {{ Query \u0026quot;users\u0026quot; \u0026quot;SELECT Name, UUID, Type, Mtime FROM users\u0026quot; | Table }}  The source() VQL plugin is very useful in reports since it generates all the results from the currently collected artifact. For artifacts with multiple named sources, you can specify which source you want using the source parameter (i.e. source(source=\u0026quot;foo\u0026quot;)\n The Table plugin The Query plugin simply generates an array of rows so it is usually necessary to pipe it to a Table, or LineChart or simply assign it to a variable.\nThe Table plugin simply renders the result of the VQL statement as a table. An example of this output is shown below.\nThe LineChart plugin Sometimes we want to see the output as a line chart. Consider the Generic.Client.Stats artifact which collects a client\u0026rsquo;s memory and CPU load footprint every 10 seconds. In this case it is useful to see the memory use as a chart:\n {{ define \u0026quot;resources\u0026quot; }} SELECT Timestamp, rate(x=CPU, y=Timestamp) * 100 As CPUPercent, RSS / 1000000 AS MemoryUse FROM source() WHERE CPUPercent \u0026gt;= 0 {{ end }} {{ Query \u0026quot;resources\u0026quot; | LineChart \u0026quot;xaxis_mode\u0026quot; \u0026quot;time\u0026quot; \u0026quot;RSS.yaxis\u0026quot; 2 }}  We first pipe the results of the VQL query to the LineChart plugin, and specify some parameters to it.\n xaxis_mode time means the x axis should be treated as a timestamp. RSS.yaxis Indicates that the second column should be an additional y axis.  Note that in the above query the source() automatically incorporated the start and end times as set by the report viewer. The report author does not need to do anything special.\n "
},
{
        "uri": "/docs/artifacts/detection/",
        "title": "Windows Malware Detection",
        "tags": [],
        "description": "These artifacts attempt to detect the presence of specific compromizes.",
        "content": " Windows.Detection.ProcessMemory Scanning process memory for signals is powerful technique. This artifact scans processes for a yara signature and when detected, the process memory is dumped and uploaded to the server.\n   Arg Default Description     processRegex notepad    yaraRule rule Process {\\n strings:\\n $a = \u0026ldquo;this is a secret\u0026rdquo; nocase wide\\n $b = \u0026ldquo;this is a secret\u0026rdquo; nocase\\n condition:\\n any of them\\n}\\n       View Artifact Source   name: Windows.Detection.ProcessMemory description: | Scanning process memory for signals is powerful technique. This artifact scans processes for a yara signature and when detected, the process memory is dumped and uploaded to the server. precondition: SELECT OS From info() where OS = 'windows' parameters: - name: processRegex default: notepad - name: yaraRule default: | rule Process { strings: $a = \u0026quot;this is a secret\u0026quot; nocase wide $b = \u0026quot;this is a secret\u0026quot; nocase condition: any of them } sources: - queries: - | LET processes = SELECT Name as ProcessName, CommandLine, Pid FROM pslist() WHERE Name =~ processRegex - | LET hits = SELECT * FROM foreach( row=processes, query={ SELECT ProcessName, CommandLine, Pid, String.Offset as Offsets FROM proc_yara(rules=yaraRule, pid=Pid) }) - | SELECT * FROM foreach( row=hits, query={ SELECT ProcessName, CommandLine, Pid, Offsets, FullPath, upload(file=FullPath) as CrashDump FROM proc_dump(pid=Pid) })    Windows.Detection.PsexecService PSExec works by installing a new service in the system. The service can be renamed using the -r flag and therefore it is not enough to just watch for a new service called psexecsvc.exe. This artifact improves on this by scanning the service binary to detect the original psexec binary.\n   Arg Default Description     yaraRule rule PsExec {\\n strings:\\n $a = \u0026ldquo;psexec\u0026rdquo; nocase\\n $b = \u0026ldquo;psexec\u0026rdquo; nocase wide\\n\\n condition:\\n any of them\\n}\\n       View Artifact Source   name: Windows.Detection.PsexecService description: | PSExec works by installing a new service in the system. The service can be renamed using the -r flag and therefore it is not enough to just watch for a new service called psexecsvc.exe. This artifact improves on this by scanning the service binary to detect the original psexec binary. type: CLIENT_EVENT parameters: - name: yaraRule default: | rule PsExec { strings: $a = \u0026quot;psexec\u0026quot; nocase $b = \u0026quot;psexec\u0026quot; nocase wide condition: any of them } sources: - queries: - | LET file_scan = SELECT File, Rule, String, now() AS Timestamp, Name, ServiceType FROM yara(rules=yaraRule, files=PathName) WHERE Rule - | LET service_creation = SELECT Parse.TargetInstance.Name AS Name, Parse.TargetInstance.PathName As PathName, Parse.TargetInstance.ServiceType As ServiceType FROM wmi_events( query=\u0026quot;SELECT * FROM __InstanceCreationEvent WITHIN 1 WHERE TargetInstance ISA 'Win32_Service'\u0026quot;, wait=5000000, namespace=\u0026quot;ROOT/CIMV2\u0026quot;) - | SELECT * FROM foreach( row=service_creation, query=file_scan)    Windows.Detection.Thumbdrives.List Users inserting Thumb drives or other Removable drive pose a constant security risk. The external drive may contain malware or other undesirable content. Additionally thumb drives are an easy way for users to exfiltrate documents.\nThis artifact watches for any removable drives and provides a complete file listing to the server for any new drive inserted. It also provides information about any addition to the thumb drive (e.g. a new file copied onto the drive).\nWe exclude very large removable drives since they might have too many files.\n   Arg Default Description     maxDriveSize 32000000000 We ignore removable drives larger than this size in bytes.      View Artifact Source   name: Windows.Detection.Thumbdrives.List description: | Users inserting Thumb drives or other Removable drive pose a constant security risk. The external drive may contain malware or other undesirable content. Additionally thumb drives are an easy way for users to exfiltrate documents. This artifact watches for any removable drives and provides a complete file listing to the server for any new drive inserted. It also provides information about any addition to the thumb drive (e.g. a new file copied onto the drive). We exclude very large removable drives since they might have too many files. type: CLIENT_EVENT parameters: - name: maxDriveSize description: We ignore removable drives larger than this size in bytes. default: \u0026quot;32000000000\u0026quot; sources: - queries: - | LET removable_disks = SELECT Name AS Drive, atoi(string=Data.Size) AS Size FROM glob(globs=\u0026quot;/*\u0026quot;, accessor=\u0026quot;file\u0026quot;) WHERE Data.Description =~ \u0026quot;Removable\u0026quot; AND Size \u0026lt; atoi(string=maxDriveSize) - | LET file_listing = SELECT FullPath, timestamp(epoch=Mtime.Sec) As Modified, Size FROM glob(globs=Drive+\u0026quot;\\\\**\u0026quot;, accessor=\u0026quot;file\u0026quot;) LIMIT 1000 - | SELECT * FROM diff( query={ SELECT * FROM foreach( row=removable_disks, query=file_listing) }, key=\u0026quot;FullPath\u0026quot;, period=10) WHERE Diff = \u0026quot;added\u0026quot;    Windows.Detection.Thumbdrives.OfficeKeywords Users inserting Thumb drives or other Removable drive pose a constant security risk. The external drive may contain malware or other undesirable content. Additionally thumb drives are an easy way for users to exfiltrate documents.\nThis artifact automatically scans any office files copied to a removable drive for keywords. This could be useful to detect exfiltration attempts of restricted documents.\nWe exclude very large removable drives since they might have too many files.\n   Arg Default Description     officeExtensions \\.(xls xlsm   yaraRule rule Hit {\\n strings:\\n $a = \u0026ldquo;this is my secret\u0026rdquo; wide nocase\\n $b = \u0026ldquo;this is my secret\u0026rdquo; nocase\\n\\n condition:\\n any of them\\n}\\n This yara rule will be run on document contents.      View Artifact Source   name: Windows.Detection.Thumbdrives.OfficeKeywords description: | Users inserting Thumb drives or other Removable drive pose a constant security risk. The external drive may contain malware or other undesirable content. Additionally thumb drives are an easy way for users to exfiltrate documents. This artifact automatically scans any office files copied to a removable drive for keywords. This could be useful to detect exfiltration attempts of restricted documents. We exclude very large removable drives since they might have too many files. type: CLIENT_EVENT parameters: - name: officeExtensions default: \u0026quot;\\\\.(xls|xlsm|doc|docx|ppt|pptm)$\u0026quot; - name: yaraRule description: This yara rule will be run on document contents. default: | rule Hit { strings: $a = \u0026quot;this is my secret\u0026quot; wide nocase $b = \u0026quot;this is my secret\u0026quot; nocase condition: any of them } sources: - queries: - | SELECT * FROM foreach( row = { SELECT * FROM Artifact.Windows.Detection.Thumbdrives.List() WHERE FullPath =~ officeExtensions }, query = { SELECT * FROM Artifact.Generic.Applications.Office.Keywords( yaraRule=yaraRule, searchGlob=FullPath, documentGlobs=\u0026quot;\u0026quot;) })    Windows.Detection.Thumbdrives.OfficeMacros Users inserting Thumb drives or other Removable drive pose a constant security risk. The external drive may contain malware or other undesirable content. Additionally thumb drives are an easy way for users to exfiltrate documents.\nThis artifact watches for any removable drives and scans any added office documents for VBA macros.\nWe exclude very large removable drives since they might have too many files.\n   Arg Default Description     officeExtensions \\.(xls xlsm      View Artifact Source   name: Windows.Detection.Thumbdrives.OfficeMacros description: | Users inserting Thumb drives or other Removable drive pose a constant security risk. The external drive may contain malware or other undesirable content. Additionally thumb drives are an easy way for users to exfiltrate documents. This artifact watches for any removable drives and scans any added office documents for VBA macros. We exclude very large removable drives since they might have too many files. type: CLIENT_EVENT parameters: - name: officeExtensions default: \u0026quot;\\\\.(xls|xlsm|doc|docx|ppt|pptm)$\u0026quot; sources: - queries: - | SELECT * FROM foreach( row = { SELECT * FROM Artifact.Windows.Detection.Thumbdrives.List() WHERE FullPath =~ officeExtensions }, query = { SELECT * from olevba(file=FullPath) })    Windows.Detection.WMIProcessCreation WMI Process creation is a common lateral movement technique. The attacker simply uses WMI to call the Create() method on the Win32_Process WMI object.\nThis can be easily done via the wmic.exe command or via powershell:\nwmic process call create cmd.exe    View Artifact Source   name: Windows.Detection.WMIProcessCreation description: | WMI Process creation is a common lateral movement technique. The attacker simply uses WMI to call the Create() method on the Win32_Process WMI object. This can be easily done via the wmic.exe command or via powershell:  bash wmic process call create cmd.exe\n type: CLIENT_EVENT sources: - queries: - | SELECT Parse from wmi_events( query=\u0026quot;SELECT * FROM MSFT_WmiProvider_ExecMethodAsyncEvent_Pre WHERE ObjectPath=\\\u0026quot;Win32_Process\\\u0026quot; AND MethodName=\\\u0026quot;Create\\\u0026quot;\u0026quot;, namespace=\u0026quot;ROOT/CIMV2\u0026quot;, wait=50000000)    Windows.Persistence.Debug Windows allows specific configuration of various executables via a registry key. Some keys allow defining a debugger to attach to a program as it is run. If this debugger is launched for commonly used programs (e.g. notepad) then another program can be launched at the same time (with the same privileges).\n   Arg Default Description     imageFileExecutionOptions HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Image File Execution Options\\*       View Artifact Source   name: Windows.Persistence.Debug description: | Windows allows specific configuration of various executables via a registry key. Some keys allow defining a debugger to attach to a program as it is run. If this debugger is launched for commonly used programs (e.g. notepad) then another program can be launched at the same time (with the same privileges). reference: - https://attack.mitre.org/techniques/T1183/ parameters: - name: imageFileExecutionOptions default: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Image File Execution Options\\* sources: - queries: - | SELECT Key.Name AS Program, Key.FullPath as Key, Debugger FROM read_reg_key( globs=imageFileExecutionOptions) WHERE Debugger    Windows.Persistence.PermanentWMIEvents Malware often registers a permanent event listener within WMI. When the event fires, the WMI system itself will invoke the consumer to handle the event. The malware does not need to be running at the time the event fires. Malware can use this mechanism to re-infect the machine for example.\n   Arg Default Description     namespace root/subscription       View Artifact Source   name: Windows.Persistence.PermanentWMIEvents description: | Malware often registers a permanent event listener within WMI. When the event fires, the WMI system itself will invoke the consumer to handle the event. The malware does not need to be running at the time the event fires. Malware can use this mechanism to re-infect the machine for example. parameters: - name: namespace default: root/subscription sources: - precondition: SELECT OS from info() where OS = \u0026quot;windows\u0026quot; queries: - | LET FilterToConsumerBinding = SELECT parse_string_with_regex( string=Consumer, regex=['((?P\u0026lt;namespace\u0026gt;^[^:]+):)?(?P\u0026lt;Type\u0026gt;.+?)\\\\.Name=\u0026quot;(?P\u0026lt;Name\u0026gt;.+)\u0026quot;']) as Consumer, parse_string_with_regex( string=Filter, regex=['((?P\u0026lt;namespace\u0026gt;^[^:]+):)?(?P\u0026lt;Type\u0026gt;.+?)\\\\.Name=\u0026quot;(?P\u0026lt;Name\u0026gt;.+)\u0026quot;']) as Filter FROM wmi( query=\u0026quot;SELECT * FROM __FilterToConsumerBinding\u0026quot;, namespace=namespace) - | SELECT { SELECT * FROM wmi( query=\u0026quot;SELECT * FROM \u0026quot; + Consumer.Type, namespace=if(condition=Consumer.namespace, then=Consumer.namespace, else=namespace)) WHERE Name = Consumer.Name } AS ConsumerDetails, { SELECT * FROM wmi( query=\u0026quot;SELECT * FROM \u0026quot; + Filter.Type, namespace=if(condition=Filter.namespace, then=Filter.namespace, else=namespace)) WHERE Name = Filter.Name } AS FilterDetails FROM FilterToConsumerBinding    Windows.Persistence.PowershellRegistry A common way of persistence is to install a hook into a user profile registry hive, using powershell. When the user logs in, the powershell script downloads a payload and executes it.\nThis artifact searches the user\u0026rsquo;s profile registry hive for signatures related to general Powershell execution. We use a yara signature specifically targeting the user\u0026rsquo;s profile which we extract using raw NTFS parsing (in case the user is currently logged on and the registry hive is locked).\n   Arg Default Description     yaraRule rule PowerShell {\\n strings:\\n $a = /ActiveXObject.{,500}eval/ wide nocase\\n\\n condition:\\n any of them\\n}\\n       View Artifact Source   name: Windows.Persistence.PowershellRegistry description: | A common way of persistence is to install a hook into a user profile registry hive, using powershell. When the user logs in, the powershell script downloads a payload and executes it. This artifact searches the user's profile registry hive for signatures related to general Powershell execution. We use a yara signature specifically targeting the user's profile which we extract using raw NTFS parsing (in case the user is currently logged on and the registry hive is locked). parameters: - name: yaraRule default: | rule PowerShell { strings: $a = /ActiveXObject.{,500}eval/ wide nocase condition: any of them } sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | SELECT * from foreach( row={ SELECT Name, Directory as HomeDir from Artifact.Windows.Sys.Users() WHERE Directory and Gid }, query={ SELECT File.FullPath As FullPath, String.Offset AS Off, String.HexData As Hex, upload(file=File.FullPath, accessor=\u0026quot;ntfs\u0026quot;) AS Upload FROM yara( files=\u0026quot;\\\\\\\\.\\\\\u0026quot; + HomeDir + \u0026quot;\\\\ntuser.dat\u0026quot;, accessor=\u0026quot;ntfs\u0026quot;, rules=yaraRule, context=50) })    "
},
{
        "uri": "/docs/vql_reference/tips/",
        "title": "Artifact Tips",
        "tags": [],
        "description": "",
        "content": " When writing a new artifact it helps to use the following tips to make it easier.\nDevelop artifacts locally Although the Velociraptor GUI allows to change the artifact, and collect it from remote machines this is tedious in general. It is easier to just develop and collect the artifact locally.\nSimply create a directory where you store your custom artifact, and run the artifact collector with that directory specified.\n$ mkdir /tmp/my_artifacts/ $ vi /tmp/my_artifacts/my_new_artifact.yaml .... $ velociraptor --definitions /tmp/my_artifacts artifacts collect -v My.New.Artifact.Name  Note the -v flag which emits verbose messages to the console. If you have VQL syntax errors or any issues you will be able to see that easily, edit the artifact source and re-collect it.\nPlace complex filters as column specs. In VQL you can put complex expressions in the WHERE clause in order to filter the result set. The trouble is that you can not actually see the results of the expression - the expression is simply evaluated for a boolean true/false.\nIt is more productive to place the complex expression in the column specification and then you can see what it evaluates to for each row.\nSELECT encode(string=Data.value, type=\u0026quot;hex\u0026quot;) AS Value FROM ..... WHERE Value =~ \u0026quot;ffffff7f$\u0026quot;  "
},
{
        "uri": "/about/license/",
        "title": "GNU Affero General Public License",
        "tags": [],
        "description": "",
        "content": " Version 3, 19 November 2007 Copyright © 2007 Free Software Foundation, Inc. \u0026lt;http://fsf.org/\u0026gt;\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\nPreamble The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program\u0026ndash;to make sure it remains free software for all its users.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nDevelopers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.\nA secondary benefit of defending all users\u0026rsquo; freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.\nThe GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.\nAn older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.\nThe precise terms and conditions for copying, distribution and modification follow.\nTERMS AND CONDITIONS 0. Definitions “This License” refers to version 3 of the GNU Affero General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n1. Source Code The “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work\u0026rsquo;s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n2. Basic Permissions All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n3. Protecting Users\u0026rsquo; Legal Rights From Anti-Circumvention Law No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work\u0026rsquo;s users, your or third parties\u0026rsquo; legal rights to forbid circumvention of technological measures.\n4. Conveying Verbatim Copies You may convey verbatim copies of the Program\u0026rsquo;s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n5. Conveying Modified Source Versions You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n a) The work must carry prominent notices stating that you modified it, and giving a relevant date. b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to “keep intact all notices”. c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it. d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.  A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation\u0026rsquo;s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n6. Conveying Non-Source Forms You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange. b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge. c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b. d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements. e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.  A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n7. Additional Terms “Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or d) Limiting the use for publicity purposes of names of licensors or authors of the material; or e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.  All other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n8. Termination You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n9. Acceptance Not Required for Having Copies You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n10. Automatic Licensing of Downstream Recipients Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party\u0026rsquo;s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n11. Patents A “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor\u0026rsquo;s “contributor version”.\nA contributor\u0026rsquo;s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor\u0026rsquo;s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient\u0026rsquo;s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n12. No Surrender of Others\u0026rsquo; Freedom If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n13. Remote Network Interaction; Use with the GNU General Public License Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.\n14. Revised Versions of this License The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy\u0026rsquo;s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n15. Disclaimer of Warranty THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n16. Limitation of Liability IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n17. Interpretation of Sections 15 and 16 If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\nEND OF TERMS AND CONDITIONS\nHow to Apply These Terms to Your New Programs If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n\u0026lt;one line to give the program's name and a brief idea of what it does.\u0026gt; Copyright (C) \u0026lt;year\u0026gt; \u0026lt;name of author\u0026gt; This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see \u0026lt;http://www.gnu.org/licenses/\u0026gt;.  Also add information on how to contact you by electronic and paper mail.\nIf your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a “Source” link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see \u0026lt;http://www.gnu.org/licenses/\u0026gt;.\n"
},
{
        "uri": "/docs/presentations/",
        "title": "Presentations and Workshops",
        "tags": [],
        "description": "",
        "content": "We present about Velociraptor frequently in conferences and workshops.\nThis page highlights some of the more interesting presentations we gave. It can be used as reference material. Note the dates of presentation as some material may be outdated.\n SANS Summit 2019   RSA Asia Pacific and Japan 2019   Crikeycon 2019 Training Workshop   New Zealand Internet Task Force Conference 2018   "
},
{
        "uri": "/docs/artifacts/events/",
        "title": "Windows Event Monitoring",
        "tags": [],
        "description": "These event artifacts stream monitoring events from the endpoint. We collect these events on the server.",
        "content": " Windows.Events.DNSQueries Monitor all DNS Queries and responses.\nThis artifact monitors all DNS queries and their responses seen on the endpoint. DNS is a critical source of information for intrusion detection and the best place to collect it is on the endpoint itself (Perimeter collection can only see DNS requests while the endpoint or laptop is inside the enterprise network).\nIt is recommended to collect this artifact and just archive the results. When threat intelligence emerges about a watering hole or a bad C\u0026amp;C you can use this archive to confirm if any of your endpoints have contacted this C\u0026amp;C.\n   Arg Default Description     whitelistRegex wpad.home We ignore DNS names that match this regex.      View Artifact Source   name: Windows.Events.DNSQueries description: | Monitor all DNS Queries and responses. This artifact monitors all DNS queries and their responses seen on the endpoint. DNS is a critical source of information for intrusion detection and the best place to collect it is on the endpoint itself (Perimeter collection can only see DNS requests while the endpoint or laptop is inside the enterprise network). It is recommended to collect this artifact and just archive the results. When threat intelligence emerges about a watering hole or a bad C\u0026amp;C you can use this archive to confirm if any of your endpoints have contacted this C\u0026amp;C. type: CLIENT_EVENT parameters: - name: whitelistRegex description: We ignore DNS names that match this regex. default: wpad.home sources: - precondition: SELECT OS from info() where OS = \u0026quot;windows\u0026quot; queries: - | SELECT timestamp(epoch=Time) As Time, EventType, Name, CNAME, Answers FROM dns() WHERE not Name =~ whitelistRegex reports: - type: MONITORING_DAILY template: | {{ define \u0026quot;dns\u0026quot; }} SELECT count(items=Name) AS Total, Name FROM source(client_id=ClientId, artifact='Windows.Events.DNSQueries') WHERE EventType = \u0026quot;Q\u0026quot; and not Name =~ \u0026quot;.home.$\u0026quot; GROUP BY Name ORDER BY Total desc LIMIT 1000 {{ end }} {{ $client_info := Query \u0026quot;SELECT * FROM clients(client_id=ClientId) LIMIT 1\u0026quot; }} # DNS Questions for {{ Get $client_info \u0026quot;0.OsInfo.Fqdn\u0026quot; }} The 1000 most common DNS Queries on this day are listed in the below table. Typically we are looking for two interesting anomalies: 1. Sorting by count for the most frequently called domains. If you do not recognize these it may be possible that a malware is frequently calling out to its C\u0026amp;C. 2. Examining some of the least commonly used DNS names might indicate DNS exfiltration. {{ Query \u0026quot;dns\u0026quot; | Table }} \u0026gt; The following domains are filtered out: `.home.`    Windows.Events.FailedLogBeforeSuccess Sometimes attackers will brute force an local user\u0026rsquo;s account\u0026rsquo;s password. If the account password is strong, brute force attacks are not effective and might not represent a high value event in themselves.\nHowever, if the brute force attempt succeeds, then it is a very high value event (since brute forcing a password is typically a suspicious activity).\nOn the endpoint this looks like a bunch of failed logon attempts in quick succession followed by a successful login.\nNOTE: In order for this artifact to work we need Windows to be logging failed account login. This is not on by default and should be enabled via group policy.\nhttps://docs.microsoft.com/en-us/windows/security/threat-protection/auditing/basic-audit-logon-events\nYou can set the policy in group policy managment console (gpmc): Computer Configuration\\Windows Settings\\Security Settings\\Local Policies\\Audit Policy.\n   Arg Default Description     securityLogFile C:/Windows/System32/Winevt/Logs/Security.evtx    failureCount 3 Alert if there are this many failures before the successful logon.   failedLogonTimeWindow 3600       View Artifact Source   name: Windows.Events.FailedLogBeforeSuccess description: | Sometimes attackers will brute force an local user's account's password. If the account password is strong, brute force attacks are not effective and might not represent a high value event in themselves. However, if the brute force attempt succeeds, then it is a very high value event (since brute forcing a password is typically a suspicious activity). On the endpoint this looks like a bunch of failed logon attempts in quick succession followed by a successful login. NOTE: In order for this artifact to work we need Windows to be logging failed account login. This is not on by default and should be enabled via group policy. https://docs.microsoft.com/en-us/windows/security/threat-protection/auditing/basic-audit-logon-events You can set the policy in group policy managment console (gpmc): Computer Configuration\\Windows Settings\\Security Settings\\Local Policies\\Audit Policy. type: CLIENT_EVENT parameters: - name: securityLogFile default: \u0026gt;- C:/Windows/System32/Winevt/Logs/Security.evtx - name: failureCount description: Alert if there are this many failures before the successful logon. default: 3 - name: failedLogonTimeWindow default: 3600 sources: - precondition: SELECT OS FROM info() where OS = 'windows' queries: - | LET failed_logon = SELECT EventData as FailedEventData, System as FailedSystem FROM watch_evtx(filename=securityLogFile) WHERE System.EventID.Value = 4625 - | LET last_5_events = SELECT FailedEventData, FailedSystem FROM fifo(query=failed_logon, max_rows=500, max_age=atoi(string=failedLogonTimeWindow)) # Force the fifo to materialize. - | LET foo \u0026lt;= SELECT * FROM last_5_events - | LET success_logon = SELECT EventData as SuccessEventData, System as SuccessSystem FROM watch_evtx(filename=securityLogFile) WHERE System.EventID.Value = 4624 - | SELECT * FROM foreach( row=success_logon, query={ SELECT SuccessSystem.TimeCreated.SystemTime AS LogonTime, SuccessSystem, SuccessEventData, enumerate(items=FailedEventData) as FailedEventData, FailedSystem, count(items=SuccessSystem) as Count FROM last_5_events WHERE FailedEventData.SubjectUserName = SuccessEventData.SubjectUserName GROUP BY LogonTime }) WHERE Count \u0026gt; atoi(string=failureCount)    Windows.Events.ProcessCreation Collect all process creation events.\n   Arg Default Description     wmiQuery SELECT * FROM __InstanceCreationEvent WITHIN 1 WHERE TargetInstance ISA \u0026lsquo;Win32_Process\u0026rsquo;    eventQuery SELECT * FROM Win32_ProcessStartTrace       View Artifact Source   name: Windows.Events.ProcessCreation description: | Collect all process creation events. type: CLIENT_EVENT parameters: # This query will not see processes that complete within 1 second. - name: wmiQuery default: SELECT * FROM __InstanceCreationEvent WITHIN 1 WHERE TargetInstance ISA 'Win32_Process' # This query is faster but contains less data. If the process # terminates too quickly we miss its commandline. - name: eventQuery default: SELECT * FROM Win32_ProcessStartTrace sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | // Convert the timestamp from WinFileTime to Epoch. SELECT timestamp(epoch=atoi(string=Parse.TIME_CREATED) / 10000000 - 11644473600 ) as Timestamp, Parse.ParentProcessID as PPID, Parse.ProcessID as PID, Parse.ProcessName as Name, { SELECT CommandLine FROM wmi( query=\u0026quot;SELECT * FROM Win32_Process WHERE ProcessID = \u0026quot; + format(format=\u0026quot;%v\u0026quot;, args=Parse.ProcessID), namespace=\u0026quot;ROOT/CIMV2\u0026quot;) } AS CommandLine, { SELECT CommandLine FROM wmi( query=\u0026quot;SELECT * FROM Win32_Process WHERE ProcessID = \u0026quot; + format(format=\u0026quot;%v\u0026quot;, args=Parse.ParentProcessID), namespace=\u0026quot;ROOT/CIMV2\u0026quot;) } AS ParentInfo FROM wmi_events( query=eventQuery, wait=5000000, // Do not time out. namespace=\u0026quot;ROOT/CIMV2\u0026quot;)    Windows.Events.ServiceCreation Monitor for creation of new services.\nNew services are typically created by installing new software or kernel drivers. Attackers will sometimes install a new service to either insert a malicious kernel driver or as a persistence mechanism.\nThis event monitor extracts the service creation events from the event log and records them on the server.\n   Arg Default Description     systemLogFile C:/Windows/System32/Winevt/Logs/System.evtx       View Artifact Source   name: Windows.Events.ServiceCreation description: | Monitor for creation of new services. New services are typically created by installing new software or kernel drivers. Attackers will sometimes install a new service to either insert a malicious kernel driver or as a persistence mechanism. This event monitor extracts the service creation events from the event log and records them on the server. type: CLIENT_EVENT parameters: - name: systemLogFile default: \u0026gt;- C:/Windows/System32/Winevt/Logs/System.evtx sources: - precondition: SELECT OS from info() where OS = \u0026quot;windows\u0026quot; queries: - | SELECT System.TimeCreated.SystemTime as Timestamp, System.EventID.Value as EventID, EventData.ImagePath as ImagePath, EventData.ServiceName as ServiceName, EventData.ServiceType as Type, System.Security.UserID as UserSID, EventData as _EventData, System as _System FROM watch_evtx(filename=systemLogFile) WHERE EventID = 7045    "
},
{
        "uri": "/docs/artifacts/misc/",
        "title": "Miscelaneous Artifacts",
        "tags": [],
        "description": "Various Artifacts which do not fit into other categories.",
        "content": " Admin.Client.Upgrade Remotely push new client updates.\nNOTE: The updates can be pulled from any web server. You need to ensure they are properly secured with SSL and at least a random nonce in their path. You may configure the Velociraptor server to serve these through the public directory.\n   Arg Default Description     clientURL http://127.0.0.1:8000/public/velociraptor.exe    configURL http://127.0.0.1:8000/public/client.config.yaml       View Artifact Source   name: Admin.Client.Upgrade description: | Remotely push new client updates. NOTE: The updates can be pulled from any web server. You need to ensure they are properly secured with SSL and at least a random nonce in their path. You may configure the Velociraptor server to serve these through the public directory. parameters: - name: clientURL default: http://127.0.0.1:8000/public/velociraptor.exe - name: configURL default: http://127.0.0.1:8000/public/client.config.yaml sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | /* This query fetches the binary and config and stores them in temp files. Note that tempfiles will be automatically cleaned at query end. */ LET tmpfiles \u0026lt;= SELECT tempfile( data=query(vql={ SELECT Content FROM http_client(url=clientURL, chunk_size=30000000) }), extension=\u0026quot;.exe\u0026quot;) as Binary, tempfile( data=query(vql={ SELECT Content FROM http_client(url=configURL) })) as Config from scope() - | // Run the installer. SELECT * from foreach( row=tmpfiles, query={ SELECT * from execve( argv=[Binary, \u0026quot;--config\u0026quot;, Config, \u0026quot;-v\u0026quot;, \u0026quot;service\u0026quot;, \u0026quot;install\u0026quot;] ) })    Admin.Events.PostProcessUploads Sometimes we would like to post process uploads collected as part of the hunt\u0026rsquo;s artifact collections\nPost processing means to watch the hunt for completed flows and run a post processing command on the files obtained from each host.\nThe command will receive the list of paths of the files uploaded by the artifact. We dont actually care what the command does with those files - we will just relay our stdout/stderr to the artifact\u0026rsquo;s result set.\n   Arg Default Description     uploadPostProcessCommand [\u0026ldquo;/bin/ls\u0026rdquo;, \u0026ldquo;-l\u0026rdquo;]\\n The command to run - must be a json array of strings! The list\\nof files will be appended to the end of the command.\\n   uploadPostProcessArtifact Windows.Registry.NTUser.Upload The name of the artifact to watch.\\n      View Artifact Source   name: Admin.Events.PostProcessUploads description: | Sometimes we would like to post process uploads collected as part of the hunt's artifact collections Post processing means to watch the hunt for completed flows and run a post processing command on the files obtained from each host. The command will receive the list of paths of the files uploaded by the artifact. We dont actually care what the command does with those files - we will just relay our stdout/stderr to the artifact's result set. type: SERVER_EVENT parameters: - name: uploadPostProcessCommand description: | The command to run - must be a json array of strings! The list of files will be appended to the end of the command. default: | [\u0026quot;/bin/ls\u0026quot;, \u0026quot;-l\u0026quot;] - name: uploadPostProcessArtifact description: | The name of the artifact to watch. default: Windows.Registry.NTUser.Upload sources: - precondition: SELECT server_config FROM scope() queries: - | LET files = SELECT Flow, array(a1=parse_json_array(data=uploadPostProcessCommand), a2=file_store(path=Flow.FlowContext.uploaded_files)) as Argv FROM watch_monitoring(artifact='System.Flow.Completion') WHERE uploadPostProcessArtifact in Flow.FlowContext.artifacts - | SELECT * from foreach( row=files, query={ SELECT Flow.Urn as FlowUrn, Argv, Stdout, Stderr, ReturnCode FROM execve(argv=Argv) })    Admin.System.CompressUploads Compresses all uploaded files.\nWhen artifacts collect files they are normally stored on the server uncompressed. This artifact watches all completed flows and compresses the files in the file store when the flow completes. This is very useful for cloud based deployments with limited storage space or when collecting large files.\nIn order to run this artifact you would normally run it as part of an artifact acquisition process:\n$ velociraptor --config /etc/server.config.yaml artifacts acquire Admin.System.CompressUploads  Note that there is nothing special about compressed files - you can also just run find and gzip in the file store. Velociraptor will automatically decompress the file when displaying it in the GUI text/hexdump etc.\n   Arg Default Description     blacklistCompressionFilename (?i).+ntuser.dat Filenames which match this regex will be excluded from compression.      View Artifact Source   name: Admin.System.CompressUploads description: | Compresses all uploaded files. When artifacts collect files they are normally stored on the server uncompressed. This artifact watches all completed flows and compresses the files in the file store when the flow completes. This is very useful for cloud based deployments with limited storage space or when collecting large files. In order to run this artifact you would normally run it as part of an artifact acquisition process:  $ velociraptor \u0026ndash;config /etc/server.config.yaml artifacts acquire Admin.System.CompressUploads\n Note that there is nothing special about compressed files - you can also just run `find` and `gzip` in the file store. Velociraptor will automatically decompress the file when displaying it in the GUI text/hexdump etc. type: SERVER_EVENT parameters: - name: blacklistCompressionFilename description: Filenames which match this regex will be excluded from compression. default: '(?i).+ntuser.dat' sources: - precondition: SELECT server_config FROM scope() queries: - | LET files = SELECT ClientId, Flow.Urn as Flow, Flow.FlowContext.uploaded_files as Files FROM watch_monitoring(artifact='System.Flow.Completion') WHERE Files and not Files =~ blacklistCompressionFilename - | SELECT ClientId, Flow, Files, compress(path=Files) as CompressedFiles FROM files    Demo.Plugins.Fifo This is a demo of the fifo() plugin. The Fifo plugin collects and caches rows from its inner query. Every subsequent execution of the query then reads from the cache. The plugin will expire old rows depending on its expiration policy - so we always see recent rows.\nYou can use this to build queries which consider historical events together with current events at the same time. In this example, we check for a successful logon preceded by a number of failed logon attempts.\nIn this example, we use the clock() plugin to simulate events. We simulate failed logon attempts using the clock() plugin every second. By feeding the failed logon events to the fifo() plugin we ensure the fifo() plugin cache contains the last 5 failed logon events.\nWe simulate a successful logon event every 3 seconds, again using the clock plugin. Once a successful logon event is detected, we go back over the last 5 login events, count them and collect the last failed logon times (using the GROUP BY operator we group the FailedTime for every unique SuccessTime).\nIf we receive more than 3 events, we emit the row.\nThis now represents a high value signal! It will only occur when a successful logon event is preceded by at least 3 failed logon events in the last hour. It is now possible to escalate this on the server via email or other alerts.\nHere is sample output:\n.. code-block:: json\n{ \u0026quot;Count\u0026quot;: 5, \u0026quot;FailedTime\u0026quot;: [ 1549527272, 1549527273, 1549527274, 1549527275, 1549527276 ], \u0026quot;SuccessTime\u0026quot;: 1549527277 }  Of course in the real artifact we would want to include more information than just times (i.e. who logged on to where etc).\n  View Artifact Source   name: Demo.Plugins.Fifo description: | This is a demo of the fifo() plugin. The Fifo plugin collects and caches rows from its inner query. Every subsequent execution of the query then reads from the cache. The plugin will expire old rows depending on its expiration policy - so we always see recent rows. You can use this to build queries which consider historical events together with current events at the same time. In this example, we check for a successful logon preceded by a number of failed logon attempts. In this example, we use the clock() plugin to simulate events. We simulate failed logon attempts using the clock() plugin every second. By feeding the failed logon events to the fifo() plugin we ensure the fifo() plugin cache contains the last 5 failed logon events. We simulate a successful logon event every 3 seconds, again using the clock plugin. Once a successful logon event is detected, we go back over the last 5 login events, count them and collect the last failed logon times (using the GROUP BY operator we group the FailedTime for every unique SuccessTime). If we receive more than 3 events, we emit the row. This now represents a high value signal! It will only occur when a successful logon event is preceded by at least 3 failed logon events in the last hour. It is now possible to escalate this on the server via email or other alerts. Here is sample output: .. code-block:: json { \u0026quot;Count\u0026quot;: 5, \u0026quot;FailedTime\u0026quot;: [ 1549527272, 1549527273, 1549527274, 1549527275, 1549527276 ], \u0026quot;SuccessTime\u0026quot;: 1549527277 } Of course in the real artifact we would want to include more information than just times (i.e. who logged on to where etc). type: CLIENT_EVENT sources: - queries: # This query simulates failed logon attempts. - | LET failed_logon = SELECT Unix as FailedTime from clock(period=1) # This is the fifo which holds the last 5 failed logon attempts # within the last hour. - | LET last_5_events = SELECT FailedTime FROM fifo(query=failed_logon, max_rows=5, max_age=3600) # We need to get it started collecting data immediately by # materializing the cache contents. Otherwise the fifo wont # start until it is first called (i.e. the first successful # login and we will miss the failed events before hand). - | LET foo \u0026lt;= SELECT * FROM last_5_events # This simulates successful logon - we assume every 3 seonds. - | LET success_logon = SELECT Unix as SuccessTime from clock(period=3) # For each successful logon, query the last failed logon # attempts from the fifo(). We also count the total number of # failed logons. We only actually emit results if there are more # than 3 failed logon attempts before each successful one. - | SELECT * FROM foreach( row=success_logon, query={ SELECT SuccessTime, enumerate(items=FailedTime) as FailedTime, count(items=FailedTime) as Count FROM last_5_events GROUP BY SuccessTime }) WHERE Count \u0026gt; 3    Generic.Applications.Office.Keywords Microsoft Office documents among other document format (such as LibraOffice) are actually stored in zip files. The zip file contain the document encoded as XML in a number of zip members.\nThis makes it difficult to search for keywords within office documents because the ZIP files are typically compressed.\nThis artifact searches for office documents by file extension and glob then uses the zip filesystem accessor to launch a yara scan again the uncompressed data of the document. Keywords are more likely to match when scanning the decompressed XML data.\nThe artifact returns a context around the keyword hit.\nNOTE: The InternalMtime column shows the creation time of the zip member within the document which may represent when the document was initially created.\nSee https://en.wikipedia.org/wiki/List_of_Microsoft_Office_filename_extensions https://wiki.openoffice.org/wiki/Documentation/OOo3_User_Guides/Getting_Started/File_formats\n   Arg Default Description     documentGlobs /*.{docx,docm,dotx,dotm,docb,xlsx,xlsm,xltx,xltm,pptx,pptm,potx,potm,ppam,ppsx,ppsm,sldx,sldm,odt,ott,oth,odm}    searchGlob C:\\Users\\**    yaraRule rule Hit {\\n strings:\\n $a = \u0026ldquo;secret\u0026rdquo; wide nocase\\n $b = \u0026ldquo;secret\u0026rdquo; nocase\\n\\n condition:\\n any of them\\n}\\n       View Artifact Source   name: Generic.Applications.Office.Keywords description: | Microsoft Office documents among other document format (such as LibraOffice) are actually stored in zip files. The zip file contain the document encoded as XML in a number of zip members. This makes it difficult to search for keywords within office documents because the ZIP files are typically compressed. This artifact searches for office documents by file extension and glob then uses the zip filesystem accessor to launch a yara scan again the uncompressed data of the document. Keywords are more likely to match when scanning the decompressed XML data. The artifact returns a context around the keyword hit. NOTE: The InternalMtime column shows the creation time of the zip member within the document which may represent when the document was initially created. See https://en.wikipedia.org/wiki/List_of_Microsoft_Office_filename_extensions https://wiki.openoffice.org/wiki/Documentation/OOo3_User_Guides/Getting_Started/File_formats parameters: - name: documentGlobs default: /*.{docx,docm,dotx,dotm,docb,xlsx,xlsm,xltx,xltm,pptx,pptm,potx,potm,ppam,ppsx,ppsm,sldx,sldm,odt,ott,oth,odm} - name: searchGlob default: C:\\Users\\** - name: yaraRule default: | rule Hit { strings: $a = \u0026quot;secret\u0026quot; wide nocase $b = \u0026quot;secret\u0026quot; nocase condition: any of them } sources: - queries: - | LET office_docs = SELECT FullPath AS OfficePath, timestamp(epoch=Mtime.Sec) as OfficeMtime, Size as OfficeSize FROM glob(globs=searchGlob + documentGlobs) # A list of zip members inside the doc that have some content. - | LET document_parts = SELECT OfficePath, FullPath AS ZipMemberPath FROM glob(globs=url( scheme=\u0026quot;file\u0026quot;, path=OfficePath, fragment=\u0026quot;/**\u0026quot;).String, accessor='zip') WHERE not IsDir and Size \u0026gt; 0 # For each document, scan all its parts for the keyword. - | SELECT OfficePath, OfficeMtime, OfficeSize, File.ModTime as InternalMtime, String.HexData as HexContext FROM foreach( row=office_docs, query={ SELECT File, String, OfficePath, OfficeMtime, OfficeSize FROM yara( rules=yaraRule, files=document_parts.ZipMemberPath, context=200, accessor='zip') })    Generic.Client.Stats An Event artifact which generates client\u0026rsquo;s CPU and memory statistics.\n   Arg Default Description     Frequency 10 Return stats every this many seconds.      View Artifact Source   name: Generic.Client.Stats description: An Event artifact which generates client's CPU and memory statistics. parameters: - name: Frequency description: Return stats every this many seconds. default: \u0026quot;10\u0026quot; type: CLIENT_EVENT sources: - queries: - | SELECT * from foreach( row={ SELECT UnixNano FROM clock(period=atoi(string=Frequency)) }, query={ SELECT UnixNano / 1000000000 as Timestamp, Times.user + Times.system as CPU, MemoryInfo.RSS as RSS FROM pslist(pid=getpid()) }) reports: - type: SERVER_EVENT template: | {{ define \u0026quot;resources\u0026quot; }} SELECT Timestamp, rate(x=CPU, y=Timestamp) * 100 As CPUPercent, RSS / 1000000 AS MemoryUse FROM source() WHERE CPUPercent \u0026gt;= 0 {{ end }} {{ Query \u0026quot;resources\u0026quot; | LineChart \u0026quot;xaxis_mode\u0026quot; \u0026quot;time\u0026quot; \u0026quot;RSS.yaxis\u0026quot; 2 }} - type: MONITORING_DAILY template: | {{ define \u0026quot;resources\u0026quot; }} SELECT Timestamp, rate(x=CPU, y=Timestamp) * 100 As CPUPercent, RSS / 1000000 AS MemoryUse FROM source() WHERE CPUPercent \u0026gt;= 0 {{ end }} {{ $client_info := Query \u0026quot;SELECT * FROM clients(client_id=ClientId) LIMIT 1\u0026quot; }} # Client Footprint for {{ Get $client_info \u0026quot;0.OsInfo.Fqdn\u0026quot; }} The client has a client ID of {{ Get $client_info \u0026quot;0.ClientId\u0026quot; }}. Clients report the Velociraptor process footprint to the server every 10 seconds. The data includes the total CPU utilization, and the resident memory size used by the client. The following graph shows the total utilization. Memory utilization is meausred in `Mb` while CPU Utilization is measured by `Percent of one core`. We would expect the client to use around 1-5% of one core when idle, but if a heavy hunt is running this might climb substantially. {{ Query \u0026quot;resources\u0026quot; | LineChart \u0026quot;xaxis_mode\u0026quot; \u0026quot;time\u0026quot; \u0026quot;RSS.yaxis\u0026quot; 2 }} ## VQL Query The following VQL query was used to plot the graph above. ```sql {{ template \u0026quot;resources\u0026quot; }} ``` \u0026gt; To learn about managing end point performance with Velociraptor see the [blog post](https://docs.velociraptor.velocidex.com/blog/html/2019/02/10/velociraptor_performance.html).    Generic.Forensic.Carving.URLs Carve URLs from files located in a glob. Note that we do not parse any files - we simply carve anything that looks like a URL.\n   Arg Default Description     UrlGlob [\u0026ldquo;C:/Documents and Settings/*/Local Settings/Application Data/Google/Chrome/User Data/\u0026rdquo;,\\n \u0026ldquo;C:/Users/*/AppData/Local/Google/Chrome/User Data/\u0026rdquo;,\\n \u0026ldquo;C:/Documents and Settings/*/Local Settings/History/\u0026rdquo;,\\n \u0026ldquo;C:/Documents and Settings/*/Local Settings/Temporary Internet Files/\u0026rdquo;,\\n \u0026ldquo;C:/Users/*/AppData/Local/Microsoft/Windows/WebCache/\u0026rdquo;,\\n \u0026ldquo;C:/Users/*/AppData/Local/Microsoft/Windows/INetCache/\u0026rdquo;,\\n \u0026ldquo;C:/Users/*/AppData/Local/Microsoft/Windows/INetCookies/\u0026rdquo;,\\n \u0026ldquo;C:/Users/*/AppData/Roaming/Mozilla/Firefox/Profiles/\u0026rdquo;,\\n \u0026ldquo;C:/Documents and Settings/*/Application Data/Mozilla/Firefox/Profiles/**\u0026rdquo;\\n ]\\n       View Artifact Source   name: Generic.Forensic.Carving.URLs description: | Carve URLs from files located in a glob. Note that we do not parse any files - we simply carve anything that looks like a URL. parameters: - name: UrlGlob default: | [\u0026quot;C:/Documents and Settings/*/Local Settings/Application Data/Google/Chrome/User Data/**\u0026quot;, \u0026quot;C:/Users/*/AppData/Local/Google/Chrome/User Data/**\u0026quot;, \u0026quot;C:/Documents and Settings/*/Local Settings/History/**\u0026quot;, \u0026quot;C:/Documents and Settings/*/Local Settings/Temporary Internet Files/**\u0026quot;, \u0026quot;C:/Users/*/AppData/Local/Microsoft/Windows/WebCache/**\u0026quot;, \u0026quot;C:/Users/*/AppData/Local/Microsoft/Windows/INetCache/**\u0026quot;, \u0026quot;C:/Users/*/AppData/Local/Microsoft/Windows/INetCookies/**\u0026quot;, \u0026quot;C:/Users/*/AppData/Roaming/Mozilla/Firefox/Profiles/**\u0026quot;, \u0026quot;C:/Documents and Settings/*/Application Data/Mozilla/Firefox/Profiles/**\u0026quot; ] sources: - queries: - | LET matching = SELECT FullPath FROM glob( globs=parse_json_array(data=UrlGlob)) - | SELECT FullPath, URL FROM foreach( row=matching, query={ SELECT FullPath, URL FROM parse_records_with_regex(file=FullPath, regex=\u0026quot;(?P\u0026lt;URL\u0026gt;https?:\\\\/\\\\/[\\\\w\\\\.-]+[\\\\/\\\\w \\\\.-]*)\u0026quot;) })    Generic.Forensic.Timeline This artifact generates a timeline of a file glob in bodyfile format. We currently do not calculate the md5 because it is quite expensive.\n   Arg Default Description     timelineGlob C:\\Users\\**    timelineAccessor file       View Artifact Source   name: Generic.Forensic.Timeline description: | This artifact generates a timeline of a file glob in bodyfile format. We currently do not calculate the md5 because it is quite expensive. parameters: - name: timelineGlob default: C:\\Users\\** - name: timelineAccessor default: file sources: # For NTFS accessors we write the MFT id as the inode. On windows # the file accessor does not give the inode at all. - precondition: SELECT OS From info() where OS = 'windows' AND timelineAccessor = 'ntfs' queries: - | SELECT 0 AS Md5, FullPath, Sys.mft as Inode, Mode.String AS Mode, 0 as Uid, 0 as Gid, Size, Atime.Sec AS Atime, Mtime.Sec AS Mtime, Ctime.Sec AS Ctime FROM glob(globs=timelineGlob, accessor=timelineAccessor) # For linux we can get the Inode from Sys.Ino - precondition: SELECT * From scope() where timelineAccessor = 'file' queries: - | SELECT 0 AS Md5, FullPath, Sys.Ino as Inode, Mode.String AS Mode, Sys.Uid AS Uid, Sys.Gid AS Gid, Size, Atime.Sec AS Atime, Mtime.Sec AS Mtime, Ctime.Sec AS Ctime FROM glob(globs=timelineGlob, accessor=timelineAccessor)    Network.ExternalIpAddress Detect the external ip address of the end point.\n   Arg Default Description     externalUrl http://www.myexternalip.com/raw The URL of the external IP detection site.      View Artifact Source   name: Network.ExternalIpAddress description: Detect the external ip address of the end point. parameters: - name: externalUrl default: http://www.myexternalip.com/raw description: The URL of the external IP detection site. sources: - precondition: SELECT * from info() queries: - | SELECT Content as IP from http_client(url=externalUrl)    Reporting.Hunts.Details Report details about which client ran each hunt, how long it took and if it has completed.\n  View Artifact Source   name: Reporting.Hunts.Details description: | Report details about which client ran each hunt, how long it took and if it has completed. sources: - precondition: SELECT server_config FROM scope() queries: - | LET hunts = SELECT basename(path=hunt_id) as hunt_id, create_time, hunt_description FROM hunts() order by create_time desc limit 6 - | LET flows = select hunt_id, hunt_description, Fqdn, ClientId, { SELECT os_info.system FROM clients(search=ClientId) } as OS, timestamp(epoch=Flow.FlowContext.create_time/1000000) as create_time, basename(path=Flow.Urn) as flow_id, (Flow.FlowContext.active_time - Flow.FlowContext.create_time) / 1000000 as Duration, format(format='%v', args=[Flow.FlowContext.state]) as State FROM hunt_flows(hunt_id=hunt_id) order by create_time desc - | SELECT * from foreach(row=hunts, query=flows)    System.Hunt.Participation Endpoints may participate in hunts. This artifact collects which hunt each system participated in.\nNote: This is an automated system hunt. You do not need to start it.\n  View Artifact Source   name: System.Hunt.Participation description: | Endpoints may participate in hunts. This artifact collects which hunt each system participated in. Note: This is an automated system hunt. You do not need to start it. reports: - type: MONITORING_DAILY template: | {{ define \u0026quot;all_hunts\u0026quot; }}LET allhunts \u0026lt;= SELECT * FROM hunts(){{ end }} {{ define \u0026quot;hunts\u0026quot; }} SELECT * FROM foreach( row={ SELECT timestamp(epoch=Timestamp) AS Scheduled, HuntId as ParticipatedHuntId FROM source(client_id=ClientId, artifact='System.Hunt.Participation') }, query={ SELECT Scheduled, HuntId, HuntDescription, StartRequest.Args.Artifacts.Names FROM allhunts WHERE HuntId = ParticipatedHuntId }) {{ end }} {{ $client_info := Query \u0026quot;SELECT * FROM clients(client_id=ClientId) LIMIT 1\u0026quot; }} # Hunt participation for {{ Get $client_info \u0026quot;0.OsInfo.Fqdn\u0026quot; }} The client with a client ID of {{ Get $client_info \u0026quot;0.ClientId\u0026quot; }} participated in some hunts today. {{ Query \u0026quot;all_hunts\u0026quot; \u0026quot;hunts\u0026quot; | Table }} ## VQL Query The following VQL query was used to plot the graph above. ```sql {{ template \u0026quot;hunts\u0026quot; }} ```    Windows.Analysis.EvidenceOfExecution In many investigations it is useful to find evidence of program execution.\nThis artifact combines the findings of several other collectors into an overview of all program execution artifacts. The associated report walks the user through the analysis of the findings.\n  View Artifact Source   name: Windows.Analysis.EvidenceOfExecution description: | In many investigations it is useful to find evidence of program execution. This artifact combines the findings of several other collectors into an overview of all program execution artifacts. The associated report walks the user through the analysis of the findings. sources: - name: UserAssist queries: - SELECT * FROM Artifact.Windows.Registry.UserAssist() - name: Timeline queries: - SELECT * FROM Artifact.Windows.Forensics.Timeline() - name: Recent Apps queries: - SELECT * FROM Artifact.Windows.Forensics.RecentApps() - name: ShimCache queries: - SELECT * FROM Artifact.Windows.Registery.AppCompatCache() - name: Prefetch queries: - SELECT * FROM Artifact.Windows.Forensics.Prefetch()    Windows.Applications.ChocolateyPackages Chocolatey packages installed in a system.\n   Arg Default Description     ChocolateyInstall        View Artifact Source   name: Windows.Applications.ChocolateyPackages description: Chocolatey packages installed in a system. parameters: - name: ChocolateyInstall default: \u0026quot;\u0026quot; sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | LET files = SELECT FullPath, parse_xml(file=FullPath) AS Metadata -- Use the ChocolateyInstall parameter if it is set. FROM glob(globs=if( condition=ChocolateyInstall, then=ChocolateyInstall, -- Otherwise just use the environment. else=environ(var='ChocolateyInstall')) + '/lib/*/*.nuspec') - | SELECT * FROM if( condition={ SELECT * FROM if( condition=ChocolateyInstall, then=ChocolateyInstall, else=environ(var=\u0026quot;ChocolateyInstall\u0026quot;)) }, then={ SELECT FullPath, Metadata.package.metadata.id as Name, Metadata.package.metadata.version as Version, Metadata.package.metadata.summary as Summary, Metadata.package.metadata.authors as Authors, Metadata.package.metadata.licenseUrl as License FROM files })    Windows.Applications.Chrome.Cookies Enumerate the users chrome cookies.\nThe cookies are typically encrypted by the DPAPI using the user\u0026rsquo;s credentials. Since Velociraptor is typically not running in the user context we can not decrypt these. It may be possible to decrypt the cookies off line.\nThe pertinent information from a forensic point of view is the user\u0026rsquo;s Created and LastAccess timestamp and the fact that the user has actually visited the site and obtained a cookie.\n   Arg Default Description     cookieGlobs \\AppData\\Local\\Google\\Chrome\\User Data\\*\\Cookies    cookieSQLQuery SELECT creation_utc, host_key, name, value, path, expires_utc,\\n last_access_utc, encrypted_value\\nFROM cookies\\n       View Artifact Source   name: Windows.Applications.Chrome.Cookies description: | Enumerate the users chrome cookies. The cookies are typically encrypted by the DPAPI using the user's credentials. Since Velociraptor is typically not running in the user context we can not decrypt these. It may be possible to decrypt the cookies off line. The pertinent information from a forensic point of view is the user's Created and LastAccess timestamp and the fact that the user has actually visited the site and obtained a cookie. parameters: - name: cookieGlobs default: \\AppData\\Local\\Google\\Chrome\\User Data\\*\\Cookies - name: cookieSQLQuery default: | SELECT creation_utc, host_key, name, value, path, expires_utc, last_access_utc, encrypted_value FROM cookies precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | LET cookie_files = SELECT * from foreach( row={ SELECT Uid, Name AS User, Directory FROM Artifact.Windows.Sys.Users() }, query={ SELECT User, FullPath, Mtime from glob( globs=Directory + cookieGlobs) }) - | SELECT * FROM foreach(row=cookie_files, query={ SELECT timestamp(winfiletime=creation_utc * 10) as Created, timestamp(winfiletime=last_access_utc * 10) as LastAccess, timestamp(winfiletime=expires_utc * 10) as Expires, host_key, name, path, value, base64encode(string=encrypted_value) as EncryptedValue FROM sqlite( file=FullPath, query=cookieSQLQuery) })    Windows.Applications.Chrome.Extensions Fetch Chrome extensions.\nChrome extensions are installed into the user\u0026rsquo;s home directory. We search for manifest.json files in a known path within each system user\u0026rsquo;s home directory. We then parse the manifest file as JSON.\nMany extensions use locale packs to resolve strings like name and description. In this case we detect the default locale and load those locale files. We then resolve the extension\u0026rsquo;s name and description from there.\n   Arg Default Description     extensionGlobs \\AppData\\Local\\Google\\Chrome\\User Data\\*\\Extensions\\*\\*\\manifest.json       View Artifact Source   name: Windows.Applications.Chrome.Extensions description: | Fetch Chrome extensions. Chrome extensions are installed into the user's home directory. We search for manifest.json files in a known path within each system user's home directory. We then parse the manifest file as JSON. Many extensions use locale packs to resolve strings like name and description. In this case we detect the default locale and load those locale files. We then resolve the extension's name and description from there. parameters: - name: extensionGlobs default: \\AppData\\Local\\Google\\Chrome\\User Data\\*\\Extensions\\*\\*\\manifest.json sources: - precondition: | SELECT OS From info() where OS = 'windows' queries: - | /* For each user on the system, search for extension manifests in their home directory. */ LET extension_manifests = SELECT * from foreach( row={ SELECT Uid, Name AS User, Directory from Artifact.Windows.Sys.Users() }, query={ SELECT FullPath, Mtime, Ctime, User, Uid from glob( globs=Directory + extensionGlobs) }) - | /* If the Manifest declares a default_locale then we load and parse the messages file. In this case the messages are actually stored in the locale file instead of the main manifest.json file. */ LET maybe_read_locale_file = SELECT * from if( condition={ select * from scope() where Manifest.default_locale }, then={ SELECT Manifest, Uid, User, Filename as LocaleFilename, ManifestFilename, parse_json(data=Data) AS LocaleManifest FROM read_file( -- Munge the filename to get the messages.json path. filenames=regex_replace( source=ManifestFilename, replace=\u0026quot;\\\\_locales\\\\\u0026quot; + Manifest.default_locale + \u0026quot;\\\\messages.json\u0026quot;, re=\u0026quot;\\\\\\\\manifest.json$\u0026quot;)) }, else={ -- Just fill in empty Locale results. SELECT Manifest, Uid, User, \u0026quot;\u0026quot; AS LocaleFilename, \u0026quot;\u0026quot; AS ManifestFilename, \u0026quot;\u0026quot; AS LocaleManifest FROM scope() }) - | LET parse_json_files = SELECT * from foreach( row={ SELECT Filename as ManifestFilename, Uid, User, parse_json(data=Data) as Manifest FROM read_file(filenames=FullPath) }, query=maybe_read_locale_file) - | LET parsed_manifest_files = SELECT * from foreach( row=extension_manifests, query=parse_json_files) - | SELECT Uid, User, /* If the manifest name contains __MSG_ then the real name is stored in the locale manifest. This condition resolves the Name column either to the main manifest or the locale manifest. */ if(condition=\u0026quot;__MSG_\u0026quot; in Manifest.name, then=get(item=LocaleManifest, member=regex_replace( source=Manifest.name, replace=\u0026quot;$1\u0026quot;, re=\u0026quot;(?:__MSG_(.+)__)\u0026quot;)).message, else=Manifest.name) as Name, if(condition=\u0026quot;__MSG_\u0026quot; in Manifest.description, then=get(item=LocaleManifest, member=regex_replace( source=Manifest.description, replace=\u0026quot;$1\u0026quot;, re=\u0026quot;(?:__MSG_(.+)__)\u0026quot;)).message, else=Manifest.description) as Description, /* Get the Identifier and Version from the manifest filename */ regex_replace( source=ManifestFilename, replace=\u0026quot;$1\u0026quot;, re=\u0026quot;(?:.+Extensions\\\\\\\\([^\\\\\\\\]+)\\\\\\\\([^\\\\\\\\]+)\\\\\\\\manifest.json)$\u0026quot;) AS Identifier, regex_replace( source=ManifestFilename, replace=\u0026quot;$2\u0026quot;, re=\u0026quot;(?:.+Extensions\\\\\\\\([^\\\\\\\\]+)\\\\\\\\([^\\\\\\\\]+)\\\\\\\\manifest.json)$\u0026quot;) AS Version, Manifest.author as Author, Manifest.background.persistent AS Persistent, regex_replace( source=ManifestFilename, replace=\u0026quot;$1\u0026quot;, re=\u0026quot;(.+Extensions\\\\\\\\.+\\\\\\\\)manifest.json$\u0026quot;) AS Path, Manifest.oauth2.scopes as Scopes, Manifest.permissions as Permissions, Manifest.key as Key FROM parsed_manifest_files    Windows.Applications.Chrome.History Enumerate the users chrome history.\n   Arg Default Description     historyGlobs \\AppData\\Local\\Google\\Chrome\\User Data\\*\\History    urlSQLQuery SELECT url as visited_url, title, visit_count,\\n typed_count, last_visit_time\\nFROM urls\\n       View Artifact Source   name: Windows.Applications.Chrome.History description: | Enumerate the users chrome history. parameters: - name: historyGlobs default: \\AppData\\Local\\Google\\Chrome\\User Data\\*\\History - name: urlSQLQuery default: | SELECT url as visited_url, title, visit_count, typed_count, last_visit_time FROM urls precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | LET history_files = SELECT * from foreach( row={ SELECT Uid, Name AS User, Directory FROM Artifact.Windows.Sys.Users() }, query={ SELECT User, FullPath, Mtime from glob( globs=Directory + historyGlobs) }) - | SELECT * FROM foreach(row=history_files, query={ SELECT User, FullPath, timestamp(epoch=Mtime.Sec) as Mtime, visited_url, title, visit_count, typed_count, timestamp(winfiletime=last_visit_time * 10) as last_visit_time FROM sqlite( file=FullPath, query=urlSQLQuery) })    Windows.Applications.OfficeMacros Office macros are a favourite initial infection vector. Many users click through the warning dialogs.\nThis artifact scans through the given directory glob for common office files. We then try to extract any embedded macros by parsing the OLE file structure.\nIf a macro calls an external program (e.g. Powershell) this is very suspicious!\n   Arg Default Description     officeExtensions *.{xls,xlsm,doc,docx,ppt,pptm}    officeFileSearchGlob C:\\Users\\**\\ The directory to search for office documents.      View Artifact Source   name: Windows.Applications.OfficeMacros description: | Office macros are a favourite initial infection vector. Many users click through the warning dialogs. This artifact scans through the given directory glob for common office files. We then try to extract any embedded macros by parsing the OLE file structure. If a macro calls an external program (e.g. Powershell) this is very suspicious! parameters: - name: officeExtensions default: \u0026quot;*.{xls,xlsm,doc,docx,ppt,pptm}\u0026quot; - name: officeFileSearchGlob default: C:\\Users\\**\\ description: The directory to search for office documents. sources: - queries: - | SELECT * FROM foreach( row={ SELECT FullPath FROM glob(globs=officeFileSearchGlob + officeExtensions) }, query={ SELECT * from olevba(file=FullPath) })    Windows.Attack.ParentProcess Maps the Mitre Att\u0026amp;ck framework process executions into artifacts.\nReferences:  https://www.sans.org/security-resources/posters/hunt-evil/165/download https://github.com/teoseller/osquery-attck/blob/master/windows-incorrect_parent_process.conf     Arg Default Description     lookupTable ProcessName,ParentRegex\\nsmss.exe,System\\nruntimebroker.exe,svchost.exe\\ntaskhostw.exe,svchost.exe\\nservices.exe,wininit.exe\\nlsass.exe,wininit.exe\\nsvchost.exe,services.exe\\ncmd.exe,explorer.exe\\npowershell.exe,explorer.exe\\niexplore.exe,explorer.exe\\nfirefox.exe,explorer.exe\\nchrome.exe,explorer.exe\\n       View Artifact Source   name: Windows.Attack.ParentProcess description: | Maps the Mitre Att\u0026amp;ck framework process executions into artifacts. ### References: * https://www.sans.org/security-resources/posters/hunt-evil/165/download * https://github.com/teoseller/osquery-attck/blob/master/windows-incorrect_parent_process.conf precondition: SELECT OS From info() where OS = 'windows' parameters: - name: lookupTable default: | ProcessName,ParentRegex smss.exe,System runtimebroker.exe,svchost.exe taskhostw.exe,svchost.exe services.exe,wininit.exe lsass.exe,wininit.exe svchost.exe,services.exe cmd.exe,explorer.exe powershell.exe,explorer.exe iexplore.exe,explorer.exe firefox.exe,explorer.exe chrome.exe,explorer.exe sources: - queries: # Build up some cached queries for speed. - LET lookup \u0026lt;= SELECT * FROM parse_csv(filename=lookupTable, accessor='data') - LET processes \u0026lt;= SELECT Name, Pid, Ppid, CommandLine, CreateTime, Exe FROM pslist() - LET processes_lookup \u0026lt;= SELECT Name As ProcessName, Pid As ProcID FROM processes - | // Resolve the Ppid into a parent name using our processes_lookup LET resolved_parent_name = SELECT * FROM foreach( row={ SELECT * FROM processes}, query={ SELECT Name AS ActualProcessName, ProcessName AS ActualParentName, Pid, Ppid, CommandLine, CreateTime, Exe FROM processes_lookup WHERE ProcID = Ppid LIMIT 1 }) - | // Get the expected parent name from the table above. SELECT * FROM foreach( row=resolved_parent_name, query={ SELECT ActualProcessName, ActualParentName, Pid, Ppid, CommandLine, CreateTime, Exe, ParentRegex as ExpectedParentName FROM lookup WHERE ActualProcessName =~ ProcessName AND NOT ActualParentName =~ ParentRegex })    Windows.Attack.Prefetch Maps the Mitre Att\u0026amp;ck framework process executions into artifacts. This pack was generated from https://github.com/teoseller/osquery-attck\n  View Artifact Source   name: Windows.Attack.Prefetch description: | Maps the Mitre Att\u0026amp;ck framework process executions into artifacts. This pack was generated from https://github.com/teoseller/osquery-attck precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - SELECT Name, ModTime, Mtime.Sec AS modified FROM glob(globs=\u0026quot;C:/Windows/Prefetch/*\u0026quot;) # Reports can be MONITORING_DAILY, CLIENT, SERVER_EVENT reports: - type: CLIENT parameters: - name: lookupTable default: | signature,description attrib,Attrib Execute is usually used to modify file attributes - ATT\u0026amp;CK T1158 schtasks.exe,Schtasks Execute: usaullay used to create a scheduled task - ATT\u0026amp;CK T1053:S0111 taskeng.exe,taskeng Execute: usaullay used to create a scheduled task - ATT\u0026amp;CK T1053 tscon.exe,tscon.exe Execute: usaullay used to Terminal Services Console - ATT\u0026amp;CK T1076 mstsc.exe,mstsc.exe Execute: usaullay used to perform a RDP Session - ATT\u0026amp;CK T1076 at.exe,Schtasks Execute: usaullay used to create a scheduled task - ATT\u0026amp;CK T1053:S0110 tasklist.exe,Tasklist Execute: usaullay used to list task - ATT\u0026amp;CK T1057:T1063:T1007:S0057 taskkill.exe,Taskkill Execute: usaullay used to kill task mshta.exe,Mshta Execute: is a utility that executes Microsoft HTML Applications (HTA) - ATT\u0026amp;CK T1170 whoami.exe,Whoami Execute: used to prints the effective username of the current user xcopy.exe,Xcopy Execute: is used for copying multiple files or entire directory trees from one directory to another and for copying files across a network. esentutl.exe,Esentutl Execute: is a legitimate built-in command-line program it could be used to create a exe from dump raw source. net.exe,Net Execute: is used in command-line operations for control of users: groups: services: and network connections - ATT\u0026amp;CK T1126:T1087:T1201:T1069:S0039:T1018:T1007:T1124 vssadmin.exe,Vssadmin Execute: usaullay used to execute activity on Volume Shadow copy InstallUtil.exe,InstallUtil Execute: InstallUtil is a command-line utility that allows for installation and uninstallation of resources by executing specific installer components specified in .NET binaries - ATT\u0026amp;CK T1118 cmstp.exe,CMSTP Execute: The Microsoft Connection Manager Profile Installer (CMSTP.exe) is a command-line program used to install Connection Manager service profiles. - ATT\u0026amp;CK T1191 cmd.exe,Command-Line Interface Execute: CMD execution - ATT\u0026amp;CK T1059 cscript.exe,Command-Line Interface Execute: Cscript execution starts a script so that it runs in a command-line environment. - ATT\u0026amp;CK T1216 powershell.exe,POWERSHELL Execute: is a powerful interactive command-line interface and scripting environment included in the Windows operating system - ATT\u0026amp;CK T1086 regsvr32.exe,POWERSHELL Execute: is a powerful interactive command-line interface and scripting environment included in the Windows operating system - ATT\u0026amp;CK T1117 PsExec.exe,PsExec Execute: is a free Microsoft tool that can be used to execute a program on another computer. - ATT\u0026amp;CK T1035:S0029 runas.exe,Runas Execute: Allows a user to run specific tools and programs with different permissions than the user's current logon provides. - ATT\u0026amp;CK T1134 bitsadmin.exe,Bitsadmin Execute: Windows Background Intelligent Transfer Service (BITS) is a low-bandwidth: asynchronous file transfer mechanism exposed through Component Object Model (COM) - ATT\u0026amp;CK T1197:S0190 certutil.exe,Certutil Execute: Certutil.exe is a legitimate built-in command-line program to manage certificates in Windows - ATT\u0026amp;CK T1105:T1140:T1130:S0160 netsh.exe,Netsh Execute: Netsh.exe (also referred to as Netshell) is a command-line scripting utility used to interact with the network configuration of a system - ATT\u0026amp;CK T1128:T1063:S0108 netstat.exe,Netstat Execute: is an operating system utility that displays active TCP connections: listening ports: and network statistics. - ATT\u0026amp;CK T1049:S0104 reg.exe,Reg Execute: Reg is a Windows utility used to interact with the Windows Registry. - ATT\u0026amp;CK T1214:T1012:T1063:S0075 regedit.exe,Regedit Execute: is a Windows utility used to interact with the Windows Registry. - ATT\u0026amp;CK T1214 systeminfo.exe,Systeminfo Execute: Systeminfo is a Windows utility that can be used to gather detailed information about a computer. - ATT\u0026amp;CK T1082:S0096 sc.exe,SC.exe Execute: Service Control - Create: Start: Stop: Query or Delete any Windows SERVICE. . - ATT\u0026amp;CK T1007 template: | {{ .Description }} The below shows any prefetch files of interest and what they could potentially mean. {{ define \u0026quot;query\u0026quot; }} LET lookup \u0026lt;= SELECT * FROM parse_csv(filename=lookupTable, accessor='data') {{ end }} {{ define \u0026quot;data\u0026quot;}} LET data \u0026lt;= SELECT * FROM source() {{ end }} {{ range (Query \u0026quot;data\u0026quot; \u0026quot;query\u0026quot; \u0026quot;SELECT * FROM lookup\u0026quot;) }} {{ $rows := Query (printf \u0026quot;SELECT * FROM source() WHERE Name =~ '%v'\u0026quot; (Get . \u0026quot;signature\u0026quot;) ) }} {{ if $rows }} ## {{ Get $rows \u0026quot;0.Name\u0026quot; }} Modified on {{ Get $rows \u0026quot;0.ModTime\u0026quot; }}. {{ Get . \u0026quot;description\u0026quot; }} {{ end }} {{ end }} # Timeline {{ Query \u0026quot;SELECT modified * 1000, Name FROM foreach(row=lookup, query={ SELECT * FROM data WHERE Name =~ signature})\u0026quot; | Timeline }}    Windows.EventLogs.DHCP This artifact parses the windows dhcp event log looking for evidence of IP address assignments.\nIn some investigations it is important to be able to identify the machine which was assigned a particular IP address at a point in time. Usually these logs are available from the DHCP server, but in many cases the server logs are not available (for example, if the endpoint was visiting a different network or the DHCP server is on a wireless router with no log retention).\nOn windows, there are two types of logs:\n The first type is the admin log (Microsoft-Windows-Dhcp-Client%4Admin.evt). These only contain errors such as an endpoint trying to continue its lease, but the lease is rejected by the server.\n The operational log (Microsoft-Windows-Dhcp-Client%4Operational.evtx) contains the full log of each lease. Unfortunately this log is disabled by default. If it is available we can rely on the information.\n     Arg Default Description     eventDirGlob C:\\Windows\\system32\\winevt\\logs\\    adminLog Microsoft-Windows-Dhcp-Client%4Admin.evtx    operationalLog Microsoft-Windows-Dhcp-Client%4Operational.evtx    accessor file       View Artifact Source   name: Windows.EventLogs.DHCP description: | This artifact parses the windows dhcp event log looking for evidence of IP address assignments. In some investigations it is important to be able to identify the machine which was assigned a particular IP address at a point in time. Usually these logs are available from the DHCP server, but in many cases the server logs are not available (for example, if the endpoint was visiting a different network or the DHCP server is on a wireless router with no log retention). On windows, there are two types of logs: 1. The first type is the admin log (`Microsoft-Windows-Dhcp-Client%4Admin.evt`). These only contain errors such as an endpoint trying to continue its lease, but the lease is rejected by the server. 2. The operational log (`Microsoft-Windows-Dhcp-Client%4Operational.evtx`) contains the full log of each lease. Unfortunately this log is disabled by default. If it is available we can rely on the information. parameters: - name: eventDirGlob default: C:\\Windows\\system32\\winevt\\logs\\ - name: adminLog default: Microsoft-Windows-Dhcp-Client%4Admin.evtx - name: operationalLog default: Microsoft-Windows-Dhcp-Client%4Operational.evtx - name: accessor default: file sources: - name: RejectedDHCP queries: - | LET files = SELECT * FROM glob( globs=eventDirGlob + adminLog, accessor=accessor) - | SELECT Time AS _Time, timestamp(epoch=Time) As Timestamp, Computer, MAC, ClientIP, DHCPServer, Type FROM foreach( row=files, query={ SELECT System.TimeCreated.SystemTime as Time, System.Computer AS Computer, format(format=\u0026quot;%x:%x:%x:%x:%x:%x\u0026quot;, args=[EventData.HWAddress]) AS MAC, ip(netaddr4_le=EventData.Address1) AS ClientIP, ip(netaddr4_le=EventData.Address2) AS DHCPServer, \u0026quot;Lease Rejected\u0026quot; AS Type FROM parse_evtx(filename=FullPath, accessor=accessor) WHERE System.EventID.Value = 1002 }) - name: AssignedDHCP queries: - | LET files = SELECT * FROM glob( globs=eventDirGlob + operationalLog, accessor=accessor) - | SELECT Time AS _Time, timestamp(epoch=Time) As Timestamp, Computer, MAC, ClientIP, DHCPServer, Type FROM foreach( row=files, query={ SELECT System.TimeCreated.SystemTime as Time, System.Computer AS Computer, EventData.InterfaceGuid AS MAC, ip(netaddr4_le=EventData.Address1) AS ClientIP, ip(netaddr4_le=EventData.Address2) AS DHCPServer, \u0026quot;Lease Assigned\u0026quot; AS Type FROM parse_evtx(filename=FullPath, accessor=accessor) WHERE System.EventID.Value = 60000 }) reports: - type: CLIENT template: | Evidence of DHCP assigned IP addresses ====================================== {{ .Description }} {{ define \u0026quot;assigned_dhcp\u0026quot; }} SELECT Computer, ClientIP, count(items=Timestamp) AS Total, enumerate(items=Timestamp) AS Times FROM source(source='AssignedDHCP') GROUP BY ClientIP {{ end }} {{ define \u0026quot;rejected_dhcp\u0026quot; }} SELECT Computer, ClientIP, count(items=Timestamp) AS Total, enumerate(items=Timestamp) AS Times FROM source(source='RejectedDHCP') GROUP BY ClientIP {{ end }} {{ $assigned := Query \u0026quot;assigned_dhcp\u0026quot;}} {{ if $assigned }} ## Operational logs This machine has DHCP operational logging enabled. We therefore can see complete references to all granted leases: {{ Table $assigned }} ## Timeline {{ Query \u0026quot;SELECT _Time * 1000, ClientIP FROM source(source='AssignedDHCP')\u0026quot; | Timeline }} {{ end }} ## Admin logs The admin logs show errors with DHCP lease requests. Typically rejected leases indicate that the machine held a least on a IP address in the past, but this lease is invalid for its current environment. For example, the machine has been moved to a different network. {{ Query \u0026quot;rejected_dhcp\u0026quot; | Table }} {{ Query \u0026quot;SELECT _Time * 1000, ClientIP FROM source(source='RejectedDHCP')\u0026quot; | Timeline }}    Windows.Network.ArpCache Address resolution cache, both static and dynamic (from ARP, NDP).\n   Arg Default Description     wmiQuery SELECT AddressFamily, Store, State, InterfaceIndex, IPAddress,\\n InterfaceAlias, LinkLayerAddress\\nfrom MSFT_NetNeighbor\\n    wmiNamespace ROOT\\StandardCimv2    kMapOfState {\\n \u0026ldquo;0\u0026rdquo;: \u0026ldquo;Unreachable\u0026rdquo;,\\n \u0026ldquo;1\u0026rdquo;: \u0026ldquo;Incomplete\u0026rdquo;,\\n \u0026ldquo;2\u0026rdquo;: \u0026ldquo;Probe\u0026rdquo;,\\n \u0026ldquo;3\u0026rdquo;: \u0026ldquo;Delay\u0026rdquo;,\\n \u0026ldquo;4\u0026rdquo;: \u0026ldquo;Stale\u0026rdquo;,\\n \u0026ldquo;5\u0026rdquo;: \u0026ldquo;Reachable\u0026rdquo;,\\n \u0026ldquo;6\u0026rdquo;: \u0026ldquo;Permanent\u0026rdquo;,\\n \u0026ldquo;7\u0026rdquo;: \u0026ldquo;TBD\u0026rdquo;\\n}\\n       View Artifact Source   name: Windows.Network.ArpCache description: Address resolution cache, both static and dynamic (from ARP, NDP). parameters: - name: wmiQuery default: | SELECT AddressFamily, Store, State, InterfaceIndex, IPAddress, InterfaceAlias, LinkLayerAddress from MSFT_NetNeighbor - name: wmiNamespace default: ROOT\\StandardCimv2 - name: kMapOfState default: | { \u0026quot;0\u0026quot;: \u0026quot;Unreachable\u0026quot;, \u0026quot;1\u0026quot;: \u0026quot;Incomplete\u0026quot;, \u0026quot;2\u0026quot;: \u0026quot;Probe\u0026quot;, \u0026quot;3\u0026quot;: \u0026quot;Delay\u0026quot;, \u0026quot;4\u0026quot;: \u0026quot;Stale\u0026quot;, \u0026quot;5\u0026quot;: \u0026quot;Reachable\u0026quot;, \u0026quot;6\u0026quot;: \u0026quot;Permanent\u0026quot;, \u0026quot;7\u0026quot;: \u0026quot;TBD\u0026quot; } sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | LET interfaces \u0026lt;= SELECT Index, HardwareAddr, IP FROM Artifact.Windows.Network.InterfaceAddresses() - | LET arp_cache = SELECT if(condition=AddressFamily=23, then=\u0026quot;IPv6\u0026quot;, else=if(condition=AddressFamily=2, then=\u0026quot;IPv4\u0026quot;, else=AddressFamily)) as AddressFamily, if(condition=Store=0, then=\u0026quot;Persistent\u0026quot;, else=if(condition=(Store=1), then=\u0026quot;Active\u0026quot;, else=\u0026quot;?\u0026quot;)) as Store, get(item=parse_json(data=kMapOfState), member=encode(string=State, type='string')) AS State, InterfaceIndex, IPAddress, InterfaceAlias, LinkLayerAddress FROM wmi(query=wmiQuery, namespace=wmiNamespace) - | SELECT * FROM foreach( row=arp_cache, query={ SELECT AddressFamily, Store, State, InterfaceIndex, IP AS LocalAddress, HardwareAddr, IPAddress as RemoteAddress, InterfaceAlias, LinkLayerAddress AS RemoteMACAddress FROM interfaces WHERE InterfaceIndex = Index })    Windows.Network.InterfaceAddresses Network interfaces and relevant metadata.\n  View Artifact Source   name: Windows.Network.InterfaceAddresses description: Network interfaces and relevant metadata. sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | LET interface_address = SELECT Index, MTU, Name, HardwareAddr, Flags, Addrs from interfaces() - | SELECT Index, MTU, Name, HardwareAddr.String As HardwareAddr, Flags, Addrs.IP as IP, Addrs.Mask.String as Mask FROM flatten(query=interface_address)    Windows.Network.ListeningPorts Processes with listening (bound) network sockets/ports.\n  View Artifact Source   name: Windows.Network.ListeningPorts description: Processes with listening (bound) network sockets/ports. sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | LET process \u0026lt;= SELECT Name, Pid from pslist() - | SELECT * from foreach( row={ SELECT Pid AS PortPid, Laddr.Port AS Port, TypeString as Protocol, FamilyString as Family, Laddr.IP as Address FROM netstat() where Status = 'LISTEN' }, query={ SELECT Pid, Name, Port, Protocol, Family, Address FROM process where Pid = PortPid })    Windows.Network.Netstat Show information about open sockets. On windows the time when the socket was first bound is also shown.\n  View Artifact Source   name: Windows.Network.Netstat description: | Show information about open sockets. On windows the time when the socket was first bound is also shown. sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | SELECT Pid, FamilyString as Family, TypeString as Type, Status, Laddr.IP, Laddr.Port, Raddr.IP, Raddr.Port, Timestamp FROM netstat()    Windows.Packs.Autoexec Aggregate of executables that will automatically execute on the target machine. This is an amalgamation of other tables like services, scheduled_tasks, startup_items and more.\n  View Artifact Source   name: Windows.Packs.Autoexec description: | Aggregate of executables that will automatically execute on the target machine. This is an amalgamation of other tables like services, scheduled_tasks, startup_items and more. sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | SELECT * from chain( q1={ SELECT Name, Command AS Path, \u0026quot;StartupItems\u0026quot; as Source FROM Artifact.Windows.Sys.StartupItems() })    Windows.Packs.Persistence This artifact pack collects various persistence mechanisms in Windows.\n  View Artifact Source   name: Windows.Packs.Persistence description: | This artifact pack collects various persistence mechanisms in Windows. precondition: SELECT OS from info() where OS = \u0026quot;windows\u0026quot; sources: - name: WMI Event Filters description: | {{ DocFrom \u0026quot;Windows.Persistence.PermanentWMIEvents\u0026quot; }} queries: - | SELECT * FROM Artifact.Windows.Persistence.PermanentWMIEvents() - name: Startup Items description: | {{ DocFrom \u0026quot;Windows.Sys.StartupItems\u0026quot; }} queries: - | SELECT * FROM Artifact.Windows.Sys.StartupItems() - name: Debug Bootstraping description: | {{ DocFrom \u0026quot;Windows.Persistence.Debug\u0026quot; }} If there are any rows in the table below then executing the program will also launch the program listed under the Debugger column. queries: - SELECT * FROM Artifact.Windows.Persistence.Debug()    Windows.Registery.AppCompatCache Parses the system\u0026rsquo;s app compatibility cache.\n   Arg Default Description     AppCompatCacheKey HKEY_LOCAL_MACHINE/System/CurrentControlSet/Control/Session Manager/AppCompatCache/AppCompatCache       View Artifact Source   name: Windows.Registery.AppCompatCache description: | Parses the system's app compatibility cache. parameters: - name: AppCompatCacheKey default: HKEY_LOCAL_MACHINE/System/CurrentControlSet/Control/Session Manager/AppCompatCache/AppCompatCache precondition: SELECT OS From info() where OS = 'windows' sources: - queries: - | SELECT * FROM foreach( row={ SELECT Data FROM read_file( filenames=AppCompatCacheKey, accessor='reg') }, query={ SELECT name, epoch, time FROM appcompatcache(value=Data) }) WHERE epoch \u0026lt; 2000000000    Windows.Registry.NTUser This artifact searches for keys or values within the user\u0026rsquo;s NTUser.dat registry hives.\nWhen a user logs into a windows machine the system creates their own \u0026ldquo;profile\u0026rdquo; which consists of a registry hive mapped into the HKEY_USERS hive. This hive file is locked as long as the user is logged in. If the user is not logged in, the file is not mapped at all.\nThis artifact bypasses the locking mechanism by parsing the raw NTFS filesystem to recover the registry hives. We then parse the registry hives to search for the glob provided.\nThis artifact is designed to be reused by other artifacts that need to access user data.\nAny artifacts that look into the HKEY_USERS registry hive should be using the Windows.Registry.NTUser artifact instead of accessing the hive via the API. The API only makes the currently logged in users available in that hive and so if we rely on the windows API we will likely miss any settings for users not currently logged on.\n    Arg Default Description     KeyGlob Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\ComDlg32\\**    UserHomes C:\\Users\\*\\NTUSER.DAT       View Artifact Source   name: Windows.Registry.NTUser description: | This artifact searches for keys or values within the user's NTUser.dat registry hives. When a user logs into a windows machine the system creates their own \u0026quot;profile\u0026quot; which consists of a registry hive mapped into the HKEY_USERS hive. This hive file is locked as long as the user is logged in. If the user is not logged in, the file is not mapped at all. This artifact bypasses the locking mechanism by parsing the raw NTFS filesystem to recover the registry hives. We then parse the registry hives to search for the glob provided. This artifact is designed to be reused by other artifacts that need to access user data. \u0026lt;div class=\u0026quot;notices note\u0026quot; \u0026gt;\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;Any artifacts that look into the HKEY_USERS registry hive should be using the `Windows.Registry.NTUser` artifact instead of accessing the hive via the API. The API only makes the currently logged in users available in that hive and so if we rely on the windows API we will likely miss any settings for users not currently logged on. \u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; precondition: SELECT OS From info() where OS = 'windows' parameters: - name: KeyGlob default: Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\ComDlg32\\** - name: UserHomes default: C:\\Users\\*\\NTUSER.DAT sources: - queries: - | SELECT * FROM foreach( row={ SELECT FullPath FROM glob(globs=UserHomes) }, query={ SELECT FullPath, Data, Mtime.Sec AS Mtime FROM glob( globs=url(scheme=\u0026quot;ntfs\u0026quot;, path=FullPath, fragment=KeyGlob).String, accessor=\u0026quot;raw_reg\u0026quot;) })    Windows.Registry.NTUser.Upload This artifact collects all the user\u0026rsquo;s NTUser.dat registry hives.\nWhen a user logs into a windows machine the system creates their own \u0026ldquo;profile\u0026rdquo; which consists of a registry hive mapped into the HKEY_USERS hive. This hive file is locked as long as the user is logged in.\nThis artifact bypasses the locking mechanism by extracting the registry hives using raw NTFS parsing. We then just upload all hives to the server.\n  View Artifact Source   name: Windows.Registry.NTUser.Upload description: | This artifact collects all the user's NTUser.dat registry hives. When a user logs into a windows machine the system creates their own \u0026quot;profile\u0026quot; which consists of a registry hive mapped into the HKEY_USERS hive. This hive file is locked as long as the user is logged in. This artifact bypasses the locking mechanism by extracting the registry hives using raw NTFS parsing. We then just upload all hives to the server. sources: - precondition: | SELECT OS From info() where OS = 'windows' queries: - | LET users = SELECT Name, Directory as HomeDir FROM Artifact.Windows.Sys.Users() WHERE Directory - | SELECT upload(file=\u0026quot;\\\\\\\\.\\\\\u0026quot; + HomeDir + \u0026quot;\\\\ntuser.dat\u0026quot;, accessor=\u0026quot;ntfs\u0026quot;) as Upload FROM users    Windows.Registry.Sysinternals.Eulacheck Checks for the Accepted Sysinternals EULA from the registry key \u0026ldquo;HKCU\\Software\\Sysinternals[TOOL]\\\u0026ldquo;. When a Sysinternals tool is first run on a system, the EULA must be accepted. This writes a value called EulaAccepted under that key.\nNote: This artifact uses HKEY_USERS and therefore will not detect users that are not currently logged on.\n   Arg Default Description     Sysinternals_Reg_Key HKEY_USERS\\*\\Software\\Sysinternals\\*       View Artifact Source   name: Windows.Registry.Sysinternals.Eulacheck description: | Checks for the Accepted Sysinternals EULA from the registry key \u0026quot;HKCU\\Software\\Sysinternals\\[TOOL]\\\u0026quot;. When a Sysinternals tool is first run on a system, the EULA must be accepted. This writes a value called EulaAccepted under that key. Note: This artifact uses HKEY_USERS and therefore will not detect users that are not currently logged on. parameters: - name: Sysinternals_Reg_Key default: HKEY_USERS\\*\\Software\\Sysinternals\\* sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - | LET users \u0026lt;= SELECT Name, UUID FROM Artifact.Windows.Sys.Users() - | SELECT Key.Name as ProgramName, Key.FullPath as Key, timestamp(epoch=Key.Mtime.Sec) AS TimeAccepted, { SELECT Name FROM users WHERE UUID=regex_replace( source=Key.FullPath, re=\u0026quot;.+\\\\\\\\(S-[^\\\\\\\\]+)\\\\\\\\.+\u0026quot;, replace=\u0026quot;$1\u0026quot;) } as User, EulaAccepted FROM read_reg_key(globs=split(string=Sysinternals_Reg_Key, sep=',[\\\\s]*'))    Windows.Registry.UserAssist Windows systems maintain a set of keys in the registry database (UserAssist keys) to keep track of programs that executed. The number of executions and last execution date and time are available in these keys.\nThe information within the binary UserAssist values contains only statistical data on the applications launched by the user via Windows Explorer. Programs launched via the command­line (cmd.exe) do not appear in these registry keys.\nFrom a forensics perspective, being able to decode this information can be very useful.\n   Arg Default Description     UserFilter  If specified we filter by this user ID.   ExecutionTimeAfter  If specified only show executions after this time.   UserAssistKey Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\UserAssist\\*\\Count\\*    userAssistProfile {\\n \u0026ldquo;Win10\u0026rdquo;: [0, {\\n \u0026ldquo;NumberOfExecutions\u0026rdquo;: [4, [\u0026ldquo;unsigned int\u0026rdquo;]],\\n \u0026ldquo;LastExecution\u0026rdquo;: [60, [\u0026ldquo;unsigned long long\u0026rdquo;]]\\n }]\\n}\\n       View Artifact Source   name: Windows.Registry.UserAssist description: | Windows systems maintain a set of keys in the registry database (UserAssist keys) to keep track of programs that executed. The number of executions and last execution date and time are available in these keys. The information within the binary UserAssist values contains only statistical data on the applications launched by the user via Windows Explorer. Programs launched via the command­line (cmd.exe) do not appear in these registry keys. From a forensics perspective, being able to decode this information can be very useful. reference: - https://www.aldeid.com/wiki/Windows-userassist-keys precondition: SELECT OS From info() where OS = 'windows' parameters: - name: UserFilter default: \u0026quot;\u0026quot; description: If specified we filter by this user ID. - name: ExecutionTimeAfter default: \u0026quot;\u0026quot; type: timestamp description: If specified only show executions after this time. - name: UserAssistKey default: Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\UserAssist\\*\\Count\\* - name: userAssistProfile default: | { \u0026quot;Win10\u0026quot;: [0, { \u0026quot;NumeberOfExecutions\u0026quot;: [4, [\u0026quot;unsigned int\u0026quot;]], \u0026quot;LastExecution\u0026quot;: [60, [\u0026quot;unsigned long long\u0026quot;]] }] } sources: - queries: - LET TMP = SELECT rot13(string=regex_replace( source=url(parse=FullPath).Fragment, re=\u0026quot;^.+/Count/\u0026quot;, replace=\u0026quot;\u0026quot;)) AS Name, binary_parse( string=Data.value, profile=userAssistProfile, target=\u0026quot;Win10\u0026quot; ) As UserAssist, parse_string_with_regex( string=FullPath, regex=\u0026quot;Users/(?P\u0026lt;User\u0026gt;[^/]+)/NTUSER\u0026quot;).User AS User FROM Artifact.Windows.Registry.NTUser(KeyGlob=UserAssistKey) - LET UserAssist = SELECT Name, User, timestamp( winfiletime=UserAssist.LastExecution.AsInteger) As LastExecution, timestamp( winfiletime=UserAssist.LastExecution.AsInteger).Unix AS LastExecutionTS, UserAssist.NumeberOfExecutions.AsInteger AS NumeberOfExecutions FROM TMP - LET A1 = SELECT * FROM if( condition=UserFilter, then={ SELECT * FROM UserAssist WHERE User =~ UserFilter }, else=UserAssist) - SELECT * FROM if( condition=ExecutionTimeAfter, then={ SELECT * FROM A1 WHERE LastExecutionTS \u0026gt; ExecutionTimeAfter }, else=A1)    Windows.Search.FileFinder Find files on the filesystem using the filename or content.\nPerformance Note This artifact can be quite expensive, especially if we search file content. It will require opening each file and reading its entire content. To minimize the impact on the endpoint we recommend this artifact is collected with a rate limited way (about 20-50 ops per second).\nThis artifact is useful in the following scenarios:\n We need to locate all the places on our network where customer data has been copied.\n We’ve identified malware in a data breach, named using short random strings in specific folders and need to search for other instances across the network.\n We believe our user account credentials have been dumped and need to locate them.\n We need to search for exposed credit card data to satisfy PCI requirements.\n We have a sample of data that has been disclosed and need to locate other similar files\n     Arg Default Description     SearchFilesGlob C:\\Users\\** Use a glob to define the files that will be searched.   Keywords None A comma delimited list of strings to search for.   Use_Raw_NTFS N    Upload_File N    Calculate_Hash N    MoreRecentThan     ModifiedBefore        View Artifact Source   name: Windows.Search.FileFinder description: | Find files on the filesystem using the filename or content. ## Performance Note This artifact can be quite expensive, especially if we search file content. It will require opening each file and reading its entire content. To minimize the impact on the endpoint we recommend this artifact is collected with a rate limited way (about 20-50 ops per second). This artifact is useful in the following scenarios: * We need to locate all the places on our network where customer data has been copied. * We’ve identified malware in a data breach, named using short random strings in specific folders and need to search for other instances across the network. * We believe our user account credentials have been dumped and need to locate them. * We need to search for exposed credit card data to satisfy PCI requirements. * We have a sample of data that has been disclosed and need to locate other similar files precondition: SELECT * FROM info() where OS = 'windows' parameters: - name: SearchFilesGlob default: C:\\Users\\** description: Use a glob to define the files that will be searched. - name: Keywords default: description: A comma delimited list of strings to search for. - name: Use_Raw_NTFS default: N type: bool - name: Upload_File default: N type: bool - name: Calculate_Hash default: N type: bool - name: MoreRecentThan default: \u0026quot;\u0026quot; type: timestamp - name: ModifiedBefore default: \u0026quot;\u0026quot; type: timestamp sources: - queries: - | LET file_search = SELECT FullPath, Sys.mft as Inode, Mode.String AS Mode, Size, Mtime.Sec AS Modified, timestamp(epoch=Atime.Sec) AS ATime, timestamp(epoch=Mtime.Sec) AS MTime, timestamp(epoch=Ctime.Sec) AS CTime, IsDir FROM glob(globs=SearchFilesGlob, accessor=if(condition=Use_Raw_NTFS = \u0026quot;Y\u0026quot;, then=\u0026quot;ntfs\u0026quot;, else=\u0026quot;file\u0026quot;)) - | LET more_recent = SELECT * FROM if( condition=MoreRecentThan, then={ SELECT * FROM file_search WHERE Modified \u0026gt; parse_float(string=MoreRecentThan) }, else=file_search) - | LET modified_before = SELECT * FROM if( condition=ModifiedBefore, then={ SELECT * FROM more_recent WHERE Modified \u0026lt; parse_float(string=ModifiedBefore) }, else=more_recent) - | LET keyword_search = SELECT * FROM if( condition=Keywords, then={ SELECT * FROM foreach( row={ SELECT * FROM modified_before }, query={ SELECT FullPath, Inode, Mode, Size, Modified, ATime, MTime, CTime, str(str=String.Data) As Keywords FROM yara(files=FullPath, key=Keywords, rules=\u0026quot;wide nocase ascii:\u0026quot;+Keywords, accessor=if(condition=Use_Raw_NTFS = \u0026quot;Y\u0026quot;, then=\u0026quot;ntfs\u0026quot;, else=\u0026quot;file\u0026quot;)) }) }, else=modified_before) - | SELECT FullPath, Inode, Mode, Size, Modified, ATime, MTime, CTime, Keywords, if(condition=(Upload_File = \u0026quot;Y\u0026quot; and NOT IsDir ), then=upload(file=FullPath, accessor=if(condition=Use_Raw_NTFS = \u0026quot;Y\u0026quot;, then=\u0026quot;ntfs\u0026quot;, else=\u0026quot;file\u0026quot;))) AS Upload, if(condition=(Calculate_Hash = \u0026quot;Y\u0026quot; and NOT IsDir ), then=hash(path=FullPath, accessor=if(condition=Use_Raw_NTFS = \u0026quot;Y\u0026quot;, then=\u0026quot;ntfs\u0026quot;, else=\u0026quot;file\u0026quot;))) AS Hash FROM keyword_search    "
},
{
        "uri": "/docs/user-interface/api/",
        "title": "The Velociraptor API",
        "tags": [],
        "description": "",
        "content": " Velociraptor is very good at collecting artifacts from endpoints. However, in modern DFIR work, the actual collection is only the first step of a much more involved process. Typically we want to post process data using more advanced data mining tools (such as data stacking), or export the data to other systems. Velociraptor usually is only a part of a wider solution which might include a SIEM and SOC integration.\nIn order to facilitate interoperability with other tools, Velociraptor offers an external API. The API is offered via gRPC so it can be used in any language which gRPC supports (e.g. Java, C++, Python etc). In this document we illustrate the Python API but any language should work in a similar way (consult the gRPC documentation for your language).\nThe Velociraptor API Server The API server exposes an endpoint ready to accept gRPC connections. By default the API server listen only on the loopback interface (127.0.0.1) but it is easy to change to be externally accessible if you need by changing the server.config.yaml file:\n API: bind_address: 127.0.0.1 bind_port: 8001  Client programs simply connect directly to this API and call gRPC methods on it.\nThe connection is encrypted using TLS and authenticated using mutual certificates. When we initially created the Velociraptor configuration file, we created a CA certificate and embedded it in the server.config.yaml file. It is this CA certificate which is used to verify that the certificate each end presents was issued by the Velociraptor CA.\nIf you need to have extra security in your environment you should keep the original server.config.yaml file generated in an offline location, then deploy a redacted file (without the CA.private_key value) on the server. This way api client certificates can only be issued offline.\n Before the client may connect to the API server they must have a certificate issued by the Velociraptor CA. This is easy to generate:\n $ velociraptor --config server.config.yaml \\ config api_client --name Fred \u0026gt; api_client.yaml  Will generate something like:\n ca_certificate: | -----BEGIN CERTIFICATE----- MIIDITCCAgmgAwIBAgIRAI1oswXLBFqWVSYZx1VibMkwDQYJKoZIhvcNAQELBQAw -----END CERTIFICATE----- client_cert: | -----BEGIN CERTIFICATE----- 2e1ftQuzHGD2XPquqfuVzL1rtEIA1tiC82L6smYbeOe0p4pqpsHN1sEDkdfhBA== -----END CERTIFICATE----- client_private_key: | -----BEGIN RSA PRIVATE KEY----- sVr9HvR2kBzM/3yVwvb752h0qDOYDfzLRENjA7dySeOgLtBSvd2gRg== -----END RSA PRIVATE KEY----- api_connection_string: 127.0.0.1:8001 name: Fred  The certificate generated has a common name as specified by the --name flag. This name will be logged in the server\u0026rsquo;s audit logs so you can use this to keep track of which programs have access. This file keeps both private key and certificate as well as the CA certificate which must be used to authenticate the server in a single file for convenience.\nUsing the API from Python Although the API exposes a bunch of functions used by the GUI, the main function (which is not exposed through the GUI) is the Query() method. This function simply executes one or more VQL queries, and streams their results back to the caller.\nThe function requires an argument which is a protobuf of type VQLCollectorArgs:\n VQLCollectorArgs: env: list of VQLEnv(string key, string value) Query: list of VQLRequest(string Name, string VQL) max_row: int max_wait: int ops_per_second: float  This very simple structure allows the caller to specify one or more VQL queries to run. The call can set up environment variables prior to the query execution. The max_row and max_wait parameters indicate how many rows to return in a single result set and how long to wait for additional rows before returning a result set.\nThe call simply executes the VQL queries and returns result sets as VQLResponse protobufs:\n VQLResponse: Response: json encoded string Columns: list of string total_rows: total number of rows in this packet  The VQL query may return many responses - each represents a set of rows. These responses may be returned over a long time, the API call will simply wait until new responses are available. For example, the VQL may represent an event query - i.e. watch for the occurrence of some event in the system - in this case it will never actually terminate, but keep streaming response packets.\nHow does this look like in code? All the files needed for python integration can be found in the bindings/python directory of the source distribution:\n The requirements.txt file can be used to install exactly the right version of dependencies (We recommend using a virtual env for python):\n$ pip install -r requirements.txt  The api.proto file specifies the exact protobufs required to connect to the API. There is no need to compile the file with the protobuf compiler - simply use the pre-compiled python files api_pb2_grpc.py and api_pb2.py by copying them into your project.\n The file client_example.py is a fully working client example. You can use this example to connect with the server and run any VQL queries.\n  Code walk through The following will cover an example implementation in python. The first step is to prepare credentials for making the gRPC call. We parse the api_client.yaml file and prepare a credential object:\nconfig = yaml.load(open(\u0026quot;api_client.yaml\u0026quot;).read()) creds = grpc.ssl_channel_credentials( root_certificates=config[\u0026quot;ca_certificate\u0026quot;].encode(\u0026quot;utf8\u0026quot;), private_key=config[\u0026quot;client_private_key\u0026quot;].encode(\u0026quot;utf8\u0026quot;), certificate_chain=config[\u0026quot;client_cert\u0026quot;].encode(\u0026quot;utf8\u0026quot;)) options = (('grpc.ssl_target_name_override', \u0026quot;VelociraptorServer\u0026quot;,),)  Next we connect the channel to the API server:\nwith grpc.secure_channel(config[\u0026quot;api_connection_string\u0026quot;], creds, options) as channel: stub = api_pb2_grpc.APIStub(channel)  The stub is the object we use to make calls with. We can then issue our call:\nrequest = api_pb2.VQLCollectorArgs( Query=[api_pb2.VQLRequest( VQL=query, )]) for response in stub.Query(request): rows = json.loads(response.Response) for row in rows: print(row)  We issue the query and then just wait for the call to generate response packets. Each packet may contain several rows which will all be encoded as JSON in the Response field. Each row is simply a dict with keys being the column names, and the values being possibly nested dicts or simple data depending on the query.\nWhat can we do with this? The Velociraptor API is deliberately open ended - meaning we do not pose any limitations on what can be done with it. It is conceptually a very simple API - just issue the query and look at the results, however this makes it extremely powerful.\nWe already have a number of very useful server side VQL plugins you can use. We also plan to add a number of other plugins in future - this means that the Velociraptor API can easily be extended in a backwards compatible way by simply adding new VQL plugins. New queries can do more, without breaking existing queries.\n"
},
{
        "uri": "/docs/vql_reference/filesystem_accessors/",
        "title": "Filesystem Accessors",
        "tags": [],
        "description": "",
        "content": " Filesystem Accessors Many VQL plugins operate on files. However how we read files on the endpoint can vary - depending on what we actually mean by a file. For example, Velociraptor is able to read files parsed from the NTFS parser, compressed files within Zip archives, or even files downloaded from a URL. VQL specifies the way a file is read via an accessor (essentially a file access driver), and a path which encodes how the accessor will actually access the file.\nSimple filesystem accessors The file accessor The file accessor uses the normal OS filesystem APIs to access files and directories.\nThe limitations with this method is that some files are locked if they are in use and we are not able to read them. For example, the registry hives or the page file.\nThe path parameter is passed directly to the filesystem APIs. Velociraptor supports both forward and reverse slashes on all supported operating systems.\nOn windows, Velociraptor emulates the top level directory as a list of available drives. For example, listing the \u0026ldquo;/\u0026rdquo; directory will yield directories such as \u0026ldquo;C:\u0026ldquo;, \u0026ldquo;D:\u0026rdquo; etc. Therefore Velociraptor paths always have a / at the top level, typically followed by a drive letter then the rest of the path.\n The ntfs accessor The ntfs accessor uses Velociraptor\u0026rsquo;s built in NTFS parser to extract directory information and file contents.\nThis bypasses the normal file locking mechanism and allows us to download and read locked files like registry hives.\nThe path is interpreted as a raw device, followed by an NTFS path. For example the path \\\\.\\c:\\Windows\\System32 refers to the System32 directory as parsed by the ntfs parser from the raw device \\\\.\\c:. Supported raw devices include volume shadow copies (VSC) as well.\nAs a convenience, the ntfs accessor recognizes a drive letter and automatically maps it to the raw device. I.e. the following paths are equivalent \\\\.\\c:\\Windows and C:\\Windows.\nListing the top level directory will display all physical drives and Volume Shadow Copies available on the machine.\nThe backslash character is considered the path separator only for paths following the device name. Supported device names include the backslash as part of their name. For example, listing the top level directory will show \\\\.\\c: as a single device, even though it contains backslashes.\n The registry accessor The registry accessor uses the OS APIs to view the registry as a filesystem. You can use this to navigate the endpoint\u0026rsquo;s registry hives interactively.\nSince registry values are typically small, Velociraptor also gets the values in the directory listing as well, so it is not usually necessary to download files from the registry hive.\nThe Windows registry differs from a regular filesystem in that Key names may have forward slash, and value names may contain both forward and backward slashes. Therefore it is difficult to properly split a path into the correct key and value. Velociraptor treats a path as a list of components. Velociraptor will surround component names with quotes if they contain slashes to allow the path to be broken back into components. For example, HKEY_CURRENT_USER\\Software\\Microsoft\\Office\\16.0\\Word\\Security\\Trusted Documents\\TrustRecords\\\u0026quot;%USERPROFILE%/Desktop/test.docx\u0026quot; represents the value named %USERPROFILE%/Desktop/test.docx.\n Other filesystem accessors The data accessor Sometimes it is necessary to pass a string to a plugin which expects a filename (for example yara() plugin. In this case it is possible to specify the data accessor which creates an in memory file from the filename path passed to it.\nThe zip accessor Zip files contain compressed members. It is sometimes useful to be able to treat members of the zip archive as simple files, then we can scan or list them using other plugins.\nThe zip accessor makes that possible. However the accessor requires an underlying file to actually unzip. Therefore the zip accessor requires a url:\n The scheme part is used to specify the underlying accessor to access the zip file.\n The path part is used to specify the path to pass to the underlying accessor.\n The fragment part is used to specify the path within the zip file to access.\n  Do not attempt to construct the url by manually concatenating parts because this does not properly handle escaping. Instead use the url() function. For example url(scheme='ntfs', path=\u0026quot;C:/Users/Test/my.zip\u0026quot;, fragment=\u0026quot;1.txt\u0026quot;).String will produce the required url: ntfs://C:/Users/Test/my.zip#1.txt to access the file 1.txt within the my.zip file as extracted by the ntfs raw parser.\n The raw_reg accessor Parsing of raw registry hives is provided by the raw_reg accessor. Similarly to the zip accessor above, the raw_reg accessor requires an underlying file to read. Therefore it also requires a path formatted as a url:\n The scheme part is used to specify the underlying accessor to access the raw registry hive file.\n The path part is used to specify the path to pass to the underlying accessor.\n The fragment part is used to specify the key or value within the registry hive to access.\n  Note that this accessor usually requires an underlying file that is accessed by the raw NTFS parser (since registry hives are locked at runtime).\n"
},
{
        "uri": "/docs/tutorial/",
        "title": "Tutorial",
        "tags": [],
        "description": "",
        "content": " In this tutorial we will deploy Velociraptor locally to our machine, and connect a client to it. This is the bare minimum required to demonstrate Velociraptor\u0026rsquo;s capabilities and walk through the GUI.\nVelociraptor can be used in a number of different ways, but in this tutorial we will use it as an end point visibility tool, collect some artifacts and set up endpoint monitoring.\nOverview Before we start it is useful to see how a Velociraptor deployment looks at a high level:\nmermaid.initialize({startOnLoad:true}); graph TD; A(User) --|Browser| B(Velociraptor GUI server) B -- C[Frontend] C -- D[fab:fa-apple] C -- E[fab:fa-linux] C -- F[fab:fa-windows]  Create a new deployment Velociraptor clients connect back to the server using an encrypted communication channel. Each deployment has its own unique cryptographic keys ensuring the security and authenticity of the server.\nTherefore before we can start the server we will create a new server configuration file:\n$ velociraptor.exe config generate \u0026gt; server.config.yaml  This command generates a new configuration file and redirect it to the file server.config.yaml. This file contains key material that controls this specific deployment.\n"
},
{
        "uri": "/docs/presentations/sans_dfir_summit2019/",
        "title": "SANS Summit 2019",
        "tags": [],
        "description": "",
        "content": "26th July 2019 We were really happy to present the keynote at the SANS DFIR Summit in Austin, TX. Thanks everyone for the warm welcome and interesting discussions.   View our presentation   This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n \n  \n\n"
},
{
        "uri": "/docs/presentations/rsa/",
        "title": "RSA Asia Pacific and Japan 2019",
        "tags": [],
        "description": "",
        "content": "16th July 2019 At the 2019 RSA Asia Pacific and Japan conference we ran a short Velociraptor LAB and also presented a talk. We really enjoyed receiving a lot of positive feedback from the community.   View the hands on LAB we presented at RSA   This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n \n    View our presentation   This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n \n  \n\n"
},
{
        "uri": "/docs/presentations/crikeycon2019/",
        "title": "Crikeycon 2019 Training Workshop",
        "tags": [],
        "description": "",
        "content": "24th February 2018 At CrikeyCon 2019 we presented a full day training workshop. It was a lot of fun and we received some great feedback. You can read the slides below.   View our presentation   This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n \n  \n\n"
},
{
        "uri": "/docs/presentations/nzitf2018/",
        "title": "New Zealand Internet Task Force Conference 2018",
        "tags": [],
        "description": "",
        "content": "Nov 2018 We presented a Velociraptor workshop at the NZITF2018 conference. This was a very early presentation and the GUI and capabilities of the tool were limited at the time, although this workshop did demonstrate some of the original design choices.\n   View our presentation   This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n \n  \n\n"
},
{
        "uri": "/categories/blog/",
        "title": "Blog",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/categories/",
        "title": "Categories",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/blog/html/2019/08/28/the_velociraptor_api/",
        "title": "The Velociraptor API and FUSE",
        "tags": [],
        "description": "We have previously shown how the Velociraptor API provides a\npowerful mechanism to integrate and automate. In this post we\ndemonstrate an example of a program which takes advantage of the API\nto present the client's VFS as a FUSE filesystem.\n",
        "content": " The Velociraptor GUI is very useful, but for the power user, the Velociraptor API provides a powerful mechanism to integrate and automate. We previously discussed how the Velociraptor API can be used by external programs. This post explore a sample program that uses the API and presents a client\u0026rsquo;s VFS as a FUSE directory.\nThis allows us to navigate the remote end point\u0026rsquo;s file system as if it was mounted locally - we can list directories or fetch files, or even open remote files using third party programs. All the while, these actions are fully audited on the server and the collected files are stored in Velociraptor\u0026rsquo;s file store for archiving and evidence preservation.\nOverview Consider an analyst investigating an end point. The analyst has some third party tools on their workstation which they would like to use on files obtained from the end point.\nFilesystem in Usespace (FUSE) is a way of creating the illusion of a real filesystem using software. When various programs on the computer requests filesystem operations, such as listing files in a directory or reading a file, Velociraptor takes over and emulates these requests.\nVelociraptor\u0026rsquo;s built in FUSE program emulates a filesystem by exporting a client\u0026rsquo;s cached VFS on the server to the FUSE layer. If the analyst attempts to list a directory that the server has no cache of - the server will issue a new directory listing request from the endpoint. If the endpoint is currently online, the updated directory listing will be returned to the server, and in turn relayed to the analyst\u0026rsquo;s workstation.\nThe overall effect is that as the analyst navigates around the FUSE filesystem on their workstation, they are issuing collection requests from the endpoint, and reading their responses in such as way that it appears the endpoint is really mounted on the FUSE filesystem.\nThe above figure shows all the components and how they are related. Assume the FUSE filesystem is mounted on drive Q: in the analyst\u0026rsquo;s workstation:\n Suppose the analyst is navigating the file Q:\\file\\ using Windows Explorer.\n Velociraptor\u0026rsquo;s FUSE program running on the analyst workstation will issue an API request to list the file directory within the client\u0026rsquo;s VFS on the server.\n If the server has a locally cached version of this VFS directory in its data store it will return it immediately.\n However, if no server side cache exists, the FUSE program will issue a directory listing request to the endpoint.\n The endpoint will respond to this and return the directory listing (if it is currently online).\n Now the server will contain a cached copy of the VFS directory and can return it (just as in step 3 above).\n The Velociraptor FUSE program on the workstation can return the directory listing to the Windows kernel and this will be fed back into the Windows Explorer. The end result is that Windows Explorer appears to be navigating the endpoint\u0026rsquo;s filesystem directly.\n  You can see this process in the screenshot below:\nDo not run the fuse API command as a different user to what is currently logged in (e.g. do not run as Administrator). If you do then you will not be able to see the FUSE drive in your user\u0026rsquo;s desktop session.\nFor example if you are logged in as user \u0026ldquo;Test\u0026rdquo;, then any FUSE drives created by Velociraptor running as user Test are only visible to user Test. If you run the above command as an elevated UAC prompt then user Test will be unable to see the new drive.\n Running the FUSE program On Windows filesystem in userspace is implemented by the WinFSP project. You will need to download and install it first.\nWe require an API key to use the fuse feature so generate one first on the server:\n $ velociraptor --config server.config.yaml \\ config api_client --name FUSE \u0026gt; api_client.yaml  Now simply copy the generate api_client.yaml file to the analyst\u0026rsquo;s workstation. You can mount any client\u0026rsquo;s VFS by simply specifying its client id and a drive letter to access it:\nC:\\Program Files\\Velociraptor\u0026gt;Velociraptor.exe --api_config f:\\api_client.yaml -v fuse q: C.8b6623b1d7c42adf The service Velociraptor has been started. [INFO] 2019-08-26T14:12:28Z Initiating VFSRefreshDirectory for /file/C:/Go/ (aff4:/clients/C.8b6623b1d7c42adf/flows/F.BLHUHJ0RDGCRU) [INFO] 2019-08-26T14:12:28Z Flow for /file/C:/Go/ still outstanding (aff4:/clients/C.8b6623b1d7c42adf/flows/F.BLHUHJ0RDGCRU) ...  Simply press Ctrl-C to stop the FUSE program as any time.\nConclusions The FUSE feature is a perfect example of a useful API program. The program fully automates the Velociraptor server - it received cached information about the client\u0026rsquo;s VFS status, and then automatically issues new collection requests as needed.\nThis kind of automated control of the Velociraptor server opens the door to many such applications. From automated response to remediation and automated evidence collection.\nSome users has asked us what the difference between the FUSE program and other tools, e.g. F-Response which also create the illusion that the remote system is mounted on the analyst\u0026rsquo;s workstation. The main difference is that Velociraptor does not export the raw block device from the endpoint - it simply exports the files and directories we collected already. So for example, it is not possible to run a low level disk analysis system (such as X-Ways) on the mounted FUSE drive. However you can still run specialized file parsers (such as Kape or log2timeline) as long as they do not require access to the raw devices.\n "
},
{
        "uri": "/",
        "title": "Velociraptor / Dig deeper",
        "tags": [],
        "description": "Velociraptor's homepage",
        "content": ""
},
{
        "uri": "/blog/html/2019/03/02/agentless_hunting_with_velociraptor.html",
        "title": "Agentless hunting with Velociraptor",
        "tags": [],
        "description": "There has been a lot of interest lately in \"Agentless hunting\"\nespecially using PowerShell.  This blog post explores an agentless\ndeployment scenario, where we do not want to install Velociraptor\npermanently on the end point, but rather push it to end points\ntemporarily to collect specific artifacts.\n",
        "content": "There has been a lot of interest lately in Agentless hunting especially using PowerShell. There are many reasons why Agentless hunting is appealing - there are already a ton of endpoint agents and yet another one may not be welcome. Somtimes we need to deploy endpoint agents as part of a DFIR engagement and we may not want to permanently install yet another agent on end points.\nThis blog post explores an agentless deployment scenario, where we do not want to install Velociraptor permanently on the end point, but rather push it to end points temporarily to collect specific artifacts. The advantage of this method is that there are no permanent changes to the end point, as nothing is actually installed. However, we do get the full power of Velociraptor to collect artifacts, hunt for evil and more...\nAgentless Velociraptor Normally when deploying Velociraptor as a service, the binary is copied to the system and a service is installed. The service ensures that the binary is restarted when the system reboots, and so Velociraptor is installed on a permanent basis.\nHowever in the agentless deployment scenario we simply run the binary from a network share using group policy settings. The downside to this approach is that the endpoint needs to be on the domain network to receive the group policy update (and have the network share accessible) before it can run Velociraptor. When we run in Agentless mode we are really after collecting a bunch of artifacts via hunts and then exiting - the agent will not restart after a reboot. So this method is suitable for quick hunts on corporate (non roaming) assets.\nIn this post I will use Windows 2019 Server but this should also work on any older version.\nCreating a network share The first step is to create a network share with the Velociraptor binary and its configuration file. We will run the binary from the share in this example, but for more reliability you may want to copy the binary into e.g. a temp folder on the end point in case the system becomes disconnected from the domain. For quick hunts though it should be fine.\nWe create a directory on the server (I will create it on the domain controller but you should probably not do that - find another machine to host the share).\nI created a directory C:\\Users\\Deployment and ensured that it is read only. I have shared the directory as the name Deployment.\nI now place the Velociraptor executable and client config file in that directory and verify that I can run the binary from the network share. The binary should be accessible via \\\\\\\\DC\\Deployment\\velociraptor.exe:\n Creating the group policy object. Next we create the group policy object which forces all domain connected machines to run the Velociraptor client. We use the Group Policy Management Console:\nSelect the OU or the entire domain and click \u0026quot;Create New GPO\u0026quot;:\nNow right click the GPO object and select \u0026quot;Edit\u0026quot;:\nWe will create a new scheduled task. Rather than schedule it at a particular time, we will select to run it immediately. This will force the command to run as soon as the endpoint updates its group policy settings (i.e. we do not want to wait for the next reboot of the endpoint).\nNext we give the task a name and a description. In order to allow Velociraptor to access raw devices (e.g. to collect memory or NTFS artifacts) we can specify that the client will run at NT_AUTHORITY\\SYSTEM privileges, and run without any user being logged on. It is also worth ticking the \u0026quot;hidden\u0026quot; checkbox here to prevent a console box from appearing.\nNext click the Actions tab and add a new action. This is where we launch the Velociraptor client. The program will simply be launched from the share (i.e. \\\\\\\\DC\\Deployment\\velociraptor.exe) and we give it the arguments allowing it to read the provided configuration file (i.e. --config \\\\\\\\DC\\Deployment\\client.config.yaml client -v).\nIn the setting tab we can control how long we want the client to run. For a quick hunt this may be an hour or two but maybe for a DFIR engagement it might be a few days. The GPO will ensure the client is killed after the allotted time.\nOnce the GPO is installed it becomes active for all domain machines. You can now schedule any hunts you wish using the Velociraptor GUI. When a domain machine refreshes its group policy it will run the client, which will enroll and immediately participate in any outstanding hunts - thus collecting and delivering its artifacts to the server. After the allotted time has passed, the client will shut down without having installed anything on the endpoint.\nYou can force a group policy update by running the gpupdate program. Now you can verify that Velociraptor is running:\n Persistence Note that when running Velociraptor in agent less mode you probably want to configure it so that the writeback file is written to the temp directory. The writeback file is how the client keeps track of its key material (and identity). The default is to store it in the client's installation folder, but you should probably change it in the client's config file:\nClient: writeback_windows: $TEMP\\\\velociraptor.writeback.yaml  The file will remain in the client's temp directory so if you ever decide to run the agentless client again (by pushing another group policy) the client id remains the same.\n  "
},
{
        "uri": "/blog/html/2019/02/14/alerting_on_event_patterns.html",
        "title": "Alerting on event patterns",
        "tags": [],
        "description": "We have shown in earlier posts how Velociraptor uses VQL to define event queries that can detect specific conditions. These conditions can be used to create alerts and escalation actions.",
        "content": "We have shown in earlier posts how Velociraptor uses VQL to define event queries that can detect specific conditions. These conditions can be used to create alerts and escalation actions.\nOne of the most useful types of alerts is detecting a pattern of activity. For example we can detect failed and successful login attempts seperately, but it is the specific pattern of events (say 5 failed login attempts followed by a successful one) that is interesting from a detection point of view.\nThis post illustrates how this kind of temporal correlation can be expressed in a VQL query. We then use it to create alerts for attack patterns commonly seen by intrusions.\nEvent Queries Velociraptor executes queries written in the Velociraptor Query Language (VQL). The queries can be executed on the client, and their results streamed to the server. Alternatively the queries may be executed on the server and process the result of other queries which collected information from the client.\nA VQL query does not have to terminate at all. VQL queries draw their data from a VQL plugin which may simply return data rows at different times. For example, consider the following query:\nSELECT EventData as FailedEventData, System as FailedSystem FROM watch_evtx(filename=securityLogFile) WHERE System.EventID = 4625  This query sets up a watcher on a windows event log file. As new events are written to the log file, the query will produce those events as new rows. The rows will then be filtered so we only see event id 4625 (Failed logon event).\nVelociraptor can implement event queries on the client or on the server. For example, say we wanted to collect all failed event logs with the query above. We would write an artifact that encapsulates this query:\nname: Windows.System.FailedLoginAttempts parameters: - name: securityLogFile default: C:/Windows/System32/Winevt/Logs/Security.evtx sources: - queries: - SELECT EventData as FailedEventData, System as FailedSystem FROM watch_evtx(filename=securityLogFile) WHERE System.EventID.Value = 4625  Then we simply add that artifact to the monitored artifact list in the config file:\nEvents: artifacts: - Generic.Client.Stats - Windows.System.FailedLoginAttempts version: 2 ops_per_second: 10  The monitored artifacts are run on all clients connected to the server. The output from these queries is streamed to the server and stored in the client's monitoring VFS directory.\nLets test this artifact by trying to run a command using the runas windows command. We will be prompted for a password but failing to give the correct password will result in a login failure event:\nAfter a few seconds the event will be written to the windows event log and the watch_evtx() VQL plugin will emit the row - which will be streamed to the VFS monitoring directory on the server, where it can be viewed in the GUI:\nThe above screenshot shows that the monitoring directory now contains a subdirectory named after the artifact we created. Inside this directory are CSV files for each day and every failed logon attempt is detailed there.\n Time correlation While it is interesting to see all failed logon attempts in many cases these events are just noise. If you put any server on the internet (e.g. an RDP or SSH server) you will experience thousands of brute force attempts to break in. This is just the nature of the internet. If your password policy is strong enough it should not be a big problem.\nHowever, what if someone guesses the password for one of your accounts? Then the activity pattern is more like a bunch of failed logons followed by a successful logon for the same account.\nThis pattern is way more interesting than just watching for a series of failed logons (although that is also good to know).\nBut how do we write a query to detect this? Essentially the query needs to look back in time to see how many failed logon attempts preceeded each successful logon.\nThis is a typical problem which may be generalized as followed:\nGoal\nWe want to detect an event A preceeded by a specified number of events B within a defined time window.\n This problem may be generalized for example:\nDetect a user account created and deleted within a short time window. A beacon to a specific DNS followed by at least 5 beacons within the last 5 hours to same DNS (Event A and B are the same).   The fifo() plugin How shall we write the VQL query to achieve this? This is made possible by use of the fifo() plugin. As its name suggests, the FIFO plugin acts as a First In First Out cache for event queries.\n3.svg The plugin is given a subquery which is also a VQL query generating its own events. As the subquery generates events, each event is kept in the fifo plugin's cache in a first in first out manner. Events are also expired if they are too old.\nWe typically store the query in a variable. Each time the variable is queried the cache is returned at once. To illustrate how this works consider the following query:\nLET fifo_events = SELECT * FROM fifo( max_rows=5, query={ SELECT * from watch_evtx(filename=securityLogFile) WHERE System.EventID.Value = 4625 }) SELECT * FROM foreach( row={ SELECT * FROM clock(period=60) }, query={ SELECT * from fifo_events })  The first query is stored into the fifo_events variable. When it is first defined, the fifo() VQL plugin launches its subquery and simply collects its output into its local cache in a fifo manner. This will essentially keep the last 5 rows in its cache.\nThe second query runs the clock() plugin to receive a clock event every 60 seconds. For each of these events, we select from the fifo_events variable - that is we select the last 5 failed events.\nYou can see that this allows us to query the last 5 events in the fifo cache for every clock event. If we now replace the clock event with a successful logon event this query will do exactly what we want:\n# This query will generate failed logon events - one per row, as # they occur. - LET failed_logon = SELECT EventData as FailedEventData, System as FailedSystem FROM watch_evtx(filename=securityLogFile) WHERE System.EventID.Value = 4625 # This query will create a fifo() to contain the last 5 failed # logon events. - LET last_5_events = SELECT FailedEventData, FailedSystem FROM fifo(query=failed_logon, max_rows=5, max_age=atoi(string=failedLogonTimeWindow)) # This query simply generates successful logon events. - LET success_logon = SELECT EventData as SuccessEventData, System as SuccessSystem FROM watch_evtx(filename=securityLogFile) WHERE System.EventID.Value = 4624 # For each successful event, we select the last 5 failed events # and count them (using the group by). If the count is greater # than 3 then we emit the row as an event. - SELECT * FROM foreach( row=success_logon, query={ SELECT SuccessSystem.TimeCreated.SystemTime AS LogonTime, SuccessSystem, SuccessEventData, FailedEventData, FailedSystem, count(items=SuccessSystem) as Count FROM last_5_events WHERE FailedEventData.SubjectUserName = SuccessEventData.SubjectUserName GROUP BY LogonTime }) WHERE Count \u0026gt; 3  The above query simply watches the event log for failed logins and populates a fifo() with the last 5 failed events. At the same time we monitor the event log for successful logon events. If we see a successful event, we go back and check the last 5 failed events and count them.\nIf the failed events are for the same user and there are more than 3 then we report this as an event. We now have a high value event.\nLet's see what it looks like when such an event is triggered:\nJust like before, the events are written to a daily CSV log, one event per CSV row. It is a bit hard to see in the GUI since there is a lot of data, (We probably need some GUI work to improve this) but there is a single row emitted for each event, and the FailedEventData column contains a list of all the failed login attempts stored in the fifo().\n Server side queries. We have seen how the fifo() plugin can be used in the monitoring artifact itself to have the client detect its own events. However, the endpoint is usually only able to see its own events in isolation. It would be nice to be able to detect patterns only evident by seeing concerted behaviour from multiple endpoints at the same time.\nFor example, consider the pattern of an attacker who compromised domain credentials running multiple PowerShell Remoting commands across the entire domain. A command like:\nPS C:\\WINDOWS\\system32\u0026gt; Invoke-Command –ComputerName testcomputer -ScriptBlock {Hostname} TestComputer  This command will generate multiple event log entries, including event 4624 (logon) on each host. While in isolation, on each individual endpoint this event is not suspicious, we might consider seeing this event repeated within a short time across the domain suspicious.\nTo set that up we would run the following artifact as a monitoring artifact on all endpoints:\nname: Windows.Event.SuccessfulLogon sources: - queries: - SELECT EventData as SuccessEventData, System as SuccessSystem FROM watch_evtx(filename=securityLogFile) WHERE System.EventID.Value = 4624  On the server we simple install a watcher on all monitoring events from this artifact and feed the result to the fifo(). This fills the fifo() with the last 500 successful logon events from all clients within the last 60 seconds:\nLET last_successful_logons = SELECT * FROM fifo( max_rows=500, max_time=60, query={ SELECT * FROM watch_monitoring( artifact=\u0026quot;Windows.Event.SuccessfulLogon\u0026quot;) })  By counting the number of such unique events we can determine if there were too many successful logon events from different hosts within the last minute. This might indicate a scripted use of powershell remoting across the domain.\n Conclusions In this post we have seen how to write artifacts which capture a time ordered pattern of behavior. This technique is useful to codify common attack techniques. The technique is general and we can use the same idea on server side queries to correlate events from many hosts at the same time.\n "
},
{
        "uri": "/blog/html/2019/02/10/velociraptor_performance.html",
        "title": "Velociraptor Performance",
        "tags": [],
        "description": "We are often asked how many resources does a Velociraptor deployment use? How should one spec a machine for a Velociraptor deployment?",
        "content": "Velociraptor Performance We are often asked how many resources does a Velociraptor deployment use? How should one spec a machine for a Velociraptor deployment?\nWe have previously said that one of the reasons we developed Velociraptor was to improve on the performance of GRR which was not scalable for our use case.\nWe've been working with the team at Klein \u0026amp; Co. on several intrusions over the past several months, which are providing valuable opportunities to deploy and test Velociraptor in a range of real world investigation scenarios. Through this process, we’ve been able to extend Velociraptor’s functionality and prove its performance on real client networks.\nI thought I would write a short blog post to show how Velociraptor performed on such a recent engagement. In this engagement we deployed Velociraptor on AWS and selectively pushed the client to around 600 machines running a mix of MacOS and Windows.\nThis post will hopefully give readers some idea of how scalable the tool is and the typical workloads we run with it.\nThe Server Since this is a smallish deployment we used a single VM with 32Gb of RAM and 8 cores. This was definitely over speced for this job as most of the time the server consumed less than 10% of one core:\ntop - 06:26:13 up 29 days, 2:31, 5 users, load average: 0.00, 0.01, 0.05 Tasks: 214 total, 1 running, 213 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.5 us, 0.1 sy, 0.0 ni, 99.4 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem: 32948060 total, 14877988 used, 18070072 free, 411192 buffers KiB Swap: 0 total, 0 used, 0 free. 13381224 cached Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 19334 root 20 0 1277924 94592 12616 S 3.0 0.3 9:11.03 ./velociraptor --config server.config.yaml frontend 8 root 20 0 0 0 0 S 0.3 0.0 7:16.30 [rcuos/0]  You can see that the server consumed about 95mb when operating normally and CPU usage was around 3% of one core.\nFor this engagement we knew that we would be collecting a lot of data and so we specified a large 500gb volume.\n Hunt performance Velociraptor works by collecting \u0026quot;Artifacts\u0026quot; from clients. Artifacts are simply encapsulated VQL queries which specify something to search for on the endpoint. Without going into the details of this engagement, we can say that we collected typical artifacts for a DFIR/Forensic investigation engagement. In the following I want to explore how well hunts performed for the following typical artifacts in order of complexity:\nSearch the filesystem for a file glob. Download the $MFT from the root filesystem. Run a Yara scan over every file on all mounted filesystems.  We ran these artifact collections over a large number of hosts (between 400-500) that fell within the scope of our engagement. Although the number of hosts is not huge, we hope to demonstrate Velociraptor's scalability.\n Searching for a file glob One of the simplest and most common tasks in DFIR is to search the filesystem for a glob based on filename. This requires traversing directories and matching the filename based on the user specified expression - for example, find all files with the extension *.exe within the C:\\Users directory.\nVelociraptor can glob over the entire filesystem or over a limited set of files. Typically a full filesystem glob can take some minutes on the endpoint (it is equivalent to running the find unix command) and touches every file. We typically try to limit the scope of the glob as much as possible (e.g. only search system directories) but sometimes it is nice to run a glob over all mounted filesystems to make sure we don't miss anything. In this case we opted for a full filesystem scan.\nWe searched the entire deployment using a hunt (The hunt is constructed using the File Finder flow in the GUI) which just launches the artifact collection. Therefore the horizontal distance between the red and blue dot, in the graph below, represents the total time taken by the host to collect the artifact.\nFileNameSearch.svg The graph shows how many hosts were recruited into this hunt on the Y axis. The X axis show the number of seconds since the hunt launch. The red points indicate the time when clients started their collection, while the blue dots indicate the time when the client completed the artifact collection and the server saved its results.\nThe inset shows the same data but zoomed into the time origin.\nVelociraptor improves and builds on the initial ideas implemented within the GRR DFIR tool, and so it is interesting to compare this graph to a typical graph produced by GRR's hunt (reproduced from this paper).\nThe first noticeable difference is that Velociraptor clients complete their collection much faster than GRR's (the horizontal distance between the red and blue dots represents the time between when the collection is issued and the time it completes).\nThe main reason for this is that GRR's communication protocol relies on polling (by default every 10 minutes). Also, since hunting is so resource intensive in GRR, the clients actually poll the hunt foreman task every 30 minutes by default. This means that GRR clients typically have to wait up to 30 minutes to run a hunt!\nThe second difference is the slope of the line around the start of the hunt. GRR implements a hunt client rate - clients are recruited into the hunt slowly (by default 20 per minute) in order to limit the load on the frontends. Unlike GRR, Velociraptor does not implement a hunt rate since the Velociraptor frontend load is controlled by limiting concurrency instead (more on this below).\nThis means that Velociraptor can deliver useful results within seconds of the hunt starting. We see that this particular filename search typically takes 25-30 seconds and we see about 200 clients completing the hunt within this time consistently. The remaining clients are probably not online and they receive the hunt as they join the network. This makes Velociraptor hunts far more responsive and useful.\nYou might also notice a few outliers which spend a long time collecting this artifact - these machines have probably been shutdown or suspended while collecting this artifact.\n MFT Download A common technique is to examine the Master File Table (MFT) of an NTFS volume. By forensically analyzing the MFT it is possible to detect deleted files, time stomping and build a timeline of the system using tools like analyseMFT.py or ntfswalker .\nIn this case we decided to collect the $MFT from all the Windows hosts and post-process them offline. Typically the MFT is around 300-400mb and could be larger. Therefore this artifact collection is about performance downloading large quantities of data from multiple hosts quickly.\nVelociraptor can read the raw NTFS partition and therefore read the $MFT file. We wrote the following artifact to just fetch the $MFT file:\nname: Artifact.NTFS.MFT_puller description: | Uses an NTFS accessor to pull the $MFT parameters: - name: path default: \\\\.\\C:\\$MFT sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - SELECT upload(file=path, accessor=\u0026quot;ntfs\u0026quot;) as Upload from scope()  Here is the timing graph for this artifact collection:\nMFTDownload.svg This collection takes a lot longer on each host as clients are uploading around 400mb each to the server, but our server was in the cloud so it had fast bandwidth. Again we see the hosts that are currently up being tasked within seconds, while as hosts come online gradually we see them receiving the hunt and a few minutes later uploading their $MFT file.\nWas the frontend loaded at the time? I took a screenshot of top on the server seconds after launching the hunt:\nWe can see that the CPU load is trivial (4.7%) but the major impact of a heavy upload collection is the memory used (about 4.7gb - up from about 100mb). The reason is that each client is posting a large buffer of data (several mb) simultaneously. The server needs to buffer the data before it can decrypt and process it which takes memory.\nIn order to limit the amount of memory used, Velociraptor limits the total number of connections it is actively processing to 8-10 concurrent connections. By carefully managing concurrency we are able to keep a limit on server memory use. We may lower the total memory use by reducing the concurrency (and therefore maybe fit into a smaller VM). Clients simply wait until the server is able to process their uploaded buffers. If the server takes too long, the clients automatically back off and retry to send the same buffer.\n Yara scan over the entire filesystem The final example of a very intense artifact is to scan the entire filesystem with a YARA rule. This not only requires traversing the entire filesystem, but also opening each file and searching it.\nOne of the dangers with such a scan is that users will be negatively impacted as their workstations start to read every file on disk! The main resources a YARA scan consumes is disk IO and CPU load. Users might complain and blame Velociraptor for their machine being slow (disk IO may negatively affect performance much more than CPU load!).\nHowever in this case, we don't care how long we take to scan the user's system, as long as every file was scanned, and as long as the endpoint is not overloaded and the user's work is not affected. Luckily Velociraptor allows us to specify the trade-off between collection time and collection intensity.\nVelociraptor rate limiting Velociraptor controls client side load by rate limiting the client's VQL query. Each VQL plugin consumes an \u0026quot;operation\u0026quot; from the throttler. We define an \u0026quot;operation\u0026quot; as a notional unit of work - the heavier the VQL plugin's work, the more operations are consumed. For example for yara scanning, an operation is defined as 1mb of scanned data, or a single file if the file is smaller.\nWhen a user issues an artifact collection task, they may limit the rate at which operations are performed by the client. The Velociraptor agent then limits the operations to the specified rate. For example, if the rate is 20 ops/sec then the client will scan less than 20mb per seconds.\nOther collections may run concurrently at different rates, though; The client is not blocked while performing a single artifact collection. This makes sense since we often need to collect a low priority artifact slowly, but we do not want this to compromise rapid response to that host.\nFor example, one of our targets was a server with large attached storage. We ran the Yara scan over this system, scanning the first 100Mb of each file, at a rate of 50 ops/sec. In total we scanned 1.5Tb of files and the scan took 14 hours (for a total scan rate of 30Mb/sec).\nVelociraptor by default collects the Generic.Client.Stats artifact, which samples the client's CPU utilization and memory usage every 10 seconds. These samples are streamed to the server and form a record of the client's footprint on the endpoint. We can use this data to visualize the effects of performing the yara scan on this host:\ncpu_utilization.svg Above is the CPU usage on that particular server over the course of a full day (24 hours). The 14 hour yara scan is clearly visible but at no time is CPU utilization exceeding 30% of one core. With endpoint disk IO limited to about 30mb/sec we have achieved a balance between performance and endpoint load we are happy with.\nYaraScanFull.svg We can see that most endpoints take approximately an hour to perform this yara scan, but server load is minimal since the server simply stores the results of the scans while doing minimal processing.\n  Conclusions This post provides some typical numbers for Velociraptor performance in typical DFIR engagements. We also covered some considerations and trade-offs we must think about when issuing large artifact collections. Readers can use these as a guideline in their own deployments - please comment below about your experiences. Velociraptor is under very active development and this feedback is important to ensure we put in place the mechanisms to account for more use cases.\nThanks We would like to thank the folk at Klein\u0026amp;Co for their wonderful support and assistance in Velociraptor development.\n  "
},
{
        "uri": "/blog/html/2019/02/09/velociraptor_python_api.html",
        "title": "The Velociraptor Python API",
        "tags": [],
        "description": "Velociraptor usually is only a part of a wider solution which might include a SIEM and SOC integration. In order to facilitate interoperability with other tools, Velociraptor now offers an external API.",
        "content": "Velociraptor is very good at collecting artifacts from endpoints. However, in modern DFIR work, the actual collection is only the first step of a much more involved process. Typically we want to post process data using more advanced data mining tools (such as data stacking). Velociraptor usually is only a part of a wider solution which might include a SIEM and SOC integration.\nIn order to facilitate interoperability with other tools, Velociraptor now offers an external API. The API is offered via gRPC so it can be used in any language which gRPC supports (e.g. Java, C++, Python etc). In this blog post we illustrate the Python API but any language should work.\nThe Velociraptor API Server The API server exposes an endpoint ready to accept gRPC connections. By default the API server listen only on the loopback interface (127.0.0.1) but it is easy to change to be externally accessible if you need by changing the server.config.yaml file:\nAPI: bind_address: 127.0.0.1 bind_port: 8001  Client programs simply connect directly to this API and call gRPC methods on it.\nThe connection is encrypted using TLS and authenticated using mutual certificates. When we initially created the Velociraptor configuration file, we created a CA certificate and embedded it in the server.config.yaml file. It is this CA certificate which is used to verify that the certificate each end presents was issued by the Velociraptor CA.\nNote\nIf you need to have extra security in your environment you should keep the original server.config.yaml file generated in an offline location, then deploy a redacted file (without the CA.private_key value) on the server. This way api client certificates can only be issued offline.\n Before the client may connect to the API server they must have a certificate issued by the Velociraptor CA. This is easy to generate:\n$ velociraptor --config server.config.yaml \\  config api_client --name Fred \u0026gt; api_client.yaml  Will generate something like:\nca_certificate: | -----BEGIN CERTIFICATE----- MIIDITCCAgmgAwIBAgIRAI1oswXLBFqWVSYZx1VibMkwDQYJKoZIhvcNAQELBQAw -----END CERTIFICATE----- client_cert: | -----BEGIN CERTIFICATE----- 2e1ftQuzHGD2XPquqfuVzL1rtEIA1tiC82L6smYbeOe0p4pqpsHN1sEDkdfhBA== -----END CERTIFICATE----- client_private_key: | -----BEGIN RSA PRIVATE KEY----- sVr9HvR2kBzM/3yVwvb752h0qDOYDfzLRENjA7dySeOgLtBSvd2gRg== -----END RSA PRIVATE KEY----- api_connection_string: 127.0.0.1:8001 name: Fred  The certificate generated has a common name as specified by the --name flag. This name will be logged in the server's audit logs so you can use this to keep track of which programs have access. This file keeps both private key and certificate as well as the CA certificate which must be used to authenticate the server in a single file for convenience.\n Using the API from Python Although the API exposes a bunch of functions used by the GUI, the main function (which is not exposed through the GUI) is the Query() method. This function simply executes one or more VQL queries, and streams their results back to the caller.\nThe function requires an argument which is a protobuf of type VQLCollectorArgs:\nVQLCollectorArgs: env: list of VQLEnv(string key, string value) Query: list of VQLRequest(string Name, string VQL) max_row: int max_wait: int ops_per_second: float  This very simple structure allows the caller to specify one or more VQL queries to run. The call can set up environment variables prior to the query execution. The max_row and max_wait parameters indicate how many rows to return in a single result set and how long to wait for additional rows before returning a result set.\nThe call simply executes the VQL queries and returns result sets as VQLResponse protobufs:\nVQLResponse: Response: json encoded string Columns: list of string total_rows: total number of rows in this packet  The VQL query may return many responses - each represents a set of rows. These responses may be returned over a long time, the API call will simply wait until new responses are available. For example, the VQL may represent an event query - i.e. watch for the occurrence of some event in the system - in this case it will never actually terminate, but keep streaming response packets.\n How does this look like in code? The following will cover an example implementation in python. The first step is to prepare credentials for making the gRPC call. We parse the api_config yaml file and prepare a credential object:\nconfig = yaml.load(open(\u0026quot;api_client.yaml\u0026quot;).read()) creds = grpc.ssl_channel_credentials( root_certificates=config[\u0026quot;ca_certificate\u0026quot;].encode(\u0026quot;utf8\u0026quot;), private_key=config[\u0026quot;client_private_key\u0026quot;].encode(\u0026quot;utf8\u0026quot;), certificate_chain=config[\u0026quot;client_cert\u0026quot;].encode(\u0026quot;utf8\u0026quot;)) options = (('grpc.ssl_target_name_override', \u0026quot;VelociraptorServer\u0026quot;,),)  Next we connect the channel to the API server:\nwith grpc.secure_channel(config[\u0026quot;api_connection_string\u0026quot;], creds, options) as channel: stub = api_pb2_grpc.APIStub(channel)  The stub is the object we use to make calls with. We can then issue our call:\nrequest = api_pb2.VQLCollectorArgs( Query=[api_pb2.VQLRequest( VQL=query, )]) for response in stub.Query(request): rows = json.loads(response.Response) for row in rows: print(row)  We issue the query and then just wait for the call to generate response packets. Each packet may contain several rows which will all be encoded as JSON in the Response field. Each row is simply a dict with keys being the column names, and the values being possibly nested dicts or simple data depending on the query.\n What can we do with this? The Velociraptor API is deliberately open ended - meaning we do not pose any limitations on what can be done with it. It is conceptually a very simple API - just issue the query and look at the results, however this makes it extremely powerful.\nWe already have a number of very useful server side VQL plugins you can use. We also plan to add a number of other plugins in future - this means that the Velociraptor API can easily be extended in a backwards compatible way by simply adding new VQL plugins. New queries can do more, without breaking existing queries.\nPost process artifacts This is the most common use case for the API. Velociraptor deliberately does not do any post processing on the server - we don't want to slow the server down by making it do more work than necessary.\nBut sometimes users need to do some more with the results - for example upload to an external system, check hashes against Virus Total, and even initiate an active response like escalation or disruption when something is detected.\nIn a recent engagement we needed to collect a large number of $MFT files from many endpoints. We wanted to analyze these using external tools like analyseMFT.py.\nWe wrote a simple artifact to collect the MFT:\nname: Windows.Upload.MFT description: | Uses an NTFS accessor to pull the $MFT parameters: - name: path default: \\\\.\\C:\\$MFT sources: - precondition: SELECT OS From info() where OS = 'windows' queries: - select upload(file=path, accessor=\u0026quot;ntfs\u0026quot;) as Upload from scope()  We then created a hunt to collect this artifact from the machines of interest. Once each $MFT file is uploaded we need to run analyseMFT.py to parse it:\nQUERY=\u0026quot;\u0026quot;\u0026quot; SELECT Flow, file_store(path=Flow.FlowContext.uploaded_files) as Files FROM watch_monitoring(artifact='System.Flow.Completion') WHERE 'Windows.Upload.MFT' in Flow.FlowContext.artifacts \u0026quot;\u0026quot;\u0026quot; with grpc.secure_channel(config[\u0026quot;api_connection_string\u0026quot;], creds, options) as channel: stub = api_pb2_grpc.APIStub(channel) request = api_pb2.VQLCollectorArgs( Query=[api_pb2.VQLRequest( VQL=QUERY, )]) for response in stub.Query(request): rows = json.loads(response.Response) for row in rows: for file_name in row[\u0026quot;Files\u0026quot;]: subprocess.check_call( [\u0026quot;analyseMFT.py\u0026quot;, \u0026quot;-f\u0026quot;, file_name, \u0026quot;-o\u0026quot;, file_name+\u0026quot;.analyzed\u0026quot;])  The previous code sets up a watcher query which will receive every completed flow on the server which collected the artifact \u0026quot;Windows.Upload.MFT\u0026quot; (i.e. each completed flow will appear as a row to the query).\nWe can have this program running in the background. We can then launch a hunt collecting the artifact, and the program will automatically process all the results from the hunt as soon as they occur. When new machines are turned on they will receive the hunt, have their $MFT collected and this program will immediately process that.\nEach flow contains a list of files that were uploaded to it. The file_store() VQL function reveals the server's filesystem path where the files actually reside. The server simply stores the uploaded files on its filesystem since Velociraptor does not use a database (everything is a file!).\nThe python code then proceeds to launch the analyseMFT.py script to parse the $MFT.\nNote\nThe nice thing with this scheme is that the analyseMFT.py is running in its own process and can be managed separately to the main Velociraptor server (e.g. we can set its execution priority or even run it on a separate machine). The Velociraptor server does not actually need to wait for post processing nor will the post processing affect its performance in any way. If the analyseMFT.py script takes a long time, it will just fall behind but it eventually will catch up. In the meantime, the Velociraptor server will continue receiving the uploads regardless.\n The above example sets up a watcher query to receive flow results in real time, but you can also just process the results of a specific hunt completely using a query like:\nSELECT Flow, file_store(path=Flow.FlowContext.uploaded_files) as Files FROM hunt_flows(hunt_id=huntId)    Conclusions The Velociraptor python API opens up enormous possibilities for automating Velociraptor and interfacing it with other systems. Combining the power of VQL and the flexibility (and user familiarity) of Python allows users to build upon Velociraptor in a flexible and creative way. I am very excited to see what the community will do with this feature - I can see integration with ELK, BigQuery and other data analytic engines being a valuable use case.\nPlease share your experiences in the comments or on the mailing list at velociraptor-discuss\u0026#64;groups.google.com.\n "
},
{
        "uri": "/blog/html/2018/09/03/velociraptor_s_client_communications.html",
        "title": "Velociraptor's client communications",
        "tags": [],
        "description": "In the latest point release of the Velociraptor IR tool (0.2.3) we\nhave improved upon GRR's client communications protocol to deliver a\nfast and efficient, yet extremely responsive client\ncommunication. This post explains the design of the client\ncommunication and how it solves the problems with the old GRR's client\ncommunication.\n",
        "content": "How does the GRR client communicate? The GRR client protocol is depicted below.\nDue to network realities such as NAT, firewalls etc, it is not possible to directly connect to the client, so GRR relies on the client connecting to the server in order to communicate with it.\nThe GRR client makes periodic POST requests to the server to both send replies and receive new instructions. Since POST requests are very short lived (most client polls carry no data) the client has to repeat the polls periodically.\nThere are two parameters which determine how the GRR client behaves - poll_max and poll_min. When there is some requests sent to the client, the client will reduce its poll wait time to poll_min (default 0.2 seconds). When nothing happens, the client will increase its poll time gradually up to poll_max (default 10 minutes).\nHaving long poll times means that any new flows launched on an idle client must wait for up to 10 minutes before the client polls again in order to send the new requests to the client. Unfortunately reducing the poll_max setting actually increases server load, as the server needs to hit the database more often to serve each poll. This scheme essentially poses a trade off - for a responsive client, we must have a low poll_max (i.e. more frequent polls) but this increases the load on the frontend so it can not be too low.\nWhen GRR is normally deployed it produces 2 types of clients on the web interface: the debug client has max_poll set to 5 seconds making testing easier because it is more responsive, but the non-debug version has max_poll set to 10 minutes. For example at Velocidex, one of our clients had once accidentally deployed the debug version and the server was slammed with 5 second polls from several thousand clients! This rendered the server useless, returning HTTP 500 status codes for most client polls. The only way to recover was to push new config to the clients and restart their GRR service in order to lower the poll frequency and recover control over the deployment.\n Catastrophic failure under load The other problem with GRR's client communication protocol is that it tends to exhibit catastrophic failure under load. When the client makes a HTTP POST operation, the server goes through the following steps in order:\nUnpack and decrypts any replies the client sends in its POST message Queue these replies on a worker queue Read the client's job queue for any outstanding requests to the client. Pack and encrypt these requests to the client. Write them as the body response of the HTTP POST with hopefully a 200 HTTP status.  In previous posts we have seen that GRR's overuse of queuing leads to extreme loads on the database, so under load (e.g. when a large hunt is taking place), the above process may take some time until the server can obtain a lock on the database row, write and read the messages, and compose its response.\nWhat tends to happen under load, is that the client will time the request out if the server takes too long, or the server itself may timeout the request with a HTTP 500 code. The client, thinking it has not got through will try to POST the same data again (this time it will wait longer though).\nThis essentially makes things worse, because the replies are probably already mostly queued so the next retry will re-queue the same requests (these will be discarded by the worker anyway but they are still queued), increasing database pressure and server load. This manifests in a critical meltdown of the frontends who pretty soon serve mostly 500 errors (making things worse again).\nThis is the reason why resource provision is so important with GRR, if the frontends are just too slow to be able to keep up, the connections will start to timeout, and load increases (rather than decreases) causing a catastrophic failure.\n How can we fix this? The main problem with a polling scheme is that the user experience is terrible - even if we reduce the poll wait times to few seconds, users will have to wait to view the results of their actions - leading to an overall experience of a slow and sluggish system. For a resposive user interface we need to have client round trips of a second or less and having poll_max set this low will just use up too many resources. This is particularly noticeable in the VFS browser since it takes so long to navigate to the desired directory and download files interactively.\nOther endpoint monitoring systems use distributed pub/sub systems like RabbitMQ or Firebase realtime database to inform the clients of new requests. In those systems, the client makes a TCP connection to an endpoint and holds the connection open for long periods of time, the server can then immediately push new requests to the client as soon as they are published. This seems like the way to go but we did not want to introduce another dependency on Velociraptor (we really like it being a self contained - working out of the box binary).\nIn particular we also wanted to solve the catastrophic failure we saw with GRR clients under load (described above). This means that we need to make sure that the clients are not sending data faster than the server can process it. We definitely want to avoid the POST timing out with a 500 error and the client retrying the same POST since this is the main cause for the catastrophic failures we experienced with GRR.\nWe can do this by keeping the client's connection open for as long as we need, but in order to not time it out, we send the HTTP status code immediately, then process the POST data, while sending the client keepalive data periodically using HTTP chunked transfer encoding.\nTo the client, and any proxies in the way, it simply looks like the POST request was received immediately, and the response body is downloaded slowly (there is always some pad data flowing so none of the TCP or HTTP timers are triggered since the connection is always active). This is illustrated in the diagram below.\nThis scheme has the two main advantages:\nBy returning a 200 status to the client before we begin processing, the client knows we received the data. They are then able to de-queue these messages and will not transmit them again. By keeping the client connected while the server is processing the request we avoid any additional data from being sent to the server while it is busy. The client will be blocked on the HTTP connection and will actually pause its running VQL query while the server is processing the current responses. This mechanism actually throttles the clients to allow the server to keep up.  Making the client more responsive We really wanted to make clients more responsive. We were frankly sick of having to wait up to 10 minutes to access a client that we knew was online in our IR work. To make the client more responsive we wanted to use the same technique to keep the client connection open for long periods of time, and then send instructions to the client as soon as the user issues a new flow.\nIn the GRR scheme new requests are sent on the same connections as client replies are received. This won't work if the client connection is held open for long periods of time because while the client is blocked reading new responses from the server, it can not send any replies (the POST header was already sent).\nTo fix this we switched to two separate POST connections on two server handlers, a reader handler and a writer handler. The writer handler only receives messages from the client to the server (i.e. replies to client requests), while the reader handler blocks the client for prolonged time and sends client requests as soon as new flows are launched.\nThis scheme allows a full duplex, responsive communication protocol, with no polling overheads. This can be seen in the diagram below.\nThe client establishes the reader channel by sending a HTTP POST request to the reader handler. The server checks for any messages for the client, and sees that there are none pending. It will then keep the client's connection open as before, trickle sending pad data (using HTTP chunked transfer encoding) to keep the connection open for as long as possible.\nWhen the user launches a new flow, the server can immediately forward the client's requests on the open channel, completing the POST operation. The client will then process the requests and send the responses with a separate HTTP POST to the writer channel. In the meantime the reader channel will re-POST to the reader handler and become blocked and ready for the next request.\nThis scheme has the following advantages:\nThe user's flow is executed instantly by the client. This makes for example, the VFS browser instant - as soon as the user clicks the \u0026quot;refresh directory listing\u0026quot; button, the directory is refreshed. As soon as the user wants to view a file, the file is downloaded etc. There is hardly any polling activity. The clients open a reader connection once and hold it for many minutes. The server need only check the queue at the beginning of the connection and then only if it knows there is a new flow launched for this client. This means server load is really low.  However, the scheme also has some disadvantages:\nTCP connections are held for long periods of time tying up server resources. In particular the open sockets count towards the process's open file descriptor limit. It is typically necessary to increase this limit (by default it is 1024 which is very low). Deploying over multiple servers is a bit more complex because a client may be blocked on one server and the flow is launched on another server. Velociraptor now has a notification API to allow inter server RPCs to propagate notifications between servers.  We believe that these limitations can be easily managed. They are no different from typical limitations of large scale pub/sub systems (they too need to hold many TCP connections open). In our testing we have not seen a problem scaling to many thousands of connected clients with very low resource use.\nVelociraptor now also has a pool client that allows spinning up several thousand clients at the same time. This helps with testing a deployment to make sure it can handle the increased open file limit and test how large scale hunts can be handled.\n Conclusions The new responsive client communications protocol allows for near instantaneous access to clients. This actually reduces the overall load on the system because we do not need to perform frequent client polls just to check if a new flow is launched. User experience is much better as users can interact with clients immediately.\n  "
},
{
        "uri": "/blog/html/2018/12/23/deploying_velociraptor_with_oauth_sso.html",
        "title": "Deploying Velociraptor with OAuth SSO",
        "tags": [],
        "description": "In the previous post we saw how to set up Velociraptor's GUI over\nSSL. In this post we discuss how to enable Google's SSO authentication for\nVelociraptor identity management.\n",
        "content": "In the previous post we saw how to set up Velociraptor's GUI over SSL. This is great, but we still need to create users and assign them passwords manually. The trouble with user account management is that we can not enforce 2 factor authentication, or any password policies or any of the usual enterprise requirements for user account management. It is also difficult for users to remember yet another password for a separate system, and so might make the password easily guessable.\nMost enterprise systems require an SSO mechanism to manage user accounts and passwords. Manual user account management simply does not scale!\nIn this post we discuss how to enable Google's SSO authentication for Velociraptor identity management.\nOAuth Identity management Velociraptor can use Google's oauth mechanism to verify a user's identity. This requires a user to authenticate to Google via their usual mechanism - if their account requires 2 factor authentication, then users need to log in this way.\nOnce the user authenticates to Google, they are redirected back into the Velociraptor application with a token that allows the application to request information about the user (for example, the username or email address).\nNote\nOAuth is an authentication protocol. This means Velociraptor can be pretty confident the user is who they claim they are. This does not automatically grant them access to the application! A Velociraptor administrator must still manually grant them access before a user may log in.\n Before we can use Google for Authentication, we need to register our Velociraptor deployment as an OAuth App with Google. Unfortunately Google is not known for having intuitive and easy to follow processes so actually doing this is complicated and bounces through many seemingly unrelated Google products and services. This post attempts to document this process at it exists in this time.\nFor our example we assume that our server is located at https://velociraptor.rekall-innovations.com as we continue on from our example in the last post (i.e. it is already configured to use SSL).\n Registering Velociraptor as an OAuth application The first step is to register Velociraptor as an OAuth app. We do this by accessing the Google cloud console at https://console.cloud.google.com . You will need to set up a cloud account first and create a cloud project. Although in this example we do not necessarily need to host our application on Google cloud or have anything to do with Google cloud, OAuth seems to exist within the Google cloud product.\nOur ultimate goal is to obtain OAuth credentials to give our Velociraptor app, but we have to have a few things set up first. The cloud console is fairly confusing so I usually use the search feature to find exactly what I need. Searching for \u0026quot;oauth\u0026quot; at the search bar indicates that it is under \u0026quot;APIs and Services\u0026quot;.\nWe need to set up the OAuth consent screen first - in which we give our application a name to be presented to the user by the OAuth flow:\nFurther down we need to provide an authorized domain\nIn order to add an Authorized domain we need to verify it. Google's help pages explain it further:\nAuthorized domains\nTo protect you and your users, Google restricts your OAuth 2.0 application to using Authorized Domains. If you have verified the domain with Google, you can use any Top Private Domain as an Authorized Domain.\n And this links to https://www.google.com/webmasters/tools/home which again seems completely unrelated to OAuth, Velociraptor or even a web app (the web masters product is supposed to help sites increase their search presence).\nWithin this product we now need to \u0026quot;Add a property\u0026quot;:\nHidden within the settings menu there is an option \u0026quot;Verification Details\u0026quot; which allows you to verify that you own the domain. If you purchased your domain from Google Domains then it should already be verified - otherwise you can set some TXT records to prove you own the domain.\nAfter all this we can go back to the cloud console and Create Credentials/OAuth client ID:\nNow select \u0026quot;Web App\u0026quot; and we must set the \u0026quot;Authorized redirect URIs\u0026quot; to https://velociraptor.rekall-innovations.com/auth/google/callback - This is the URL that successful OAuth authentication will direct to. Velociraptor accepts this redirect and uses it to log the user on.\nNote\nThe UI is a bit confusing here - you must press enter after typing the redirect URL to have it registered before you hit Create otherwise it misses that you typed it completely. I spent some time stumped on this UI bug.\n If all goes well the Google cloud console will give us a client ID and a client secret. We can then copy those into the Velociraptor configuration file under the GUI section:\nGUI: google_oauth_client_id: 1234xxxxxx.apps.googleusercontent.com google_oauth_client_secret: qsadlkjhdaslkjasd public_url: https://velociraptor.rekall-innovations.com/ logging: output_directory: /var/log/velociraptor/ separate_logs_per_component: true  In the above config we also enabled logging (which is important for a secure application!). The separate_logs_per_component option will create a separate log file for the GUI, Frontend as well as important Audit related events.\nNow we can start the Velociraptor frontend:\n$ velociraptor --config server.config.yaml frontend  Connecting using the browser goes through the familiar OAuth flow and arrives at this Velociraptor screen:\nThe OAuth flow ensures the user's identity is correct but does not give them permission to log into Velociraptor. Note that having an OAuth enabled application on the web allows anyone with a Google identity to authenticate to the application but the user is still required to be authorized. We can see the following in the Audit logs:\n{ \u0026quot;level\u0026quot;: \u0026quot;error\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;msg\u0026quot;: \u0026quot;User rejected by GUI\u0026quot;, \u0026quot;remote\u0026quot;: \u0026quot;192.168.0.10:40570\u0026quot;, \u0026quot;time\u0026quot;: \u0026quot;2018-12-21T18:17:47+10:00\u0026quot;, \u0026quot;user\u0026quot;: \u0026quot;mike\u0026#64;velocidex.com\u0026quot; }  In order to authorize the user we must explicitly add them using the velociraptor admin tool:\n$ velociraptor --config ~/server.config.yaml user add mike\u0026#64;velocidex.com Authentication will occur via Google - therefore no password needs to be set.  Note that this time, Velociraptor does not ask for a password at all, since authentication occurs using Google's SSO. If we hit refresh in the browser we can now see the Velociraptor application:\nWe can see that the logged in user is authenticated by Google, and we can also see their Google avatar at the top right for some more eye candy :-).\nThanks\nShouts to the folks from Klein \u0026amp; Co who sponsored this exciting feature!.\n  "
},
{
        "uri": "/blog/html/2018/12/22/configuring_velociraptor_for_ssl.html",
        "title": "Configuring Velociraptor for SSL",
        "tags": [],
        "description": "This post describes how to deploy Velociraptor with SSL on a cloud VM.\n",
        "content": "We have previously seen how to deploy a new Velociraptor server. For a simple deployment we can have Velociraptor server and clients provisioned in minutes.\nUsually we deploy a specific Velociraptor deployment on our DFIR engagements. We use cloud resources to provision the server and have the clients connect to this cloud VM. A proper secure deployment of Velociraptor will use SSL for securing both client communication and protecting the web GUI.\nIn the past provisioning an SSL enabled web application was complex and expensive - you had to create certificate signing requests, interact with a CA. Pay for the certificates, then configure the server. In particular you had to remember to renew the cert in 2 years or your website suddenly broke!\nThose days are over with the emergence of Lets Encrypt! and autocert. These days applications can automatically provision their own certificates. Velociraptor can manage its own certificates, fully automatically - and then renew its certificates when the time comes with no user intervention required.\nIn this blog post we will see how to configure a new Velociraptor server in a cloud VM.\nSetting up a domain The first step in deploying an SSL enabled web application is to have a domain name. SSL verifies the authenticity of a web site by its DNS name.\nWe go over to Google Domains and buy a domain. In this post I will be using the domain rekall-innovations.com.\n Provisioning a Virtual Machine Next we provision an Ubuntu VM from any cloud provider. Depending on your deployment size your VM should be large enough. An 8 or 16Gb VM should be sufficient for around 5-10k clients. Additionally we will need sufficient disk space to hold the data we will collect. We recommend to start with a modest amount of storage and then either backup data as it gets collected or increase the storage volume as needed.\nOur virtual machine will receive connections over ports 80 and 443.\nNote\nWhen using SSL both the client communication and the GUI are served over the same ports to benefit from SSL transport encryption.\n When we deploy our Virtual Machine we may choose either a static IP address or allow the cloud provider to assign a dynamic IP address. We typically choose a dynamic IP address and so we need to configure Dynamic DNS.\nGo to the Google Domains dashboard and create a new dynamic DNS for your domain. In our example we will use velociraptor.rekall-innovations.com as our endpoint address.\nAfter the dynamic address is created, we can get the credentials for updating the IP address.\nNext we install ddclient on our VM. This will update our dynamic IP address whenever the external interface changes. Configure the file /etc/ddclient.conf:\nprotocol=dyndns2 use=web server=domains.google.com ssl=yes login=X13342342XYZ password='slk43521kj' velociraptor.rekall-innovations.com  Next configure the service to start:\n# Configuration for ddclient scripts # generated from debconf on Tue Oct 23 20:25:23 AEST 2018 # # /etc/default/ddclient # Set to \u0026quot;true\u0026quot; if ddclient should be run every time DHCP client ('dhclient' # from package isc-dhcp-client) updates the systems IP address. run_dhclient=\u0026quot;false\u0026quot; # Set to \u0026quot;true\u0026quot; if ddclient should be run every time a new ppp connection is # established. This might be useful, if you are using dial-on-demand. run_ipup=\u0026quot;false\u0026quot; # Set to \u0026quot;true\u0026quot; if ddclient should run in daemon mode # If this is changed to true, run_ipup and run_dhclient must be set to false. run_daemon=\u0026quot;true\u0026quot; # Set the time interval between the updates of the dynamic DNS name in seconds. # This option only takes effect if the ddclient runs in daemon mode. daemon_interval=\u0026quot;300\u0026quot;  Run dhclient and check that it updates the address correctly.\n Configuring Velociraptor for SSL Now comes the hard part! We need to configure Velociraptor to use SSL. Edit the following in your server.config.yaml file (if you do not have one yet you can generate one using velociraptor config generate \u0026gt; server.config.yaml ):\nClient: server_urls: - https://velociraptor.rekall-innovations.com/ autocert_domain: velociraptor.rekall-innovations.com autocert_cert_cache: /etc/velociraptor_cache/  The autocert_domain parameter tells Velociraptor to provision its own cert for this domain automatically. The certificates will be stored in the directory specified by autocert_cert_cache. You don't have to worry about rotating the certs, Velociraptor will automatically renew them.\nObviously now the clients need to connect to the control channel over SSL so we also need to direct the client's server_urls parameter to the SSL port.\nLets start the frontend (We need to start Velociraptor as root because it must be able to bind to port 80 and 443):\n$ sudo velociraptor --config server.config.yaml frontend -v [INFO] 2018-12-22T17:12:42+10:00 Loaded 43 built in artifacts [INFO] 2018-12-22T17:12:42+10:00 Increased open file limit to 999999 [INFO] 2018-12-22T17:12:42+10:00 Launched gRPC API server on 127.0.0.1:8888 [INFO] 2018-12-22T17:12:42+10:00 Autocert specified - will listen on ports 443 and 80. I will ignore specified GUI port at 8889 [INFO] 2018-12-22T17:12:42+10:00 Autocert specified - will listen on ports 443 and 80. I will ignore specified Frontend port at 8889 [INFO] 2018-12-22T17:12:42+10:00 Frontend is ready to handle client requests using HTTPS  If all goes well we now can point our browser to https://velociraptor.rekall-innovations.com/ and it should just work. Don't forget to provision a user and password using:\n$ velociraptor --config server.config.yaml user add mic   Notes The autocert configuration is very easy to do but there are a few caveats:\nBoth ports 80 and 443 must be accessible over the web. This is needed because Letsencrypt's servers need to connect to our domain name in order to verify our domain ownership. It is not possible to change the ports from port 80 and 443 due to limitations in Letsencrypt's ACME protocol. This is why we can not have more than one Velociraptor deployment on the same IP currently.  We have seen how easy it is to deploy secure Velociraptor servers. In the next post we will discuss how to enhance security further by deploying two factor authentication with Google's Single Sign On (SSO).\nNote\nThis feature will be available in the upcoming 0.27 release. You can try it now by building from git head.\n  "
},
{
        "uri": "/blog/html/2018/12/11/velociraptor_interactive_shell.html",
        "title": "Velociraptor Interactive Shell",
        "tags": [],
        "description": "One of the interesting new features in the latest release of\nVelociraptor is an interactive shell. One can interact with the end\npoint over the standard Velociraptor communication mechanism - an\nencrypted and authenticated channel.\n",
        "content": "One of the interesting new features in the latest release of Velociraptor is an interactive shell. One can interact with the end point over the standard Velociraptor communication mechanism - an encrypted and authenticated channel.\nThis feature is implemented by utilizing the Velociraptor event monitoring, server side VQL queries. This post explores how these components come together to deliver a responsive, interactive workflow.\nEndpoint shell access Although we generally try to avoid it, sometimes the easiest way to extract certain information is to run a command and parse its output. For example, consider the windows ipconfig command. It is possible to extract this information using win32 apis but this requires additional code to be written in the client. The ipconfig command is guaranteed to be available. Soemtimes running a command and parsing its output is the easiest option.\nThe GRR client has a client action which can run a command. However that client action is restricted to run a whitelist of commands, since GRR chose to prevent the running of arbitrary commands on the endpoint. In practice, though it is difficult to add new commands to the whitelist (and rebuild and deploy new clients that have the updated whitelist). But users need to run arbitrary commands (including their own third party tools) anyway. So in the GRR world, most people use \u0026quot;python hacks\u0026quot; routinely to run arbitrary commands.\nWhen we came to redesign Velociraptor we pondered if arbitrary command execution should be included or not. To be sure, this is a dangerous capability - effectively giving Velociraptor root level access on the endpoint. In our experience restricting it in an arbitrary way (as was done in GRR) is not useful because it is harder adapt to real incident response needs (you hardly ever know in advance what is needed at 2am in the morning when trying to triage an incident!).\nOther endpoint monitoring tools also have a shell interface (For example Carbon Black). It is understood that this feature is extremely powerful, but it is necessary sometimes.\nVelociraptor mitigates this risk in a few ways:\nIf an organization deems the ability to run arbitrary commands too dangerous, they can completely disable this feature in the client's configuration. Every shell command run by the client is audited and its output is archived. Misuse can be easily detected and investigated. This feature is considered high risk and it is not available via the GUI. One must use the velociraptor binary on the server itself to run the interactive shell.   Interactive Shell The interactive shell feature is accessed by issueing the shell command to the velociraptor binary:\n$ velociraptor --config ~/server.config.yaml shell C.7403676ab8664b2b C.7403676ab8664b2b (trek) \u0026gt;ls / Running ls / on C.7403676ab8664b2b Received response at 2018-12-11 13:12:35 +1000 AEST - Return code 0 bin boot core data dev C.7403676ab8664b2b (trek) \u0026gt;id Running id on C.7403676ab8664b2b Received response at 2018-12-11 13:13:05 +1000 AEST - Return code 0 uid=1000(mic) gid=1000(mic) groups=1000(mic),4(adm),24(cdrom),27(sudo) C.7403676ab8664b2b (trek) \u0026gt;whoami Running whoami on C.7403676ab8664b2b Received response at 2018-12-11 13:13:10 +1000 AEST - Return code 0 mic  As you can see it is pretty straight forward - type a command, the command is sent to the client, and the client responds with the output.\n How does it work? The main components are shown in the figure below. Note that the shell process is a different process from the frontend:\nThe workflow starts when a user issues a command (for example \u0026quot;ls -l /\u0026quot;) on the terminal. The shell process schedules a VQL query for the client:\nSELECT now() as Timestamp, Argv, Stdout, Stderr, ReturnCode FROM execve(argv=['ls', '-l', '/'])  However, this query is scheduled as part of the monitoring flow - which means it's response will be sent and stored with the monitoring logs. As soon as the shell process schedules the VQL query the frontend is notified and the client is woken. Note that due to Velociraptor's near instantaneous communication protocol this causes the client to run the command almost immediately.\nThe client executes the query which returns one or more rows containing the Stdout of the process. The client will then send the response to the server as a monitoring event. The frontend will then append the event to a CSV file.\nAfter sending the initial client query, the interactive shell process will issue a watch VQL query to watch for the shell response:\nSELECT ReturnCode, Stdout, Stderr, Timestamp, Argv FROM watch_monitoring(client_id=ClientId, artifact='Shell')  The process now blocks until this second query detects the response arrived on the monitoring queue. Now we simply display the result and go back to the interactive prompt.\nNote that the interactive shell is implemented using the same basic building blocks that Velociraptor offers:\nIssuing client VQL queries. Waking the client immediately gives instant results (no need for polling). Utilizing the event monitoring flow to receive results from queries immediately. Writing server side event queries to watch for new events, such as responses from the client.  Note that the frontend is very simple and does no specific processing of the interactive shell, the feature is implemented completely within the interactive shell process itself. This design lowers the load on the frontends since their job is very simple, but enables complex post processing and interaction to tbe implemented by other processes.\n Auditing We mentioned previously that running shell commands on endpoints is a powerful feature and we need to audit its use closely. Since shell command output is implemented via the monitored event queues it should be obvious that we can monitor all such commands by simply watching the Shell artifact event queue:\n$ velociraptor query \u0026quot;select * from watch_monitoring(artifact='Shell')\u0026quot; [ { \u0026quot;Argv\u0026quot;: \u0026quot;\\\u0026quot;{\\\\\\\u0026quot;Argv\\\\\\\u0026quot;:[\\\\\\\u0026quot;id\\\\\\\u0026quot;]}\\\u0026quot;\u0026quot;, \u0026quot;Artifact\u0026quot;: \u0026quot;Shell\u0026quot;, \u0026quot;ClientId\u0026quot;: \u0026quot;C.7403676ab8664b2b\u0026quot;, \u0026quot;ReturnCode\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Stderr\u0026quot;: \u0026quot;\\\u0026quot;\\\u0026quot;\u0026quot;, \u0026quot;Stdout\u0026quot;: \u0026quot;\\\u0026quot;uid=1000(mic) gid=1000(mic) groups=1000(mic)\\\\n\\\u0026quot;\u0026quot;, \u0026quot;Timestamp\u0026quot;: \u0026quot;1544499929\u0026quot; } ]  We can easily write an artifact that escalates any use of the interactive shell by sending the admin an mail (See previous blog post). This way we can see if someone missused the feature. Alternatively we may simply archive the event queue CSV file for long term auditing of any interactive shell use.\n "
},
{
        "uri": "/blog/html/2018/12/10/server_side_vql_queries_and_events.html",
        "title": "Server side VQL queries and Escalation Events",
        "tags": [],
        "description": "It is also possible to run VQL\nqueries on the server side! Similarly server side Velociraptor\nartifacts can be used to customize the operation of the server -\nwithout modifying any code or redeploying the server components.\n",
        "content": "Previously we have seen how Velociraptor collects information from end points using Velociraptor artifacts. These artifacts encapsulate user created queries using the Velociraptor Query Language (VQL). The power of VQL is that it provides for a very flexible way of specifying exactly what should be collected from the client and how - without needing to modify client code or deploy new clients!\nThis is not the whole story though! It is also possible to run VQL queries on the server side! Similarly server side Velociraptor artifacts can be used to customize the operation of the server - without modifying any code or redeploying the server components.\nServer Side VQL Queries. By now you are probably familiar with Velociraptor and VQL. We have seen that it is possible to run a VQL query interactively from the commandline. For example to find all processes matching the 'gimp':\n$ velociraptor query \\  \u0026quot;SELECT Pid, Exe, Cmdline FROM pslist() WHERE Exe =~ 'gimp'\u0026quot; [ { \u0026quot;Cmdline\u0026quot;: \u0026quot;gimp-2.10\u0026quot;, \u0026quot;Exe\u0026quot;: \u0026quot;/usr/bin/gimp-2.10\u0026quot;, \u0026quot;Pid\u0026quot;: 13207 } ]  We have used this feature previously in order to perfect and test our queries by interactively building the query as we go along.\nHowever it is also possible to run queries on the server itself in order to collect information about the server. There is nothing special about this as such - it is simply that some VQL plugins are able to operate on the server's internal data store and therefore provide a way to interact with the server via VQL queries.\nNote\nOther endpoint monitoring tools export a rich API and even an API client library to enable users to customize and control their installation. For example, GRR expects users write python scripts using the GRR client API library.\nVelociraptor's approach is different - the functionality typically available via APIs is made available to VQL queries via VQL plugins (e.g. client information, flow information and results collected). In this way the VQL itself forms an API with which one controls the server and deployment. There is no need to write any code - simply use existing VQL plugins in any combination that makes sense to create new functionality - then encapsulates these queries inside Velociraptor artifacts for reuse and sharing.\n For example, to see all the clients and their hostnames:\n$ velociraptor query \\  \u0026quot;SELECT os_info.fqdn as Hostname, client_id from clients()\u0026quot; --format text +-----------------+--------------------+ | Hostname | client_id | +-----------------+--------------------+ | mic-Inspiron | C.772d16449719317f | | TestComputer | C.11a3013cca8f826e | | trek | C.952156a4b022ddee | | DESKTOP-IOME2K5 | C.c916a7e445eb0868 | +-----------------+--------------------+ SELECT os_info.fqdn AS Hostname, client_id FROM clients()  To inspect what flows were run on a client:\n$ velociraptor query \\  \u0026quot;SELECT runner_args.creator, runner_args.flow_name, \\ runner_args.start_time FROM \\ flows(client_id='C.772d16449719317f')\u0026quot; [ { \u0026quot;runner_args.creator\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;runner_args.flow_name\u0026quot;: \u0026quot;MonitoringFlow\u0026quot;, \u0026quot;runner_args.start_time\u0026quot;: 1544338661236625 }, { \u0026quot;runner_args.creator\u0026quot;: \u0026quot;mic\u0026quot;, \u0026quot;runner_args.flow_name\u0026quot;: \u0026quot;VFSDownloadFile\u0026quot;, \u0026quot;runner_args.start_time\u0026quot;: 1544087705756469 }, ...   Client Event Monitoring We have also previously seen that Velociraptor can collect event streams from clients. For example, the client's process execution logs can be streamed to the server. Clients can also receive event queries which forward selected events from the windows event logs.\nWhen we covered those features in earlier blog posts, we stressed that the Velociraptor server does not actually do anything with the client events, other than save them to a file. The server just writes the client's events in simple Comma Separated files (CSV files) on the server.\nWe mentioned that it is possible to import this file into another tool (e.g. a spreadsheet or database) for post-processing. An alternative is to perform post-processing with Velociraptor itself using server side VQL queries.\nFor example, we can filter a client's process execution log using a VQL query:\n$ velociraptor query \u0026quot;SELECT * from monitoring( client_id='C.87b19dba006fcddb', artifact='Windows.Events.ProcessCreation') WHERE Name =~ '(?i)psexesvc' \u0026quot; [ { \u0026quot;CommandLine\u0026quot;: \u0026quot;\\\u0026quot;C:\\\\\\\\Windows\\\\\\\\PSEXESVC.exe\\\u0026quot;\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;\\\u0026quot;PSEXESVC.exe\\\u0026quot;\u0026quot;, \u0026quot;PID\u0026quot;: \u0026quot;452\u0026quot;, \u0026quot;PPID\u0026quot;: \u0026quot;512\u0026quot;, \u0026quot;Timestamp\u0026quot;: \u0026quot;\\\u0026quot;2018-12-09T23:30:42-08:00\\\u0026quot;\u0026quot;, \u0026quot;artifact\u0026quot;: \u0026quot;Windows.Events.ProcessCreation\u0026quot;, \u0026quot;client_id\u0026quot;: \u0026quot;C.87b19dba006fcddb\u0026quot; } ]  The above query finds running instances of psexec's service component - a popular method of lateral movement and privilege escalation.\nThis query uses the monitoring() VQL plugin which opens each of the CSV event monitoring logs for the specified artifact on the server, decodes the CSV file and emits all the rows within it into the VQL Query. The rows are then filtered by applying the regular expression to the name.\n Server side event queries VQL queries do not have to terminate at all. Some VQL plugins can run indefinitely, emitting rows at random times - usually in response to some events. These are called Event Queries since they never terminate. We saw this property when monitoring the client - the above Windows.Events.ProcessCreation artifact uses an event query which emits a single row for each process execution on the end point.\nHowever, we can also have Event Queries on the server. When used in this way the query triggers in response to data collected by the server of various clients.\nFor example, consider the above query to detect instances of psexec executions. While we can detect this by filtering existing monitoring event logs, it would be nice to be able to respond to such an event dynamically.\nOne way is to repeatedly run the same query (say every minute) and look for newly reported instances of psexec executions. But this approach is not terribly efficient. A better approach is to install a watcher on the monitoring event log:\n$ velociraptor query \u0026quot;SELECT * from watch_monitoring( client_id='C.87b19dba006fcddb', artifact='Windows.Events.ProcessCreation') where Name =~ '(?i)psexesvc' \u0026quot; [ { \u0026quot;CommandLine\u0026quot;: \u0026quot;\\\u0026quot;C:\\\\\\\\Windows\\\\\\\\PSEXESVC.exe\\\u0026quot;\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;\\\u0026quot;PSEXESVC.exe\\\u0026quot;\u0026quot;, \u0026quot;PID\u0026quot;: \u0026quot;4592\u0026quot;, \u0026quot;PPID\u0026quot;: \u0026quot;512\u0026quot;, \u0026quot;Timestamp\u0026quot;: \u0026quot;\\\u0026quot;2018-12-10T01:18:06-08:00\\\u0026quot;\u0026quot;, \u0026quot;artifact\u0026quot;: \u0026quot;Windows.Events.ProcessCreation\u0026quot;, \u0026quot;client_id\u0026quot;: \u0026quot;C.87b19dba006fcddb\u0026quot; } ]  The watcher efficiently follows the monitoring CSV file to detect new events. These events are then emitted into the VQL query and subsequently filtered. When the query processes all rows in the file, the plugin just sleeps and waits for the file to grow again. The watch_monitoring() plugin essentially tails the CSV file as it is being written. Note that due to the fact that log files are never truncated and always grow, and that CSV file format is a simple, one row per line format it is possible to both read and write to the same file without locking. This makes following a growing log file extremely efficient and safe - even from another process.\n Responding to server side events The previous query will return a row when psexec is run on the client. This is a very suspicious event in our environment and we would like to escalate this by sending us an email.\nWe can modify the above query to send an email for each event:\nSELECT * FROM foreach( row={ SELECT * from watch_monitoring( client_id='C.87b19dba006fcddb', artifact='Windows.Events.ProcessCreation') WHERE Name =~ '(?i)psexesvc' }, query={ SELECT * FROM mail( to='admin\u0026#64;example.com', subject='PsExec launched on host', period=60, body=format(format='PsExec execution detected at %v: %v', args=[Timestamp, Commandline]) ) })  The query sends an email from each event emitted. The message body is formatted using the format() VQL function and this includes important information from the generated event. Note that the mail() plugin restricts the frequency of mails to prevent triggering the mail server's spam filters. So if two psexec executions occur within 60 seconds we will only get one email.\nIn order for Velociraptor to be able to send mail you must configure SMTP parameters in the server's configuration file. The following example uses gmail to send mails (other mail providers will have similar authentication requirements).\nMail: server: \u0026quot;smtp.gmail.com\u0026quot; auth_username: someuser\u0026#64;gmail.com auth_password: zldifhjsdflkjfsdlie  The password in the configuration is an application specific password obtained from https://security.google.com/settings/security/apppasswords\n Tying it all together: Server Side Event Artifacts As always we really want to encapsulate VQL queries in artifact definitions. This way we can design specific alerts, document them and invoke them by name. Let us encapsulate the above queries in a new artifact:\nname: Server.Alerts.PsExec description: | Send an email if execution of the psexec service was detected on any client. Note this requires that the Windows.Event.ProcessCreation monitoring artifact be collected. parameters: - name: EmailAddress default: admin\u0026#64;example.com - name: MessageTemplate default: | PsExec execution detected at %v: %v for client %v sources: - queries: - | SELECT * FROM foreach( row={ SELECT * from watch_monitoring( artifact='Windows.Events.ProcessCreation') WHERE Name =~ '(?i)psexesvc' }, query={ SELECT * FROM mail( to=EmailAddress, subject='PsExec launched on host', period=60, body=format( format=MessageTemplate, args=[Timestamp, CommandLine, ClientId]) ) })  We create a new directory called my_artifact_directory and store that file inside as psexesvc.yaml. Now, on the server we invoke the artifact collector and instruct it to also add our private artifacts:\n$ velociraptor --definitions my_artifact_directory/ \\  --config ~/server.config.yaml \\  --format json \\  artifacts collect Server.Alerts.PsExec INFO:2018/12/10 21:36:27 Loaded 40 built in artifacts INFO:2018/12/10 21:36:27 Loading artifacts my_artifact_directory/ [][ { \u0026quot;To\u0026quot;: [ \u0026quot;admin\u0026#64;example.com\u0026quot; ], \u0026quot;CC\u0026quot;: null, \u0026quot;Subject\u0026quot;: \u0026quot;PsExec launched on host\u0026quot;, \u0026quot;Body\u0026quot;: \u0026quot;PsExec execution detected at \\\u0026quot;2018-12-10T03:36:49-08:00\\\u0026quot;: \\\u0026quot;C:\\\\\\\\Windows\\\\\\\\PSEXESVC.exe\\\u0026quot;\u0026quot;, \u0026quot;Period\u0026quot;: 60 } ]   Conclusions This blog post demonstrates how VQL can be used on the server to create a full featured incident response framework. Velociraptor does not dictate a particular workflow, since all its actions are governed by VQL queries and artifacts. Using the same basic building blocks, users can fashion their own highly customized incident response workflow. Here is a brainstorm of possible actions:\nAn artifact can be written to automatically collect a memory capture if a certain event is detected. Using the http_client() VQL plugin, when certain events are detected on the server open a ticket automatically (using a SOAP or JSON API). If a particular event is detected, immediately shut the machine down or quarantine it (by running shell commands on the compromised host).  The possibilities are truly endless. Comment below if you have more interesting ideas and do not hesitate to contribute artifact definitions to address your real world use cases.\n "
},
{
        "uri": "/blog/html/2018/12/09/more_on_client_event_collection.html",
        "title": "More on client event collection",
        "tags": [],
        "description": "Previously we have seen that Velociraptor can monitor client events\nusing Event Artifacts. To recap, Event Artifacts are simply artifacts\nwhich contain event VQL queries. Velociraptor's VQL queries do not\nhave to terminate by themselves - instead VQL queries may run\nindefinitely, trickling results over time.\n\nThis post takes another look at event queries and demonstrates how\nthese can be used to implement some interesting features.\n",
        "content": "Periodic Event queries The simplest kind of events are periodically generated events. These are created using the clock() VQL plugin. This is a simple event plugin which just emits a new row periodically.\n$ velociraptor query \u0026quot;select Unix from clock(period=5)\u0026quot; --max_wait 1 [ { \u0026quot;Unix\u0026quot;: 1544339715 } ][ { \u0026quot;Unix\u0026quot;: 1544339720 } ]^C  The query will never terminate, instead the clock() plugin will emit a new timestamp every 5 seconds. Note the --max_wait flag which tells Velociraptor to wait at least for 1 second in order to batch rows before reporting them.\nThis query is not very interesting! Let's do something more interesting. GRR has a feature where each client sends its own CPU use and memory footprint sampled every minutes to the server. This is a really useful feature because it can be used to make sure the client's impact on the host's performance is minimal.\nLet us implement the same feature with a VQL query. What we want is to measure the client's footprint every minute and send that to the server:\nSELECT * from foreach( row={ SELECT UnixNano FROM clock(period=60) }, query={ SELECT UnixNano / 1000000000 as Timestamp, Times.user + Times.system as CPU, MemoryInfo.RSS as RSS FROM pslist(pid=getpid()) })  This query runs the clock() VQL plugin and for each row it emits, we run the pslist() plugin, extracting the total CPU time (system + user) used by our own pid (i.e. the Velociraptor client).\nWe can now encapsulate this query in an artifact and collect it:\n$ velociraptor artifacts collect Generic.Client.Stats --max_wait 1 --format json [][ { \u0026quot;CPU\u0026quot;: 0.06999999999999999, \u0026quot;RSS\u0026quot;: 18866176, \u0026quot;Timestamp\u0026quot;: 1544340582.9939497 } ][ { \u0026quot;CPU\u0026quot;: 0.09, \u0026quot;RSS\u0026quot;: 18866176, \u0026quot;Timestamp\u0026quot;: 1544340602.9944408 } ]^C  Note\nYou must specify the --format json to be able to see the results from event queries on the command line. Otherwise Velociraptor will try to get all the results so it can format them in a table and never return any results.\n  Installing the event collector. In order to have clients collect this event, we need to add the artifact to the server. Simply add the YAML file into a directory on the server and start the server with the --definitions flag. Then simply add the event name to the Events clause of the server configuration. When clients connect to the server they will automatically start collecting these events and sending them to the server:\n$ velociraptor --definitions path/to/my/artifacts/ frontend  Events: artifacts: - Generic.Client.Stats version: 2  Note that we do not need to redeploy any clients, modify any code or recompile anything. We simply add the new artifact definition and clients will automatically start monitoring and feeding back our information.\nThe data is sent to the server where it is stored in a file (Events are stored in a unique file for each day).\nFor example, the path /var/lib/velociraptor/clients/C.772d16449719317f/monitoring/Artifact%20Generic.Client.Stats/2018-12-10 stores all events collected from client id C.772d16449719317f for the Generic.Client.Stats artifact on the day of 2018-12-10.\nIn the next blog post we will demonstrate how these events can be post processed and acted on. It is important to note that the Velociraptor server does not interpret the collected monitoring events at all - they are simply appended to the daily log file (which is a CSV file).\nThe CSV file can then be imported into basically any tool designed to work with tabular data (e.g. spreadsheets, databases, BigQuery etc). CSV is almost universally supported by all major systems.\nTimestamp,CPU,RSS 1544363561.8001275,14.91,18284544 1544363571.8002906,14.91,18284544 1544363581.8004665,14.920000000000002,18284544 1544363591.8007126,14.920000000000002,18284544 1544363601.8008528,14.920000000000002,18284544   "
},
{
        "uri": "/blog/html/2018/11/13/velociraptor_training_at_nzitf.html",
        "title": "Velociraptor training at NZITF",
        "tags": [],
        "description": "We are very excited to run this full day training workshop at the New\nZealand Internet Engineering Task Force (NZITF) conference.\n",
        "content": "Velociraptor training at NZITF We are very excited to run this full day training workshop at the New Zealand Internet Engineering Task Force (NZITF) conference.\nThe training material can be downloaded here \u0026quot;Velociraptor NZITF training\u0026quot;.\n"
},
{
        "uri": "/blog/html/2018/11/09/event_queries_and_endpoint_monitoring.html",
        "title": "Event Queries and Endpoint Monitoring",
        "tags": [],
        "description": "In previous posts we have seen how Velociraptor can run artifacts to\ncollect information from hosts. For example, we can collect WMI\nqueries, user accounts and files.\n\nHowever it would be super awesome to be able to do this collection in\nreal time, as soon as an event of interest appears on the host, we\nwould like to have that collected on the server. This post describes\nthe new event monitoring framework and shows how Velociraptor can\ncollect things such as event logs, process execution and more in real\ntime.\n",
        "content": "Why monitor endpoint events? Recording end point event information on the server gives a bunch of advantages. For one, the server keeps a record of historical events, which makes going back to search for these easy as part of an incident response activity.\nFor example, Velociraptor can keep a running log of process execution events for all clients, on the server. If a particular executable is suspected to be malicious, we can now go back and search for the execution of that process in the past on the infected machine (for establishing the time of infection), as well as search the entire deployment base for the same binary execution to be able identify lateral movement and wider compromises.\nHow are events monitored? Velociraptor relies heavily on VQL queries. A VQL query typically produces a single table of multiple rows. For example, the query:\nSELECT Name, CommandLine FROM pslist()  Returns a single row of all running processes, and then returns.\nHowever, VQL queries do not have to terminate at all. If the VQL plugin they are calling does not terminate, the VQL query will continue to run and pass events in partial results to the VQL caller.\nEvent queries are just regular VQL queries which do not terminate (unless cancelled) returning rows whenever an event is generated.\nConsider the parse_evtx() plugin. This plugin parses an event log file and returns all events in it. We can then filter events and return specific events of interest. The following query returns all the service installation events and terminates:\nF:\\\u0026gt;velociraptor.exe query \u0026quot;SELECT EventData, System.TimeCreated.SystemTime from parse_evtx(filename='c:/windows/system32/winevt/logs/system.evtx') where System.EventId.value = '7045'\u0026quot; [ { \u0026quot;EventData\u0026quot;: { \u0026quot;AccountName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;ImagePath\u0026quot;: \u0026quot;system32\\\\DRIVERS\\\\VBoxGuest.sys\u0026quot;, \u0026quot;ServiceName\u0026quot;: \u0026quot;VirtualBox Guest Driver\u0026quot;, \u0026quot;ServiceType\u0026quot;: \u0026quot;kernel mode driver\u0026quot;, \u0026quot;StartType\u0026quot;: \u0026quot;boot start\u0026quot; }, \u0026quot;System.TimeCreated.SystemTime\u0026quot;: \u0026quot;2018-11-10T06:32:34Z\u0026quot; } ]  The query specifically looks at the 7045 event \u0026quot;A service was installed in the system\u0026quot;\nLets turn this query into an event query:\nF:\\\u0026gt;velociraptor.exe query \u0026quot;SELECT EventData, System.TimeCreated.SystemTime from watch_evtx(filename='c:/windows/system32/winevt/logs/system.evtx') where System.EventId.value = '7045'\u0026quot; --max_wait 1 [ \u0026quot;EventData\u0026quot;: { \u0026quot;AccountName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;ImagePath\u0026quot;: \u0026quot;C:\\\\Users\\\\test\\\\AppData\\\\Local\\\\Temp\\\\pmeFF0E.tmp\u0026quot;, \u0026quot;ServiceName\u0026quot;: \u0026quot;pmem\u0026quot;, \u0026quot;ServiceType\u0026quot;: \u0026quot;kernel mode driver\u0026quot;, \u0026quot;StartType\u0026quot;: \u0026quot;demand start\u0026quot; }, \u0026quot;System.TimeCreated.SystemTime\u0026quot;: \u0026quot;2018-11-10T04:57:35Z\u0026quot; } ]  The watch_evtx() plugin is the event watcher equivalent of the parse_evtx() plugin. If you ran the above query, you will notice that Velociraptor does not terminate. Instead it will show all existing service installation events in the log file, and then just wait in the console.\nIf you then install a new service (in another terminal), for example using winpmem.exe -L, a short time later you should see the event reported by Velociraptor as in the above example. You will notice that the watch_evtx() plugin emits event logs as they occur, but Velociraptor will try to group the events into batches. The max_wait flag controls how long to wait before releasing a partial result set.\n Employing event queries for client monitoring The above illustrates how event queries work, but to actually be able to use these we had to implement the Velociraptor event monitoring framework.\nNormally, when we launch a CollectVQL flow, the client executes the query and returns the result to the flow. Clearly since event queries never terminate, we can not run them in series (because the client will never be able to do anything else). The Velociraptor client has a table of executing event queries which are run in a separate thread. As these queries return more results, the results are sent back to the server.\nWe also wanted to be able to update the events the clients are monitoring on the fly (without a client restart). Therefore we needed a way to be able to update the client's event table. This simply cancels current event queries, and installs new queries in their place.\nAs events are generated by the Event Table, they are sent back to the server into the Monitoring flow. This flow is automatically created for each client. The monitoring flow simply writes events into the client's VFS. Therefore, events are currently simply recorded for each client. In future there will be a mechanism to post process event and produce alerts based on these.\n Process Execution logs One of the most interesting event plugins is the WMI eventing plugin. This allows Velociraptor to install a temporary WMI event listener. For example, we can install a listener for new process creation:\n// Convert the timestamp from WinFileTime to Epoch. SELECT timestamp(epoch=atoi( string=Parse.TIME_CREATED) / 10000000 - 11644473600 ) as Timestamp, Parse.ParentProcessID as PPID, Parse.ProcessID as PID, Parse.ProcessName as Name, { SELECT CommandLine FROM wmi( query=\u0026quot;SELECT * FROM Win32_Process WHERE ProcessID = \u0026quot; + format(format=\u0026quot;%v\u0026quot;, args=Parse.ProcessID), namespace=\u0026quot;ROOT/CIMV2\u0026quot;) } AS CommandLine FROM wmi_events( query=\u0026quot;SELECT * FROM __InstanceCreationEvent WITHIN 1 WHERE TargetInstance ISA 'Win32_Process'\u0026quot;, wait=5000000, // Do not time out. namespace=\u0026quot;ROOT/CIMV2\u0026quot;)  The wmi_events() plugin installs an event listener into WMI and therefore receives events from the OS about new process creation events. Unfortunately these events, do not contain a lot of information about the process. They only provide the ProcessID but not the full command line. The above query executes a second subquery to retrieve the command line for the process. We also parse the timestamp and convert it into a more standard epoch based timestamp.\n Specifying what should the client monitor We have seen how Event VQL queries can generate events for the server. However, this is difficult for Velociraptor's end users to directly use. Who can really remember the full query?\nAs we have shown previously, Velociraptor's Artifacts are specifically designed to solve this issue. Artifacts encapsulate a VQL query so it can be called by name alone.\nFor example, the Windows.Events.ProcessCreation artifact encapsulates the above query in one easy to remember name.\nTo specify what clients should collect, users simply need to name the event artifacts that should be monitored. Currently this is done in the server configuration (in future this may be done via the GUI).\nEvents: artifacts: - Windows.Events.ServiceCreation - Windows.Events.ProcessCreation version: 1  The event table version should be incremented each time the monitored event list is updated. This forces all clients to refresh their event tables.\n How does it look like in the GUI? The Monitoring flow simply writes files into the client's VFS. This allows these to be downloaded and post processed outside of Velociraptor.\n Conclusions Adding event monitoring to Velociraptor is a great step forward. Even just keeping the logs around is extremely helpful for incident response. There is a lot of value in things like process execution logging, and remote event log forwarding. We will cover some more examples of event log monitoring in future blog posts. Until then, have a play and provide feedback as usual by filing issues and feature requests.\n "
},
{
        "uri": "/blog/html/2018/09/30/velorciraptor_s_filesystem_s_accessors.html",
        "title": "Velorciraptor's filesystem's accessors",
        "tags": [],
        "description": "The latest release of Velociraptor introduces the ability to access\nraw NTFS volumes, allowing users to read files which are normally\nlocked by the operating system such as registry hives, pagefile and\nother locked files.\n",
        "content": "In addition, Velociraptor can now also read Volume Shadow Copy snapshots. The gives a kind of time-machine ability to allow the investigator to look through the drive content at a previous point in the past.\nThis blog post introduces the new features and describe how Velociraptor's filesystem accessors work to provide data from multiple sources to VQL queries.\nWe have previously seen that Velociraptor can list and download files from the client's filesystem, as well as registry keys and values. The client's filesystem is made available to VQL plugins such as glob() allowing many Artifacts to be written that work on files, registry keys and raw NTFS volumes.\nWhile Velociraptor is a great remote response tool, everything that it can do remotely, it can also do locally using a command line interface. This gives the user an opportunity to interactively test their VQL queries while writing artifacts.\nThe latest release adds a couple of convenient command line options which allow the user to interact with the filesystem accessors. For example, to list the files in a directory we can use the \u0026quot;velociraptor fs ls\u0026quot; command:\nF:\\\u0026gt;velociraptor.exe fs ls +------+------+------------+---------------------------+---------------------------------+ | Name | Size | Mode | mtime | Data | +------+------+------------+---------------------------+---------------------------------+ | C: | 0 | d--------- | 1969-12-31T16:00:00-08:00 | Description: Local Fixed Disk | | | | | | DeviceID: C: | | | | | | FreeSpace: 12686422016 | | | | | | Size: 33833349120 | | | | | | SystemName: DESKTOP-IOME2K5 | | | | | | VolumeName: | | | | | | VolumeSerialNumber: 9459F443 | | D: | 0 | d--------- | 1969-12-31T16:00:00-08:00 | Description: CD-ROM Disc | | | | | | DeviceID: D: | | | | | | FreeSpace: 0 | | | | | | Size: 57970688 | | | | | | SystemName: DESKTOP-IOME2K5 | | | | | | VolumeName: VBox_GAs_5.2.11 | | | | | | VolumeSerialNumber: A993F576 | +------+------+------------+---------------------------+---------------------------------+ SELECT Name, Size, Mode.String AS Mode, timestamp(epoch=Mtime.Sec) AS mtime, Data FROM glob(globs=path, accessor=accessor)  The \u0026quot;fs ls\u0026quot; command instructs Velociraptor to list directories using its internal filesystem accessors. By default it will use the \u0026quot;file\u0026quot; accessor - which simply uses the usual Win32 api filesystem calls (i.e. CreateFile, FindFirstFile etc).\nOn windows, the file accessor lists the drive letters at the root of the filesystem, then allows subdirectories to be listed under each letter. The above output shows some metadata for each drive letter (like its size etc) and below the table we can see the VQL query that was used to generate the table. To be clear, the \u0026quot;fs ls\u0026quot; command is simply a shortcut for producing a VQL query that ultimately uses the filesystem accessor in the glob() VQL plugin. Therefore, we can enter any glob expression to find files:\nF:\\\u0026gt;velociraptor.exe fs ls -v \u0026quot;c:\\program files\\**\\*.exe\u0026quot; +--------------------------------+----------+------------+---------------------------+------+ | FullPath | Size | Mode | mtime | Data | +--------------------------------+----------+------------+---------------------------+------+ | C:\\Program Files\\Windows Defen | 4737448 | -rw-rw-rw- | 2018-07-14T17:56:49-07:00 | | | der Advanced Threat Protection | | | | | | \\MsSense.exe | | | | | | C:\\Program Files\\Windows Defen | 791384 | -rw-rw-rw- | 2018-07-14T17:56:43-07:00 | | | der Advanced Threat Protection | | | | | | \\SenseCncProxy.exe | | | | | | C:\\Program Files\\Windows Defen | 3832016 | -rw-rw-rw- | 2018-07-14T17:56:50-07:00 | | | der Advanced Threat Protection | | | | | | \\SenseIR.exe | | | | | | C:\\Program Files\\Windows Defen | 2147192 | -rw-rw-rw- | 2018-07-14T18:05:00-07:00 | | | der Advanced Threat Protection | | | | | | \\SenseSampleUploader.exe | | | | | ........ +--------------------------------+----------+------------+---------------------------+------+ SELECT FullPath, Size, Mode.String AS Mode, timestamp(epoch=Mtime.Sec) AS mtime, Data FROM glob(globs=path, accessor=accessor)  When using the registry filesystem accessor, the registry appears like a filesystem, allowing us to run glob expressions against registry keys and values (Note that the registry accessor provides the value in the metadata):\nF:\\\u0026gt;velociraptor.exe fs --accessor reg ls \u0026quot;HKEY_USERS\\*\\Software\\Microsoft\\Windows\\CurrentVersion\\{Run,RunOnce}\\*\u0026quot; +---------------+------+------------+---------------------------+---------------------------------+ | Name | Size | Mode | mtime | Data | +---------------+------+------------+---------------------------+---------------------------------+ | OneDriveSetup | 104 | -rwxr-xr-x | 2018-09-03T02:48:53-07:00 | type: SZ | | | | | | value: C:\\Windows\\SysWOW64\\ | | | | | | OneDriveSetup.exe /thfirstsetup | | OneDriveSetup | 104 | -rwxr-xr-x | 2018-09-03T02:48:47-07:00 | type: SZ | | | | | | value: C:\\Windows\\SysWOW64\\ | | | | | | OneDriveSetup.exe /thfirstsetup | +---------------+------+------------+---------------------------+---------------------------------+ SELECT Name, Size, Mode.String AS Mode, timestamp(epoch=Mtime.Sec) AS mtime, Data FROM glob(globs=path, accessor=accessor)  Finally, the NTFS accessor can access files by parsing the NTFS filesystem directly. At the top level, the accessor shows all NTFS formatted partitions. These include regular drives as well as Volume Shadow Copies:\nF:\\\u0026gt;velociraptor.exe fs --accessor ntfs ls +--------------------------------+------+------------+---------------------------------------------------------+ | Name | Size | Mode | Data | +--------------------------------+------+------------+---------------------------------------------------------+ | \\\\.\\C: | 0 | d--------- | Description: Local Fixed Disk | | | | | DeviceID: C: | | | | | FreeSpace: 11802157056 | | | | | Size: 33833349120 | | | | | SystemName: DESKTOP-IOME2K5 | | | | | VolumeName: | | | | | VolumeSerialNumber: 9459F443 | | \\\\?\\GLOBALROOT\\Device\\Harddisk | 0 | d--------- | DeviceObject: \\\\?\\GLOBALROOT\\Device\\ | | | | | HarddiskVolumeShadowCopy1 | | VolumeShadowCopy1 | | | ID: {CAF25144-8B70-4F9E-B4A9-5CC702281FA1} | | | | | InstallDate: 20180926154712.490617-420 | | | | | OriginatingMachine: DESKTOP-IOME2K5 | | | | | VolumeName: \\\\?\\Volume{3dc4b590-0000-000-501f00000000}\\ | | \\\\?\\GLOBALROOT\\Device\\Harddisk | 0 | d--------- | DeviceObject: \\\\?\\GLOBALROOT\\Device\\ | | | | | HarddiskVolumeShadowCopy2 | | VolumeShadowCopy2 | | | ID: {E48BFDD7-7D1D-40AE-918C-36FCBB009941} | | | | | InstallDate: 20180927174025.893104-420 | | | | | OriginatingMachine: DESKTOP-IOME2K5 | | | | | VolumeName: \\\\?\\Volume{3dc4b590-0000-000-501f00000000}\\ | +--------------------------------+------+------------+---------------------------------------------------------+ SELECT Name, Size, Mode.String AS Mode, timestamp(epoch=Mtime.Sec) AS mtime,, Data FROM glob(globs=path, accessor=accessor) WHERE Sys.name_type != 'DOS'  The above example shows two volume shadow copies that Windows has takens on two different dates (highlighted above). We can browse these snapshots just like they were another drive (We can also apply any glob expressions to this path):\nF:\\\u0026gt;velociraptor.exe fs --accessor ntfs ls \u0026quot;\\\\?\\GLOBALROOT\\Device\\HarddiskVolumeShadowCopy1\\ Users\\test\\*.exe\u0026quot; +------------------+----------+------------+---------------------------+------------------+ | Name | Size | Mode | mtime | Data | +------------------+----------+------------+---------------------------+------------------+ | velociraptor.exe | 12521472 | -rwxr-xr-x | 2018-08-19T23:37:01-07:00 | mft: 39504-128-0 | | | | | | name_type: Win32 | | winpmem.exe | 3619260 | -rwxr-xr-x | 2017-12-28T21:17:50-08:00 | mft: 39063-128-1 | | | | | | name_type: POSIX | +------------------+----------+------------+---------------------------+------------------+ SELECT Name, Size, Mode.String AS Mode, timestamp(epoch=Mtime.Sec) AS mtime, Data FROM glob(globs=path, accessor=accessor) WHERE Sys.name_type != 'DOS'  Volume shadow copies are like a time machine - they can reveal data that was stored on the drive days or weeks prior to the time we inspected it which makes them very useful for some investigations.\nUsing filesystem accessors remotely - The Velociraptor VFS The above description shows how Velociraptor's command line interface can be used to interact with the various filesystem accessors. This is important for writing and collecting artifacts for triage and general system state exploration.\nHowever, how do filesystem accessors appear in the Velociraptor GUI?\nThe nice thing about Velociraptor's GUI is that it is just a way to present the same information that the \u0026quot;fs ls\u0026quot; command is getting by using the same VQL queries. Therefore the view is very familiar:\nThe top level of the Velociraptor VFS represents all the filesystem accessors implemented in the client. Each of these accessors shows its own view:The file accessor uses the OS APIs to list files and directories. Its top level is a list of mounted drives (which may be CDROM's or even network shares). The NTFS accessor shows all NTFS volumes accessible, including local drives and Volume Shadow Copies. The registry accessor uses Win32 APIs to access the registry and shows at the top level a list of all system hives currently attached.   For each file listed, the accessor also includes a Data attribute. This contains accessor specific metadata about the file (for example the MFT entry).  In the below screenshot we can see how the user may navigate into the Volume Shadow Copy and retrieve files from it:\n A note about filenames. NTFS can have several different names to the same file. Typically, a short DOS 8.3 style filename (e.g. PROGRA~1), as well as a Win32 long filename (e.g. Program Files). You can see the short name for a file using the API GetShortPathName() (or the command dir /x), but a program needs to deliberately ask for it. Most programs do not explicitly collect or show the short filename of a file.\nThis can cause problems for DFIR applications. For example, Imagine we discovered a Run key to C:\\Users\\test\\runme.exe. If we only considered the long filename (as for example returned by the Win32API FindFile() or the output of the dir command), then we would assume the file has been removed and the run key is not active. In reality however, the file may be called \u0026quot;This is some long filename.exe\u0026quot; with a DOS name of \u0026quot;runme.exe\u0026quot;. Explorer (and most tools) will only show the long filename by default, but the runkey will still execute by referring to the DOS filename!\nUsually the short filename is some variation of the long filename with a ~1 or ~2 at the end. In reality it can be anything. In the snippet below, I am setting the short filename for the velociraptor.exe binary to be something completely unrelated, then I am running the binary using the unrelated filename:\nC:\\Users\\test\u0026gt;fsutil file setshortname velociraptor.exe runme.exe C:\\Users\\test\u0026gt;dir /x *.exe Volume in drive C has no label. Volume Serial Number is 9459-F443 Directory of C:\\Users\\test 08/19/2018 11:37 PM 12,521,472 RUNME.EXE velociraptor.exe 2 File(s) 16,140,732 bytes 0 Dir(s) 11,783,704,576 bytes free C:\\Users\\test\u0026gt;runme.exe -h usage: velociraptor [\u0026lt;flags\u0026gt;] \u0026lt;command\u0026gt; [\u0026lt;args\u0026gt; ...] An advanced incident response and monitoring agent.  You can see that Windows explorer shows no trace of the runme.exe file since it only displays the Win32 long file name:\nIt is important for DFIR investigators to be aware of this and test your tools! You can see that sysinternals' autoruns program won't have any of these shenanigans when I added a runkey to \u0026quot;runme.exe\u0026quot;. It shows the real filename velociraptor.exe even though the runkey indicates runme.exe:\nVelocirpator treats a file's DOS name and Win32 Name as distinct entries in the NTFS directory listing. This allows us to find any references to the file by it's DOS name as well as its Win32 name.\n Conclusions As Velociraptor gains more functionality, we envision more filesystem accessors to become available. The nice thing about these accessors is that they just slot in to the rest of the VQL plugins. By providing a new accessor, we are able to glob, hash, yara scan etc the new abstraction. For example, to yara scan a registry key one simply calls the VQL plugin yara with an accessor of reg: yara(rules=myRules, files=my_reg_keys, accessor=\u0026quot;reg\u0026quot;)\n "
},
{
        "uri": "/blog/html/2018/09/29/detecting_powershell_persistence_with_velociraptor_and_yara.html",
        "title": "Detecting powershell persistence with Velociraptor and Yara",
        "tags": [],
        "description": "I was watching the SANS DFIR Summit 2018 videos on youtube and came\nacross Mari DeGrazia's talk titled \"Finding and Decoding Malicious\nPowershell Scripts\"\n",
        "content": "Update\nAs of the latest release of Velociraptor we have raw registry parsing which can also be done via raw NTFS to get around file locking. It is no longer necessary to run yara scans and parse with regripper as shown here.\n I was watching the SANS DFIR Summit 2018 videos on youtube and came across Mari DeGrazia's talk titled \u0026quot;Finding and Decoding Malicious Powershell Scripts\u0026quot;. This is an excellent talk and it really contains a wealth of information. It seems that Powershell is really popular these days, allowing attacker to \u0026quot;live off the land\u0026quot; by installing fully functional reverse shells and backdoors, in a few lines of obfuscated scripts.\nMari went through a number of examples and also expanded on some in her blog post Malicious PowerShell in the Registry: Persistence, where she documents persistence through an autorun key launching powershell to execute a payload within another registry key.\nA similar persistence mechanism is documented by David Kennedy from Binary defence in his post PowerShell Injection with Fileless Payload Persistence and Bypass Techniques. In that case an msha.exe link was stored in the user's Run key which executed a payload from another registry key.\nI was eager to write a Velociraptor artifact to attempt to detect such keys using a YARA signature. Of course signature based detection is not as robust as behavioural analysis but it is quick and usually quite effective.\nI thought it was still quite instructive to document how one can develop the VQL queries for a simple Velociraptor artifact. We will be developing the artifact interactively on a Windows system.\nPreparation Our artifact will attempt to detect the persistence mechanism detailed in the above posts. We start by adding a value to our test user account under the key\nKey: \u0026quot;HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\u0026quot; Value: \u0026quot;C:\\Windows\\system32\\mshta.exe\u0026quot; Data: about:\u0026lt;script\u0026gt;c1hop=\u0026quot;X642N10\u0026quot;;R3I=new%20ActiveXObject(\u0026quot;WScript.Shell\u0026quot;); QR3iroUf=\u0026quot;I7pL7\u0026quot;;k9To7P=R3I.RegRead(\u0026quot;HKCU\\\\software\\\\bkzlq\\\\zsdnhepyzs\u0026quot;); J7UuF1n=\u0026quot;Q2LnLxas\u0026quot;;eval(k9To7P);JUe5wz3O=\u0026quot;zSfmLod\u0026quot;;\u0026lt;/script\u0026gt;   Defining the Artifact. We create a directory called \u0026quot;artifacts\u0026quot; then create a new file inside it called powershell_persistence.yaml. Velociraptor artifacts are just YAML files that can be loaded at runtime using the --definitions flag.\nEvery artifact has a name, by convention the name is separated into its major categories. We will call ours Windows.Persistence.Powershell:\nname: Windows.Persistence.Powershell  This is the minimum required for Velociraptor to identify it. We can see a listing of all artifacts Velociraptor knows about using the \u0026quot;artifacts list\u0026quot; command:\nF:\\\u0026gt;velociraptor.exe --definitions artifacts artifacts list INFO:2018/09/28 07:59:40 Loaded 34 built in artifacts Linux.Applications.Chrome.Extensions Linux.Applications.Chrome.Extensions.Upload … Windows.Persistence.Powershell ... Windows.Sys.Users  We can collect the artifact simply by using the \u0026quot;artifacts collect\u0026quot; command:\nF:\\\u0026gt;velociraptor.exe --definitions artifacts artifacts collect Windows.Persistence.Powershell INFO:2018/09/28 20:01:32 Loaded 34 built in artifacts  Ok so Velociraptor can load and collect this new artifact, but as yet it does nothing! We need to think about what exactly we want to collect.\nWe know we want to search for all values in the Run/RunOnce hive of all the users. Let's first see if we can retrieve all the values using a glob:\nname: Windows.Persistence.Powershell parameters: - name: keyGlob default: \u0026quot;HKEY_USERS\\\\*\\\\Software\\\\Microsoft\\\\Windows\\  \\\\CurrentVersion\\\\{Run,RunOnce}\\\\*\u0026quot; sources: - precondition: SELECT OS from info() where OS = \u0026quot;windows\u0026quot; queries: - | SELECT FullPath from glob( globs=keyGlob, accessor=\u0026quot;reg\u0026quot; )  This artifact demonstrates a few concepts:\nWe can define parameters by name, and reference them from within the VQL query. This keeps the VQL query clean and more readable. We can define a precondition on the artifact. If the precondition is not met, the VQL query will not be run.  Lets run this artifact:\nF:\\\u0026gt;velociraptor.exe --definitions artifacts artifacts collect Windows.Persistence.Powershell INFO:2018/09/28 20:51:47 Loaded 34 built in artifacts +--------------------------------+ | FullPath | +--------------------------------+ | HKEY_USERS\\S-1-5-19\\Software\\M | | icrosoft\\Windows\\CurrentVersio | | n\\Run\\OneDriveSetup | | HKEY_USERS\\S-1-5-20\\Software\\M | | icrosoft\\Windows\\CurrentVersio | | n\\Run\\OneDriveSetup | | HKEY_USERS\\S-1-5-21-546003962- | | 2713609280-610790815-1001\\Soft | | ware\\Microsoft\\Windows\\Current | | Version\\Run\\\u0026quot;C:\\Windows\\system | | 32\\mshta.exe\u0026quot; | +--------------------------------+ Artifact: Windows.Persistence.Powershell  It returns a couple of results so there are two Run/RunOnce values defined. For this artifact, we only want to return those entries which match a specific yara signature. We can work later on improving the yara signature, but for now let's just detect uses of the eval() powershell command within 500 characters of an ActiveXObject instantiation. We will try to match each value returned from the Run keys with this object:\nname: Windows.Persistence.Powershell parameters: - name: keyGlob default: \u0026quot;HKEY_USERS\\\\*\\\\Software\\\\Microsoft\\\\Windows\\  \\\\CurrentVersion\\\\{Run,RunOnce}\\\\*\u0026quot; - name: yaraRule default: | rule Powershell { strings: $ = /ActiveXObject.{,500}eval/ nocase $ = /ActiveXObject.{,500}eval/ wide nocase condition: any of them } sources: - precondition: SELECT OS from info() where OS = \u0026quot;windows\u0026quot; queries: - | // This is a stored query LET file = SELECT FullPath from glob( globs=keyGlob, accessor=\u0026quot;reg\u0026quot; ) - | SELECT * FROM yara( rules=yaraRule, files=file.FullPath, // This will expand to a list of paths. accessor=\u0026quot;reg\u0026quot;)  This version recovers the FullPath of all the Run/RunOnce values and stores them in a stored query. We then issue another query that applies the yara rule on these values:\nF:\\\u0026gt;velociraptor.exe --definitions artifacts artifacts collect Windows.Persistence.Powershell INFO:2018/09/28 21:29:10 Loaded 34 built in artifacts +------------+------+------+--------------------------------+--------------------------------+ | Rule | Meta | Tags | Strings | File | +------------+------+------+--------------------------------+--------------------------------+ | Powershell | | | {\u0026quot;Name\u0026quot;:\u0026quot;$\u0026quot;,\u0026quot;Offset\u0026quot;:40,\u0026quot;HexDa | {\u0026quot;FullPath\u0026quot;:\u0026quot;HKEY_USERS\\\\S-1-5 | | | | | ta\u0026quot;:[\u0026quot;00000000 41 63 74 69 76 | -21-546003962-2713609280-61079 | | | | | 65 58 4f 62 6a 65 63 74 28 2 | 0815-1001\\\\Software\\\\Microsoft | | | | | 2 57 |ActiveXObject(\\\u0026quot;W|\u0026quot;,\u0026quot;00 | \\\\Windows\\\\CurrentVersion\\\\Run | | | | | 000010 53 63 72 69 70 74 2e 5 | \\\\\\\u0026quot;C:\\\\Windows\\\\system32\\\\msh | | | | | 3 68 65 6c 6c 22 29 3b 51 |S | ta.exe\\\u0026quot;\u0026quot;,\u0026quot;Type\u0026quot;:\u0026quot;SZ\u0026quot;,\u0026quot;Data\u0026quot;:{ | | | | | cript.Shell\\\u0026quot;);Q|\u0026quot;,\u0026quot;00000020 | \u0026quot;type\u0026quot;:\u0026quot;SZ\u0026quot;,\u0026quot;value\u0026quot;:\u0026quot;about:\\u0 | | | | | 52 33 69 72 6f 55 66 3d 22 49 | 03cscript\\u003ec1hop=\\\u0026quot;X642N10 | | | | | 37 70 4c 37 22 3b |R3iroUf=\\ | \\\u0026quot;;R3I=new%20ActiveXObject(\\\u0026quot;W | | | | | \u0026quot;I7pL7\\\u0026quot;;|\u0026quot;,\u0026quot;00000030 6b 39 5 | Script.Shell\\\u0026quot;);QR3iroUf=\\\u0026quot;I7p | | | | | 4 6f 37 50 3d 52 33 49 2e 52 | L7\\\u0026quot;;k9To7P=R3I.RegRead(\\\u0026quot;HKCU | | | | | 65 67 52 65 |k9To7P=R3I.RegRe | \\\\\\\\software\\\\\\\\bkzlq\\\\\\\\zsdnh | | | | | |\u0026quot;,\u0026quot;00000040 61 64 28 22 48 4 | epyzs\\\u0026quot;);J7UuF1n=\\\u0026quot;Q2LnLxas\\\u0026quot;; | | | | | b 43 55 5c 5c 73 6f 66 74 77 | eval(k9To7P);JUe5wz3O=\\\u0026quot;zSfmLo | | | | | 61 |ad(\\\u0026quot;HKCU\\\\\\\\softwa|\u0026quot;,\u0026quot;00 | d\\\u0026quot;;\\u003c/script\\u003e\u0026quot;},\u0026quot;Mti | | | | | 000050 72 65 5c 5c 62 6b 7a 6 | me\u0026quot;:{\u0026quot;sec\u0026quot;:1538191253,\u0026quot;usec\u0026quot;:1 | | | | | c 71 5c 5c 7a 73 64 6e 68 |r | 538191253231489700},\u0026quot;Ctime\u0026quot;:{\u0026quot; | | | | | e\\\\\\\\bkzlq\\\\\\\\zsdnh|\u0026quot;,\u0026quot;0000006 | sec\u0026quot;:1538191253,\u0026quot;usec\u0026quot;:1538191 | | | | | 0 65 70 79 7a 73 22 29 3b 4a | 253231489700},\u0026quot;Atime\u0026quot;:{\u0026quot;sec\u0026quot;:1 | | | | | 37 55 75 46 31 6e 3d |epyzs\\ | 538191253,\u0026quot;usec\u0026quot;:1538191253231 | | | | | \u0026quot;);J7UuF1n=|\u0026quot;,\u0026quot;00000070 22 51 | 489700}} |  We can see that the last query returns 5 columns, but each column actually contains objects with quite a lot of additional information. For example, the File column returns information about the file that matched the yara rule (its filename, timestamps etc). The output is a bit confusing so we just return the relevant columns. We can replace the * in the last query with a curated list of columns to return:\nSELECT File.FullPath as ValueName, File.Data.value as Contents, timestamp(epoch=File.Mtime.Sec) as ModTime FROM yara(rules=yaraRule, files=file.FullPath, accessor=\u0026quot;reg\u0026quot;)  Which results in the quite readable:\nF:\\\u0026gt;velociraptor.exe --definitions artifacts artifacts collect Windows.Persistence.Powershell INFO:2018/09/28 21:42:18 Loaded 34 built in artifacts +--------------------------------+--------------------------------+---------------------------+ | ValueName | Contents | ModTime | +--------------------------------+--------------------------------+---------------------------+ | HKEY_USERS\\S-1-5-21-546003962- | about:\u0026lt;script\u0026gt;c1hop=\u0026quot;X642N10\u0026quot;; | 2018-09-28T20:20:53-07:00 | | 2713609280-610790815-1001\\Soft | R3I=new%20ActiveXObject(\u0026quot;WScri | | | ware\\Microsoft\\Windows\\Current | pt.Shell\u0026quot;);QR3iroUf=\u0026quot;I7pL7\u0026quot;;k9 | | | Version\\Run\\\u0026quot;C:\\Windows\\system | To7P=R3I.RegRead(\u0026quot;HKCU\\\\softwa | | | 32\\mshta.exe\u0026quot; | re\\\\bkzlq\\\\zsdnhepyzs\u0026quot;);J7UuF1 | | | | n=\u0026quot;Q2LnLxas\u0026quot;;eval(k9To7P);JUe5 | | | | wz3O=\u0026quot;zSfmLod\u0026quot;;\u0026lt;/script\u0026gt; | | +--------------------------------+--------------------------------+---------------------------+ Artifact: Windows.Persistence.Powershell  Great! This works and only returns values that match the yara signature we developed.\n Testing the artifact Let's test this artifact for real now. We restart the frontend with the --definition flag and this makes the new artifact available in the GUI under the Artifact Collector flow. The GUI also shows the entire artifact we defined so we can see what VQL will be run:\nLaunching the flow appears to work and shows exactly the same result as we collected on the command line:\n But wait! There is a problem! When we log out of the machine, and then rerun the artifact it returns no results!\nWhy is that? Experienced incident responders would recognize that any artifact that works from the HKEY_USERS registry hive is inherently unreliable. This is because the HKEY_USERS hive is not a real hive - it is a place where Windows mounts the user's hive when the user logs in.\nHow does HKEY_USERS hive work? Windows implements the concept of user profiles. Each user has a personal registry hive that stores user specific settings. It is actually a file stored on their home directory called ntuser.dat. When a user logs into the workstation, the file may be synced from the domain controller and then it is mounted under the HKEY_USERS\u0026lt;sid\u0026gt; registry hive.\nThis means that when the user logs out, their user registry hive is unmounted and does not appear in HKEY_USERS any longer. Any artifacts based around the HKEY_USERS hive will work only if the collection is run when a user is logged in.\nThis is obviously not what we want when we hunt for persistence! We want to make sure that none of the users on the system have this persistence mechanism installed. You can imagine a case where a system has been cleaned up but then a user logs into the machine, thereby reinfecting it!\n How to fix this? Yara is a very powerful tool because it allows us to search for patterns in amorphous data (such as process memory and structured files) without having to fully understand the structure of the data we are searching for. Of course this has its limitations, but yara can raise a red flag if the signature matches the file, and we can analyse this file more carefully later.\nIn this case, we can not rely on globbing the HKEY_USER registry hive, so maybe we can just search the files that back these hives? We know that each user on the system has an NTUSER.DAT file in their home directory (usually C:\\Users\\\u0026lt;username\u0026gt;), so let's write an artifact to find these files. We can reuse the artifact Windows.Sys.Users that reports all user accounts on a system (we display it as JSON to enhance readability):\nF:\\\u0026gt;velociraptor.exe artifacts collect Windows.Sys.Users --format json INFO:2018/09/28 22:44:26 Loaded 34 built in artifacts { \u0026quot;Description\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Directory\u0026quot;: \u0026quot;C:\\\\Users\\\\test\u0026quot;, \u0026quot;Gid\u0026quot;: 513, \u0026quot;Name\u0026quot;: \u0026quot;test\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;local\u0026quot;, \u0026quot;UUID\u0026quot;: \u0026quot;S-1-5-21-546003962-2713609280-610790815-1001\u0026quot;, \u0026quot;Uid\u0026quot;: 1001 }, { \u0026quot;Description\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Directory\u0026quot;: \u0026quot;C:\\\\Users\\\\user1\u0026quot;, \u0026quot;Gid\u0026quot;: 513, \u0026quot;Name\u0026quot;: \u0026quot;user1\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;local\u0026quot;, \u0026quot;UUID\u0026quot;: \u0026quot;S-1-5-21-546003962-2713609280-610790815-1003\u0026quot;, \u0026quot;Uid\u0026quot;: 1003 },  So we just want to YARA scan the NTUSER.DAT file in each home directory:\nSELECT * from foreach( row={ SELECT Name, Directory as HomeDir FROM Artifact.Windows.Sys.Users() WHERE Directory.value and Gid }, query={ SELECT File.FullPath As FullPath, Strings.Offset AS Off, Strings.HexData As Hex, upload(file=File.FullPath, accessor=\u0026quot;ntfs\u0026quot;) AS Upload FROM yara( files=\u0026quot;\\\\\\\\.\\\\\u0026quot; + HomeDir + \u0026quot;\\\\ntuser.dat\u0026quot;, accessor=\u0026quot;ntfs\u0026quot;, rules=yaraRule, context=10) })  This query:\nSelects all the usernames and their home directory from the Windows.Sys.Users artifact. For each directory prepends \\\\.\\ and appends \u0026quot;ntuser.dat\u0026quot;. For example c:\\Users\\test becomes \\\\.\\c:\\Users\\test\\NTUSER.dat The file is accessed using the NTFS filesystem accessor. This is necessary because the registry hive is locked if the user is logged in. Therefore we must access it using raw NTFS parsing to bypass the OS locking. For each file that matches the yara expression, we upload the file to the server for further analysis.  Lets run this new artifact on the server:\nUnlike the previous artifact, this one simply returns the YARA hit, but because we do not have any context on which value contained the signature, or even if it had been deleted. Luckily we uploaded the raw registry hive for further analysis, and we can use a tool such as RegRipper to extract more information from the hive:\n$ wine rip.exe -p user_run -r /tmp/velociraptor/clients/C.c916a7e445eb0868/uploads/F.078739d6/ntfs/ %5C%5C.%5CC%3A%5CUsers%5Cuser1%5CNTUSER.DAT Launching user_run v.20140115 user_run v.20140115 (NTUSER.DAT) [Autostart] Get autostart key contents from NTUSER.DAT hive Software\\Microsoft\\Windows\\CurrentVersion\\Run LastWrite Time Thu Sep 27 01:19:08 2018 (UTC) OneDrive: \u0026quot;C:\\Users\\user1\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe\u0026quot; /background c:\\windows\\system32\\mshta.exe: about:\u0026lt;script\u0026gt;c1hop=\u0026quot;X642N10\u0026quot;; R3I=new%20ActiveXObject(\u0026quot;WScript.Shell\u0026quot;); QR3iroUf=\u0026quot;I7pL7\u0026quot;;k9To7P=R3I.RegRead(\u0026quot;HKCU\\\\software\\\\ bkzlq\\\\zsdnhepyzs\u0026quot;);J7UuF1n=\u0026quot;Q2LnLxas\u0026quot;;eval(k9To7P);JUe5wz3O=\u0026quot;zSfmLod\u0026quot;;\u0026lt;/script\u0026gt;  Note above how we can simply retrieve the uploaded file from Velociraptor's filestore. Velociraptor stores uploaded files on the filesystem within the flow's directory.\n Conclusions In this blog post we saw how to utilize YARA to find suspicious powershell persistence mechanisms. YARA is a powerful tool and using Velociraptor's artifacts we can apply it to files, registry values, and raw NTFS files such as locked registry hives and the pagefile.\nWe also saw some of the inherent problems with relying on the HKEY_USERS registry hive for detection - the hive is only present when a user is logged in so when we hunt, we might miss those users who are currently logged out. We saw how YARA can be used to detect suspicious patterns in raw registry hive files and how artifacts may retrieve those files for further analysis.\n  "
},
{
        "uri": "/blog/html/2018/09/03/velociraptor_walk_through_and_demo.html",
        "title": "Velociraptor walk through and demo",
        "tags": [],
        "description": "A screencast of the latest Velociraptor",
        "content": "I just uploaded a screencast of the latest Velociraptor - check it out and play with it, and please provide feedback at velociraptor-discuss\u0026#64;googlegroups.com\n  "
},
{
        "uri": "/blog/html/2018/08/20/velociraptor_artifacts.html",
        "title": "Velociraptor Artifacts",
        "tags": [],
        "description": "We are super excited to introduce this point release of Velociraptor\n(0.2.2) which introduces the concept of Velociraptor Artifacts for the\nfirst time. This post is about what artifacts are, what they do and\nhow can you use them.\n",
        "content": "First a bit of history. When we first started writing endpoint monitoring tools (With GRR then Rekall Agent) we implemented the ability to collect files, registry keys and other data. If an analyst wanted to collect, say the chrome extensions, they would need to know where chrome extensions typically reside ( %homedir%/.config/google-chrome/Extensions/**) and enter that in each time.\nWe soon realized this was error prone and required too much mental overhead for analysts to constantly remember these details. GRR inspired the creation of the Forensic Artifacts project. It was created in order to solve the problem of documenting and sharing knowledge about forensic evidence file and registry location.\nFurther, since GRR can only collect files and has limited parsing support, the parsing and interpretation of the artifacts is not specified. GRR Artifacts can only specify file sets (via globs), registry key/value sets and collections of other artifacts. These are a bit limited in their expressiveness, and so it means that GRR has to augment forensic artifacts with a lot of GRR specific things (like post processing, parsing etc) to make them useful. Although Forensic Artifacts are supposed to be tool agnostic they carry over a lot of GRR implementation details (e.g. the knowledge base interpolations, glob patterns etc).\nNext came OSQuery with their SQL like syntax. This was a huge advancement at the time because it allows users to customize the data they obtained from their endpoint, and ask questions from the entire enterprise at once. For the first time it was possible to combine data from multiple sources (i.e. OSQuery \u0026quot;tables\u0026quot;) in an intelligent way and customize the output to fit a processing pipeline, rather than write a lot of interface glue code to filter and extract data.\nCurrently OSQuery has grown many tables - each table typically implements a specific parser to extract one set of data. In this sense OSQuery also solves the same problem as GRR's artifacts - they provide a single named entity (called a table in OSQuery) which produces results about one type of thing (e.g. arp_cache table produces results about the arp cache entries). The user can then just ask for the arp cache and doesn't care how we get it.\nThe next logical development was the development of Velociraptor Query Language (VQL). VQL is not pure SQL - instead it is an SQL like language with a severely reduced feature set. The main difference with regular SQL is the ability to provide arguments to table names - that is a VQL plugin is a data source that can receive arbitrary arguments.\nThis changes the entire game - since we can now provide high level functions to control plugin execution. Combined with the VQL ability to combine multiple queries into subqueries this opens the door for very complex types of queries.\nFor example, consider the OSQuery users table. This table reads the system's /etc/passwd file and parses out the different columns. It is hard coded into the OSQuery binary. While this is a very simple table, it shares its operation with many other similar tables. Other tables open similar files, parse them line by line and return each field as the query's columns. There are many similar files that contain useful information on a system. If one was to add a parser for each one in OSQuery, then they need to write a small amount of code, recompile the binary and push it out to clients.\nRe-deploying new code to endpoints is a difficult task in practice. There is testing and release processes to employ. Furthermore if a local modification is made to OSQuery one needs to submit PRs upstream, otherwise the codebases may diverge and maintainance would be difficult.\nRather than have a built in plugin for each such table, Velociraptor simply includes a number of generic parsers which may be reused for parsing different files. For example, consider the following VQL Query:\nSELECT User, Desc, Uid, Gid, Homedir, Shell FROM parse_records_with_regex( file=\u0026quot;/etc/passwd\u0026quot;, regex='(?m)^(?P\u0026lt;User\u0026gt;[^:]+):([^:]+):' + '(?P\u0026lt;Uid\u0026gt;[^:]+):(?P\u0026lt;Gid\u0026gt;[^:]+):(?P\u0026lt;Desc\u0026gt;[^:]*):' + '(?P\u0026lt;Homedir\u0026gt;[^:]+):(?P\u0026lt;Shell\u0026gt;[^:\\\\s]+)')  The parse_records_with_regex() plugin simply applies one or more regex to a file and each match is sent as a record. In this case, each line is matched and parsed into its components automatically. Note how the query produces the same results as OSQuery's users table, but uses completely generic parsers.\nThe generic parser can be used to parse many other file types. Here is query which parses debian apt-source lines:\nSELECT * FROM parse_records_with_regex( file=\u0026quot;/etc/apt/sources.list\u0026quot;, regex=\u0026quot;(?m)^ *(?P\u0026lt;Type\u0026gt;deb(-src)?) \u0026quot;+ \u0026quot;(?:\\\\[arch=(?P\u0026lt;Arch\u0026gt;[^\\\\]]+)\\\\] )?\u0026quot; + \u0026quot;(?P\u0026lt;URL\u0026gt;https?://(?P\u0026lt;base_uri\u0026gt;[^ ]+))\u0026quot; + \u0026quot; +(?P\u0026lt;components\u0026gt;.+)\u0026quot;)  Having the ability to control parsing directly in the query opens up many possibilities. What if we need to parse new files which do not have an OSQuery parser yet (maybe an enterprise application configuration file)? We can easily construct a query using the generic parsers and issue it to the endpoint to support new file format.\nVelociraptor Artifacts In the previous section we saw how we can express very complex queries to support novel parsing scenarios. However it is hard for users to directly issue the queries - who can remember this complex regex and type it in every time?\nWe clearly need some way to record the queries in a simple, reusable way. This sounds a lot like GRR's Artifacts! What if we could just write the complex query in a YAML file and then just said to Velociraptor - go collect that artifact and the correct queries would be issued to the client automatically.\nRather than try to make artifacts generic, we define Velociraptor Artifacts as YAML files which simply bundle together a bunch of VQL statements that together run a particular query. In a sense, Velociraptor's artifacts are similar to OSQuery's table definition (since they specify output columns), except they are defined completely by the YAML definition file, using generic reusable VQL plugins, put together with VQL queries.\nHere is an example of the the Linux.Sys.Users artifact - this is the equivalent artifact to OSQuery's users table:\nname: Linux.Sys.Users description: Get User specific information like homedir, group etc from /etc/passwd. parameters: - name: PasswordFile default: /etc/passwd description: The location of the password file. sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - SELECT User, Desc, Uid, Gid, Homedir, Shell FROM parse_records_with_regex( file=PasswordFile, regex='(?m)^(?P\u0026lt;User\u0026gt;[^:]+):([^:]+):' + '(?P\u0026lt;Uid\u0026gt;[^:]+):(?P\u0026lt;Gid\u0026gt;[^:]+):(?P\u0026lt;Desc\u0026gt;[^:]*):' + '(?P\u0026lt;Homedir\u0026gt;[^:]+):(?P\u0026lt;Shell\u0026gt;[^:\\\\s]+)')  The artifact has a specific name (Linux.Sys.Users) and a description. The Artifact will only run if the precondition is satisfied (i.e. if we are running on a linux system). Running the artifact locally produces the following output:\n$ velociraptor artifacts collect Linux.Sys.Users +-------------------+-------------------------+-------+-------+--------------------------+ | USER | DESC | UID | GID | HOMEDIR | +-------------------+-------------------------+-------+-------+--------------------------+ | root | root | 0 | 0 | /root | | daemon | daemon | 1 | 1 | /usr/sbin | | bin | bin | 2 | 2 | /bin | | sys | sys | 3 | 3 | /dev | | sync | sync | 4 | 65534 | /bin | | games | games | 5 | 60 | /usr/games | | man | man | 6 | 12 | /var/cache/man | | lp | lp | 7 | 7 | /var/spool/lpd | | mail | mail | 8 | 8 | /var/mail |   Why would I want to use Artifacts? We just demonstrated that Velociraptor's artifact produces the same output as OSQuery's users table - so what? Why use an artifact over hard coding the table in the executable?\nVelociraptor is inherently a remote endpoint monitoring agent. Agents are installed on many end points and once installed it is often difficult to remotely update them. For various reasons, endpoints are often difficult to upgrade - for example, they might be off the corporate LAN, or have a broken update agent.\nIn particular, when responding to a major incident, we often have to rapidly deploy a new hunt to search for an indicator of compromise. In most cases we don't have time to go through proper software deployment best practice and upgrade our endpoint agent in rapid succession (it typically takes weeks to have endpoint agents upgraded).\nHowever, Velociraptor's artifacts allow us to write a new type of parser immediately since it is just a YAML file with VQL statements, we can push it immediately to the clients with no code changes, rebuild, or redeploy scripts. That is very powerful!\nNot only can we add new artifacts, but we can adapt artifacts on the fly to different systems - perhaps there is a slightly different version of Linux which keeps files in different locations? Or maybe a slightly different format of the file we are trying to parse. Being able to adapt rapidly is critical.\n So how do I use Artifacts? Velociraptor exposes artifacts via two main mechanisms. The first is the Artifact Collector flow. This flow presents a special GUI which allows us to view the different artifacts, choose which ones we want to launch and describes them:\nAs we can see in the screenshot above, the artifact collector flow allows the user to inspect the artifacts, before issuing the VQL to the client. The responses are received by the server and displayed as part of the same flow:\nThis is a pretty easy set and forget type system. However, Velociraptor makes artifacts available within any VQL query too. The artifact simply appears as another VQL plugin. Consider the following VQL Query that filters only user accounts which have a real shell:\n$ velociraptor query --format text \u0026quot;SELECT * FROM Artifact.Linux.Sys.Users() where Shell =~ 'bash'\u0026quot; +------+------+------+------+-----------+-----------+ | USER | DESC | UID | GID | HOMEDIR | SHELL | +------+------+------+------+-----------+-----------+ | root | root | 0 | 0 | /root | /bin/bash | | mic | | 1000 | 1000 | /home/mic | /bin/bash | +------+------+------+------+-----------+-----------+ SELECT * FROM Artifact.Linux.Sys.Users() WHERE Shell =~ 'bash'  An artifact definition can use other artifacts by simply issuing queries against these artifact plugins. This forms a natural system of interdependency between artifacts, and leads to artifact reuse.\nHow powerful are Velociraptor Artifacts? Previously we described Velociraptor artifacts as having some properties in common with GRR's artifacts (pure YAML, reusable and server side) and OSQuery's tables (very detailed and potentially complex parsers, directly using APIs and libraries). We said that Velociraptor attempts to replace many of the specific \u0026quot;one artifact per table\u0026quot; model in OSQuery with a set of YAML files referencing generic plugins.\nVelociraptor's artifacts can never fully emulate all OSQuery's tables because some OSQuery tables call specific APIs and have very complex operation. However, most of OSQuery's tables are fairly simple and can be easily emulated by Velociraptor artifacts. In this sense - Velociraptor lies somewhere in between GRR's simple collect all files and registry keys without parsing them, and OSQuery's specialized parsers. However VQL is quite capable, as we shall see. Although we can not implement all tables using pure VQL queries, the ability to implement many artifacts this way provides us with unprecedented flexibility and enables rapid response to evolving threats.\nLet's looks at some artifacts that demonstrate this flexiblity.\n Parsing debian packages. Debian packages keep a manifest file with records delimited by an empty line. Each record consists of possible fields.\n- LET packages = SELECT parse_string_with_regex( string=Record, regex=['Package:\\\\s(?P\u0026lt;Package\u0026gt;.+)', 'Installed-Size:\\\\s(?P\u0026lt;InstalledSize\u0026gt;.+)', 'Version:\\\\s(?P\u0026lt;Version\u0026gt;.+)', 'Source:\\\\s(?P\u0026lt;Source\u0026gt;.+)', 'Architecture:\\\\s(?P\u0026lt;Architecture\u0026gt;.+)']) as Record FROM parse_records_with_regex( file=linuxDpkgStatus, regex='(?sm)^(?P\u0026lt;Record\u0026gt;Package:.+?)\\\\n\\\\n') - SELECT Record.Package as Package, Record.InstalledSize as InstalledSize, Record.Version as Version, Record.Source as Source, Record.Architecture as Architecture from packages  The above query uses the parse_records_with_regex() plugin to split the file into records (anything between the Package: and the next empty line). Each record is then parsed separately using the parse_string_with_regex() VQL function. Being able to parse in two (or more) passes makes writing regexes much easier since they can be simplified greatly.\n Complex multi-query example: Chrome extensions. An example of a sophisticated artifact is the chrome extensions artifact. It implements the following algorithm:\nFor each user on the system, locate all chrome extension manifest files by using a glob expression. Parse the manifest file as JSON If the manifest contains a \u0026quot;default_locale\u0026quot; item, then locate the locale message file. Parse the locale message file. Extract the extension name - if the extension has default locale then return the string from the locale file, otherwise from the manifest file.  The full artifact is rather long so will not be listed here in full, but are a couple of interesting VQL plugins which make writing artifacts more powerful.\nThe foreach() plugin runs a query and for each row produced, a second query is run (with the first row present in the scope). This is similar to SQL's JOIN operator but more readable. For example the following query executes a glob on each user's home directory (as obtained from the password file):\nLET extension_manifests = SELECT * from foreach( row={ SELECT Uid, User, Homedir from Artifact.Linux.Sys.Users() }, query={ SELECT FullPath, Mtime, Ctime, User, Uid from glob( globs=Homedir + '/' + extensionGlobs) })  Note how the query is assigned to the variable \u0026quot;extension_manifests\u0026quot; which can be used as an input to other queries. The if() plugin evaluates a condition (or a query) and runs the \u0026quot;then\u0026quot; query if true, or the \u0026quot;else\u0026quot; query:\nLET maybe_read_locale_file = SELECT * from if( condition={ select * from scope() where Manifest.default_locale }, query={ SELECT Manifest, Uid, User, Filename as LocaleFilename, ManifestFilename, parse_json(data=Data) AS LocaleManifest FROM read_file( -- Munge the filename to get the messages.json path. filenames=regex_replace( source=ManifestFilename, replace=\u0026quot;/_locales/\u0026quot; + Manifest.default_locale + \u0026quot;/messages.json\u0026quot;, re=\u0026quot;/manifest.json$\u0026quot;)) }, else={ -- Just fill in empty Locale results. SELECT Manifest, Uid, User, \u0026quot;\u0026quot; AS LocaleFilename, \u0026quot;\u0026quot; AS ManifestFilename, \u0026quot;\u0026quot; AS LocaleManifest FROM scope() })   Parsing binary data: Wtmp file parser. It is also possible to parse binary files with VQL. For example, consider the wtmp file parser implemented in the Linux.Sys.LastUserLogin artifact. This artifact uses the binary_parser() VQL plugin which accepts a Rekall style profile string to instantiate an iterator over the file. Since the wtmp file is simply a sequence of wtmp structs, we can iterate over them in a query.\nSELECT * from foreach( row={ SELECT FullPath from glob(globs=split(string=wtmpGlobs, sep=\u0026quot;,\u0026quot;)) }, query={ SELECT ut_type, ut_id, ut_host as Host, ut_user as User, timestamp(epoch=ut_tv.tv_sec) as login_time FROM binary_parse( file=FullPath, profile=wtmpProfile, iterator=\u0026quot;Array\u0026quot;, Target=\u0026quot;wtmp\u0026quot; ) })   The future We started implementing many of the simpler OSQuery tables using VQL. For the remaining tables (the ones that need to call out to libraries or more complex APIs), we will integrate these using a set of specialized VQL plugins over time.\n  "
},
{
        "uri": "/blog/html/2018/08/10/design_differences_between_velociraptor_and_grr.html",
        "title": "Design differences between Velociraptor and GRR",
        "tags": [],
        "description": "One of the main motivators for developing Velociraptor is the\nopportunity to try different approaches than GRR. Velociraptor has a\nnumber of fundamental design differences in contrast with the GRR\ndesign which improve overall performance and scalability.  We tried to\nkeep it light weight cutting out the features we think we did not need\nand leaving behind a fast, lean and mean raptor!\n",
        "content": "Velociraptor Clients run full VQL queries GRR's design started off with the assumption that the client should be minimalist and only support a few simple primitives (such as ListDirectory, ListProcesses etc). The intention was that most of the processing would be executed on the server inside a \u0026quot;Flow\u0026quot;. The main motivation for this design choice was the observation that it is difficult to upgrade the client in practice, and so with a minimal client, it would be possible to develop more sophisticated Flows, server side, without needing to update the clients.\nAfter running GRR for a while we noticed that this design choice was problematic, since it leads to many client round trips. For example the FileFinder flow searches the client's filesystem for files by name, date etc. GRR's original file finder uses a complex algorithm to issue ListDirectory requests to the client, receive their responses, filter and recurse into directories by communicating with the client again. This leads to many round trips and has a huge performance hit on both the server and client.\nVelociraptor does away with all that by including rich client side functionality (through VQL plugins), and implementing VQL queries to perform the filtering. This means that in reality, Velociraptor has very few client round trips, generally just one: The VQL query is sent to the client, and the result is received by the server.\nSome types of analysis require the results of one operation to feed into the next operation. For example, suppose we wanted to upload all executables that are run from a temp directory. This requires listing all processes, then filtering the ones running from a temp directory, and finally uploading those to the server.\nGRR's model requires writing a new flow for this - the flow first issues a ListProcesses request to the client, then receives all processes where the filtering happens on the server. The server then issues upload commands for each matching process. Performing this analysis requires writing and deploying new code making it difficult to adapt rapidly to changing threats.\nWith Velociraptor one simply issues the following VQL query:\nLET files = SELECT Exe, Cmdline, Username FROM pslist() WHERE Exe =~ '(?i)temp' SELECT Exe, Cmdline, Username, upload(file=Exe) AS Upload FROM files  VQL avoids this round trip completely, since VQL queries can be nested and chained together. Therefore one simply runs the first query (list all processes running from temp directory), and sends the results to the next query (download the matching files) inside the same VQL client request. It is rare that Velociraptor flows run multiple client round trips, resulting in lightweight and fast completing flows.\n Worker and Database queues. The GRR model of long running flows with multiple client/server interactions required more complex design. Since client messages can be delivered in multiple POST requests, and a single request can result in multiple responses, GRR must queue responses somewhere until they are all ready to be processed. Otherwise writing GRR flows would be difficult because one would need to account for incomplete responses.\nGRR uses a complex request/response protocol to ensure messages are delivered in order, reminiscent of the TCP stack's packet reassembling algorithms.\nConsider the simple request \u0026quot;ListDirectory\u0026quot;. The client request may elicit thousands of responses (one for each file) and may span multiple POST operations. The GRR frontend queues all the responses in the database until it receives a STATUS response, and then fet once. So even if the client sends the responses over multiple packets, the flow only sees a single list. When a status message is seen by the frontend, it notifies the worker via a worker queue, which collects all responses, orders them by response ID and delivers to the flow object.\nThis design is necessary if flows are long lived and need to handle thousands of responses for each request. However in practice this design has a couple of serious problems:\nThe frontend receives responses and just writes them into the database in special queue rows, then the worker reads them from the queue rows for processing (after which they must be deleted from the database). This leads to a lot of unnecessary read/write/delete cycles and extra load on the database. The worker queue rows are used by all clients and all flows. This leads to a lot of database contention on these rows. Extra care must be taken to ensure no race conditions, through careful management of database locks. Extra locks slow down the database and typically for a busy system queue contention is a huge bottleneck.  This is easy to observe in practice on a busy GRR system (i.e. one that is running many flows or hunts) by simply looking at the output from top. Typically the mysql process uses as much CPU or more than the frontends and workers combined. This indicates a huge load on the database and limits scalability. Increasing the number of frontends only helps marginally because the database throughput becomes the limiting factor. In fact, increasing the number of workers can deteriorate performance because workers poll on their queues while holding locks thereby increasing row lock contention even more.\nVelociraptor takes a different approach. Since Velociraptor flows are very simple and typically only consist of a few request/response cycles, the server does not bother to reorder replies that come in different packets. Therefore there is no need to temporarily store or queue responses. Responses can be delivered to the flow as soon as they are received - and flows typically just write them to the database in their final storage location.\nTherefore Velociraptor does not have a dedicated worker, nor does it have database queues. The frontend itself runs the flows directly on the received packets while serving the client's poll request. This completely eliminates the need for worker queues and their associated database contention issues. Removing the worker queues eliminates a significant amount of very complex and delicate code. Additionally, since the responses are not written/read to the queue, the total load on the database is significantly reduced. (In fact because database lock contention is so low, Velociraptor can work very well with plain files through the FileBaseDataStore, even at large scale!)\nThe following illustration demonstrates how significant this is for the simple example of a ListDirectory request of a directory with 1000 files in it (e.g. the c:windows directory). The equivalent VQL is select * from glob(paths='c:/windows/*') and only produces a single response packet containing all the files in the one table, whereas GRR's ListDirectory client action produces a single response for each file, which is then queued and stored independently in the database.\nThe overall effect, in the GRR case, is that 2000 database rows are created, of which 1000 rows are immediately deleted - a significant database load. Compare this with the Velociraptor equivalent flow - the VQL request is sent to the client once, then the response is returned to the frontend in a single POST operation. Since Velociraptor does not have a separate worker and does not need to queue messages to it, the frontend immediately runs the flow which just writes the result into a single DB row - total database operations: 1 row written.\nEliminating the need for a separate worker process also simplifies deployment significantly. GRR needs to deploy separate frontends and worker processes, and it is often difficult to know which one to scale up. Scaling up the frontend will allow more packets to be received but actually increases the load on the database. Not having sufficient workers will leave many requests on the queue for a long time and will prolong the execution of the flow since a worker must run the flow in order to issue the next set of requests. This leads to flows which take many hours to complete and even hung flows (if the client reboots or disconnects before the flow finished).\nVelociraptor deployment is much simpler - there is only a single binary and it can be scaled and load balanced as needed. Since database load is much lower, the frontend can handle a much larger load. Furthermore, the flows typically execute in very short time (since there is only one round trip). The overall result is that flow throughput is much increased and resource usage is reduced.\n "
},
{
        "uri": "/blog/html/2018/02/09/files_files_everything_is_just_a_file.html",
        "title": "Files, files everything is just a file!",
        "tags": [],
        "description": "Velociraptor introduced a major redesign of the underlying data\nstore architecture. The default data store\nis now the FileBasedDataStore which stores all data in flat files.\n",
        "content": "GRR's original design abstracted the data storage to a simple key/value store originally based around Bigtable. For open source deployments various key value stores were used starting from MongoDB, to SQLite and finally MySQL. Although the original idea was to use a simple key/value implementation, due to locking requirements the data store implementation became very complex.\nAs Velociraptor introduced a major redesign of the underlying data store architecture, we are now able to relax our demands of the datastore and use a true key/value model (since we have no requirements for locking and synchronization). The default data store is now the FileBasedDataStore which stores all data in flat files.\nUsing flat files over a database has many advantages, including ease of deployment, and simplification of the data model. Having flat files allows one to use standard tools to visualize Velociraptor's data structures (e.g. with less), archive old data (e.g. with tar/zip) and clean up old data (e.g. with find/rm). Velociraptor also includes an inspect command which allows users to decode the stored files and provides context as to what these files actually mean. This simplicity increases the transparency in the system and makes it more accessible for deployers, while increasing reliability, stability and speed.\nIn the following section we examine some of the files in the datastore and see how they relate to the features we discuss elsewhere in this document.\nFile organization The Velociraptor data store needs to provide only two types of operations: Read and Write complete files and list files in a directory. Using only these primitives we can implement the entire filestore. Most modern file systems provide very fast file creation, reading and deletion, as well as fast directory listing, even when containing millions of files. Modern file systems also provide advanced features like caching, journaling and rollbacks so it is not such a crazy idea to use the file systems themselves as a data store.\nLet's begin by listing the files in a typical Velociraptor file store using the find command. We then use the velociraptor inspect command to view the file's content.\nSearching Searching for clients is implemented by simply creating empty files in directories based on the search term. For example in order to retrieve all clients which have the user \u0026quot;mic\u0026quot;, we simply list the directory \u0026quot;client_index/user%3Amic\u0026quot;:\n$ find ./client_index/ ./client_index/c.84216c7aab97557d ./client_index/c.84216c7aab97557d/C.84216c7aab97557d.db./client_index/user%3Amic ./client_index/user%3Amic/C.84216c7aab97557d.db ./client_index/user%3Amic/C.1b0cddfffbfe40f5.db./client_index/all ./client_index/all/C.84216c7aab97557d.db ./client_index/all/C.1b0cddfffbfe40f5.db  Modern file systems can hold many thousands of files in the same directory and list these very quickly. This feature is only really used in the GUI's search box but can also be used to script or post process collected data.\n Client information Information about each client is kept in a directory based on the client's ID:\n./C.0fc63b45671af1a6/ping.db \u0026lt;- Last ping stats. ./C.0fc63b45671af1a6/key.db \u0026lt;- Client's public key ./C.0fc63b45671af1a6/flows ./C.0fc63b45671af1a6/flows/F.a8787c26.db \u0026lt;- Flows running on this client. ./C.0fc63b45671af1a6/flows/F.e05952ff.db ./C.0fc63b45671af1a6/tasks ./C.0fc63b45671af1a6/tasks/1533517805834284.db \u0026lt;- Client messages waiting to be collected. ./C.0fc63b45671af1a6/tasks/1533517805834283.db ./C.0fc63b45671af1a6/tasks/1533517206859989.db ./C.0fc63b45671af1a6/tasks/1533517206860477.db ./C.84216c7aab97557d.db \u0026lt;- Client information (from Interrogate).  Each piece of data is kept in its own file as an encoded protobuf. Files all have their names end with \u0026quot;.db\u0026quot;. Velociraptor has an inspect command which decodes the protobuf and displays it in a human friendly way. For example let us see what information we keep about each s last poll:\n$ velociraptor --config server.yaml inspect /tmp/velociraptor/C.2d406f47d80f5583/ping.db { \u0026quot;ipAddress\u0026quot;: \u0026quot;127.0.0.1:33600\u0026quot;,\u0026quot;ping\u0026quot;: \u0026quot;1533517053018582\u0026quot; }   The Flow's results. Velociraptor's flows typically only produce VQL results. As described above, the VQL results are typically split into parts by the client (by default 10000 rows per part), and Velociraptor simply writes these in the flow's directory:\n./C.1b0cddfffbfe40f5/flows/F.a31255a1 ./C.1b0cddfffbfe40f5/flows/F.a31255a1/results ./C.1b0cddfffbfe40f5/flows/F.a31255a1/results/0.db \u0026lt;- VQL result part 1. ./C.1b0cddfffbfe40f5/flows/F.a31255a1.db \u0026lt;- Flow information.  Velociraptor's inspect command understands that VQL collections represent a table of results, and so it displays these in a more friendly way.\n$ velociraptor --config server.yaml inspect /tmp/velociraptor/C.1b0cddfffbfe40f5/flows/F.a31255a1/results/0.db +-------+----------------+---------+------+-----------------------------+----------------------------+ | ISDIR | FULLPATH | SIZE | MODE | MTIME | ATIME | +-------+----------------+---------+------+-----------------------------+----------------------------+ | false | /bin/bash | 1037528 | 493 | 2017-05-16T22:49:55+10:00 | 2018-01-22T12:47:25+10:00 | | false | /bin/busybox | 1964536 | 493 | 2015-08-19T22:07:39+10:00 | 2018-01-23T15:41:46+10:00 | +-------+----------------+---------+------+-----------------------------+----------------------------+ File Finder Response: SELECT IsDir , FullPath , Size , Mode , mtime , atime , ctime, upload(file=FullPath)as Upload FROM files  We can also see the original VQL query which was run to produce this output. The bottom line, though, is that the entire flow's result is just a flat JSON encoded file. You can easily decode the data using any programming language and post process it in whatever way is appropriate (e.g. export the results to BigQuery or ElasticSearch). Velociraptor does not really do anything with the result other than just store it on disk.\n The Virtual File System As described above, Velociraptor's VFS consists of VQL tables for each directory on the client, listing the entire directory content:\n./C.1b0cddfffbfe40f5/vfs/usr/share/doc/gir1.2-freedesktop.db ./C.1b0cddfffbfe40f5/vfs/usr/share/doc/libdatrie1.db ./C.1b0cddfffbfe40f5/vfs/usr/share/doc/dh-strip-nondeterminism.db ./C.1b0cddfffbfe40f5/vfs/usr/share/doc/libcap2-bin.db ./C.1b0cddfffbfe40f5/vfs/usr/share/doc/libsoup2.4-1.db ./C.1b0cddfffbfe40f5/vfs/usr/share/doc/libgphoto2-port12.db ./C.1b0cddfffbfe40f5/vfs/usr/share/doc/libsodium18.db  Inspecting each of these shows it is just a simple VQL table. This particular VFS entry was produced from a recursive directory listing of /usr (of depth 5).\n$ velociraptor --config server.yaml inspect .../vfs/usr/share/doc/libcap2-bin.db +-------+--------------------------------+---------------------+------+-----------+-------------------- | ISDIR | FULLPATH | NAME | SIZE | MODE | MTIME +-------+--------------------------------+---------------------+------+-----------+-------------------- | false | /usr/share/doc/libcap2-bin/REA | README.Debian | 1149 | 420 | 2015-10-02T23:34:07 | | DME.Debian | | | | | false | /usr/share/doc/libcap2-bin/cha | changelog.Debian.gz | 30 | 134218239 | 2015-10-24T07:11:34 | | ngelog.Debian.gz | | | | | false | /usr/share/doc/libcap2-bin/cop | copyright | 4367 | 420 | 2015-10-02T23:34:07 | | yright | | | | +-------+--------------------------------+---------------------+------+-----------+-------------------- /usr: SELECT IsDir, FullPath as _FullPath, Name, Size, Mode, timestamp(epoch=Sys.Mtim.Sec) as mtime, timestamp(epoch=Sys.Atim.Sec) as ys.Ctim.Sec) as ctime FROM glob(globs=path + '/**5')    "
},
{
        "uri": "/blog/html/2018/08/10/hunting_what_velociraptors_do_best.html",
        "title": "Hunting - What Velociraptors do best!",
        "tags": [],
        "description": "A hunt is a feature where a single flow may be run on multiple clients\nat the same time. Typically a hunt looks for a particular indicator of\ncompromise across the entire deployment, or maybe collect the same\nfiles from every deployed agent. By their nature, hunts cause multiple\nflows to run simultaneously and so this creates a large contention of\nshared state.\n",
        "content": "Velociraptor has completely redesigned the way that hunts are implemented in order to avoid database locking and increase hunt processing efficiency.\nNow we hunt like this:\nHow are hunts scheduled? GRR allows hunts to be scheduled by a few client properties such as OS type, label, users etc. This works because GRR has an extensive data model of endpoint properties. However, this requires that the data model be refreshed periodically to be kept accurate. For example, to run a hunt of all machines with a suspected compromised user account we can schedule the run on all machines where the user has logged in, but because GRR uses its data model to decide if a machine should be issued the hunt, the data model may be out of date and GRR will not schedule the hunt on machines which have only recently been logged into. For this reason we typically run the Interrogate hunt very frequently causing a lot of extra load on the system and clients hoping to minimize the time window where the data model is out of date with reality.\nVelociraptor's approach is different - since Velociraptor does not really maintain a data model server side, we check the client's information for every hunt, before we even decide if the hunt should be scheduled for this client. This is done by issuing a VQL query to the client.\nSometimes we don't necessarily want the client to know exactly why we are scheduling the hunt (e.g. in the compromised user account case we don't want to advertise the exact username we are looking for). In these cases we run another VQL query on the server side.\nSo hunt selection is managed by two different VQL queries - a client side one and a server side on.\nThe default client side VQL queries simply collects the usual facts like OS version, Username etc, while the server side query filters the results with more specific conditions. This approach does not reveal to the client the hunt's condition:\nClient side VQL: SELECT OS, Architecture, Fqdn, Platform, config.Client_labels AS Labels FROM info() Server side VQL: SELECT * from rows WHERE Fqdn =~ '(?i)myhostname.+' AND 'MY_LABEL' IN Labels   The hunt's life cycle When the hunt is started, the server updates its in-memory list of active hunts managed by the Foreman. Clients then poll the foreman for new hunts they should participate in. Clients remember the last hunt they participated in and so they present this hunt's timestamp to the foreman. If a new hunt is available, the foreman can immediately launch the CheckHuntCondition flow on the client.\nNote\nThe clients themselves are actively keeping track of the hunts they participated in. This avoids the server having to check the client's DB record.\n The CheckHuntCondition flow issues the client side VQL queries and then runs the server side query on the results. If the query matches (i.e. the hunt should be scheduled for this client), the client's record is written into the hunt's \u0026quot;pending\u0026quot; queue.\n The hunt manager Each hunt specifies its own client recruitment rate (i.e. how many clients will be started per minute). The hunt manager is a component which periodically reads all hunts and schedules flows for these hunts if the hunts' client rate allows for more clients to be scheduled. It does this by moving clients from the pending queue to the running queue and starting respective flows for them.\nOnce each of those flows completes, the record is moved from the running queue to the completed queue or the results queue if the flow produced any results. We can observe how many clients exist in each queue using the GUI.\nThe flows that hunts launch arn the client. However, when they complete, a small record is made in the hunts's results queue pointing to the flow. It is therefore possible to retrieve all results from the hunt from all client's. For example, the GUI allows downloading a zip file of all the results and files uploaded:\nSince hunts invoke regular flows, and Velociraptor flows are much lighter than GRR's flows, hunts are much cheaper to run in terms of resources consumed.\n "
},
{
        "uri": "/blog/html/2018/08/10/interrogation_make_the_endpoint_tell_us_what_it_knows.html",
        "title": "Interrogation - Make the endpoint tell us what it knows!",
        "tags": [],
        "description": "Interrogation is the process of learning general information about the\nendpoint we are monitoring. Each endpoint is automatically\ninterrogated when it first joins the Velociraptor server, and the GUI\nshows this general information about each client.\n",
        "content": "When writing Velociraptor we decided to keep things very simple - we did away with a lot of the information gathered during interrogate in favor of a much simpler data model.\nData Modelling - The Interrogate Flow GRR maintains an elaborate model of client data. For example, GRR collects and maintains a list of clients' network interfaces, users, user's home directory etc. This information is maintained in elaborate protobufs and stored in the database in many rows.\nWhile some of this information is needed for client searching, GRR maintains vastly more information than necessary in this data model. The client data model is built during the interrogate phase (A periodic flow run on the clients to refresh server side data).\nMaintaining such a complex data model results in a very rigid design. For example, if a user wanted to collect more information from clients they would need to modify protobufs, update the interrogate flow, recompile the code and redeploy. These modifications are also very invasive as once code has been heavily modified, there is an overhead of keeping these modifications in sync with newer upstream versions.\nVelociraptor also maintains client information via its Interrogate flow. However, Velociraptor's interrogate flow simply issues a series of VQL queries, and these responses are stored directly in the database with minimal interpretation. Indexes are maintained for some information which users should be able to search on, but there is no attempt to build or maintain a client data model at all (You can see details of the model described below in the FileBaseDataStore post).\nThe advantage of this approach is that users can simply add extra VQL queries to the interrogate phase to collect more tailored site specific information. This does not require compiling of any code or redeploying the server. The following example illustrates the power of this technique.\n Customizing the Interrogate flow. Normally Velociraptor collects minimal information from the client upon interrogation (i.e. when the client first enrols or when interrogated periodically). However it is very easy to customize this collection depending on local site requirements. In this section we work through a step by step example of extending the Velociraptor interrogate flow.\nSuppose that in our deployment we wanted to check if a machine is able to be logged into remotely. For a Linux machine we want to see all authorized_keys files on every machine that enrolls. Collecting this information allows us to quickly see which machines a compromised user account could spread to.\nWe know we need to issue a VQL query but we are not 100% sure which one. Luckily we can use Velociraptor itself to run the query locally using the syntax \u0026quot;velociraptor query \u0026lt;query\u0026gt;\u0026quot;.\nStart with a simple glob query to find all authorized_keys files:\nSELECT FullPath from glob(globs=\u0026quot;/home/*/.ssh/authorized_keys\u0026quot;)  Suppose we now want to actually grab a copy of all files so we can archive them on the server This will keep a record of the authorized keys on the server for each Interrogate flow. If we run the flow periodically we will end up with a time based evolution of the authorized keys files on each host. Pretty handy!\nSELECT FullPath, timestamp(epoch=Sys.Mtim.Sec) as Mtime, upload(file=FullPath) as Upload FROM glob(globs=\u0026quot;/home/*/.ssh/authorized_keys\u0026quot;)  We can run the query locally using the Velociraptor tool:\nmic\u0026#64;localhost:/tmp\u0026gt; velociraptor query \u0026quot;select FullPath, \\ timestamp(epoch=Sys.Mtim.Sec) as Mtime, \\ upload(file=FullPath) as Upload \\ FROM glob(globs=['/home/*/.ssh/authorized_keys'])\u0026quot; velociraptor: Uploaded home/mic/.ssh/authorized_keys (395 bytes) [ { \u0026quot;FullPath\u0026quot;: \u0026quot;/home/mic/.ssh/authorized_keys\u0026quot;, \u0026quot;Mtime\u0026quot;: \u0026quot;2018-08-03T18:20:19+10:00\u0026quot;, \u0026quot;Upload\u0026quot;: { \u0026quot;Path\u0026quot;: \u0026quot;home/mic/.ssh/authorized_keys\u0026quot;, \u0026quot;Size\u0026quot;: 395 } } ]  Velociraptor's query command enables us to run the query directly on the local host and observe the results. When the same query is issued to the Velociraptor client, the same result will be generated and sent to the server. This enables us to interactively develop and test our queries without needing to run a full client/server.\nNote the upload() VQL function which causes the file to be uploaded to the server. (When run locally the file will be copied to the upload directory as can be seen by the upload confirmation message), but when run within the Velociraptor client, the file will be uploaded to the server and stored within the flow.\nWe can now add the query to all Interrogate flows that will be run from now on. We simply add it to the configuration file under the Interrogate.additional_queries key:\nInterrogate.additional_queries: Query: - Name: Authorized Keys VQL: \u0026gt; select FullPath, timestamp(epoch=Mtime.Sec) as Mtime, upload(file=FullPath) as Upload from glob(globs='/home/*/.ssh/authorized_keys')  From now on the additional query will be recorded for all clients. The GUI shows it in the client information page:\n "
},
{
        "uri": "/blog/html/2018/08/10/the_velocidex_query_language.html",
        "title": "Velocidex Query Language (VQL)",
        "tags": [],
        "description": "Velociraptor is powered by VQL and VQL is the killer feature which\nmakes it so powerful. But what exactly is VQL? This section is a quick\noverview of VQL.\n",
        "content": "VQL Overview VQL is only loosely based around SQL in the sense that the general statement structure is similar. However, VQL is a very simple dialect. Like SQL, a VQL query produces a table of results with specific columns and multiple rows. Unlike SQL, the data inside each cell is not limited to simple primitive types (like string, integer etc). In fact any JSON serializable object can be generated in a table's cell. It is not uncommon to generate an entire JSON object with additional fields in each row for a single column.\nThe basic structure of a VQL statement is:\nSELECT Column1, Column2, Column3 from plugin(arg=value) WHERE Column1 \u0026gt; 5  There are three main parts: Column selectors, Plugin and Filter Conditions.\nPlugins The VQL plugin is VQL's data source. Plugins are specific pieces of code which may accept arguments and generate a sequence of rows. VQL's strength is that these plugins are very easy to write and can be added to Velociraptor in order to add extra functionality.\nUnlike SQL, VQL plugins take keyword arguments. This allows Velociraptor plugins to be easily customizable and adaptable. For example, a plugin may list all chrome extensions, and receive an argument pointing it to the user's home directory so it can flexibly be applied to different situations. The ability to provide arguments to plugins encourages writing more generic plugins which can be reused in multiple situations.\nNote\nVQL plugins currently only accept keyword arguments. It is a syntax error to pass args without naming them - glob(\u0026quot;/bin/*\u0026quot;) is not valid syntax, it should be glob(globs=\u0026quot;/bin/*\u0026quot;)\n It is important to appreciate that Plugins generate data dynamically. The data is not stored in a database table first! Plugins may begin generating data immediately and the VQL query will begin processing this data, even if the total amount of data is very large. The Plugin's data is not stored in memory all at once! This allows for plugins to produce an unbounded number of rows and the query will proceed until the required number of results is achieved.\nPlugins may also be cancelled when the query completes, even if the plugin itself is not exhausted.\n Column selectors The Column selectors are a group of expressions specifying which columns will be produced in the output table. As mentioned previously, the values produced in each column are not limited to simple types - it is common to produce entire JSON objects (and even additional tables), lists of values etc.\nThe column selectors specify a transformation to be performed on the output of the plugin in producing the query's columns. The simplest transformation is a single \u0026quot;*\u0026quot;, which means no transformation at all (i.e. relay to the output table exactly the output of the plugin).\nSince plugins may produce any object (for example, a JSON object with nested fields), VQL column specifications can dereference nested fields within the produced data.\nSELECT Sys.Mtim.Sec from glob(globs=\u0026quot;/bin/*\u0026quot;)  Specifying only selected columns can limit the number of columns produced and make the output more useful by removing unneeded fields. For example the following will produce a result table with two columns named FullPath and SIze and a row per file found in the /bin/ directory:\nSELECT FullPath, Size from glob(globs=\u0026quot;/bin/*\u0026quot;)  Column specifications can consist of arbitrary expressions - for example addition, comparisons:\nSELECT FullPath + '.bindir', Size from glob(globs=\u0026quot;/bin/*\u0026quot;) WHERE Size \u0026lt; 1000  In this case it is often useful to add a Column Alias (Note that column aliases can also be used in the WHERE clause):\nSELECT FullPath + '.bindir' as Santized, Size from glob(globs=\u0026quot;/bin/*\u0026quot;)  VQL Functions provide a way to extend VQL expressions. Unlike full plugins they do not produce a sequence of rows, but simply produce a single value (which can be an arbitrary o function formats a timestamp as a string. This is useful since many plugins produce times in seconds since epoch time:\nSELECT FullPath, timestamp(epoch=Sys.Mtim.Sec) as mtimefrom glob(globs=\u0026quot;/bin/*\u0026quot;)  Note\nSome VQL functions have side effects, or are more expensive to run. It is important to understand that VQL transforms the columns emitted from a plugin BEFORE it applies filtering conditions. This is needed in order to allow for column transformations to participate in the filter condition (via the alias).\nDue to this order of operations the following query will upload all files, ignoring the WHERE condition because the upload() function will be evaluated on each row, even if the WHERE clause causes the row to be ignored:\nSELECT FullPath, upload(path=FullPath) from glob(globs=\u0026quot;/bin/*\u0026quot;) WHERE Name =~ \u0026quot;bash\u0026quot;  To upload only the files matching the expression, the query must be split into two - the first query applies the filtering condition and the second query does the upload:\nLET files = SELECT FullPath from glob(globs=\u0026quot;/bin/*\u0026quot;) WHERE Name =~ \u0026quot;bash\u0026quot; SELECT FullPath, upload(path=FullPath) from files    VQL Subselects Unlike SQL, VQL does not have a join operator. SQL is designed to work with databases, and databases have multiple strategies for optimizing query execution (like adding table indexes, query planners etc). Traditionally, SQL authors prefers joins over subselects because in a real database JOIN operations are more optimized to use the database's indexes and query optimizer. However JOIN operations are arguably harder to read and it is hard to predict the order at where operations will be run (e.g. which table will use an index and which will use a row scan).\nSince VQL has no indexes nor does it have a query optimizer, implementing JOIN operations does not make sense. Instead, VQL implements subselects and multi-statement queries and using these tools it is possible for VQL authors to precisely control the query execution plan so it is most efficient.\nIn this sense VQL authors are left to specify the most efficient course of query execution themselves instead of relying on a query optimizer. This is normally done by dividing the query into smaller queries and combining their results in the best order.\nConsider the following query that attempts to search small files for the keyword \u0026quot;foobar\u0026quot;:\nSELECT FullPath from glob(globs=\u0026quot;/bin/*\u0026quot;) where grep(path=FullPath, keywords=[\u0026quot;foobar\u0026quot;]) and Size \u0026lt; 1000  Velociraptor will execute the following steps:\nRun the glob() plugin to produce all the files in the /bin/ directory Transform each row to produce the FullPath. Evaluate the Filter condition on each row. The filter condition requires running the grep() plugin on each file looking for the keyword and evaluating if the SIze of the file is less than 1000. If both conditions are TRUE then Velociraptor will emit the row into the result table.  It is obvious that this is an inefficient query because each and every file will be searched for the keyword regardless of its size. However, there is no point even trying if the file size is not less than 1000 bytes!\nThe problem here is that there are two conditions which both must be true - but each condition has a different cost associated with it. Clearly the grep() condition is more expensive since it requires opening the file and reading it completely. The Size condition is extremely cheap since it is just an integer comparison.\nHowever, VQL is not aware of the relative cost of the two conditions - it does not know that grep() is inherently an expensive operation since to VQL it just looks like another function. Although VQL does some shortcutting (for example it will cancel the grep() function if Size \u0026gt;= 1000) this shortcut cancellation may arrive too late to stop grep() from doing a significant amount of work. The VQL author must be aware of the relative costs of the different operations and how the query should be structured for maximum efficiency.\nWhat we would really like is for VQL to evaluate the cheap condst, and only for those files smaller than 1000 bytes, evaluate the grep() condition. This allows us to eliminate most files immediately (since most files are larger than 1000 bytes) such that we only bother to grep() very few files.\nThis can be achieved by splitting the query into two and chaining them together:\nLET file = select * from glob(globs=\u0026quot;/bin/*\u0026quot;) WHERE Size \u0026lt; 1000 SELECT FullPath from file WHERE grep( path=FullPath, keywords=[\u0026quot;foobar\u0026quot;])  The LET keyword allows us to define a \u0026quot;stored query\u0026quot;. A Stored Query is a query which is assigned into a variable name - you can think of the statement as running the entire query and storing the output into a single variable.\nThe second query then takes the result of this query and applies further transformations and filtering on it. By ensuring that the cheap conditions are evaluated in the stored query, we can ensure that the number of rows stored in the LET expression is smaller than the total number of rows produced by the glob() plugin, and therefore the grep() function will be applied on few rows.\nNote\nYou can think of stored queries as running in multiple steps: First the LET query is executed, then all its rows are stored in the files variable, while the second query reads each row and applies its own filtering on it. In reality though, the LET query is lazy in its evaluation and will only produce results when required. Velociraptor does not store the entire result table of the LET query in memory at once! It is quite safe therefore to run a very large query in the LET clause without fear of memory overrun.\n  Escaping parameters VQL queries often need to take user input. For example consider the query:\nSELECT FullPath from glob(globs=\u0026quot;/bin/*\u0026quot;)  We might want to allow the user to specify the glob expression and create the query programmatically. While it is possible to ensure user input is escaped this is inefficient and tedious.\nVQL queries have an \u0026quot;Environment\u0026quot;. The Environment is essentially the evaluation scope of the query - in other words it contains all the values which can be accessed by name. For example when we call a VQL function like timestamp(), it is placed in the evaluation scope. It is possible to place anything in the environment (or the evaluation scope) and in particular, user parameters can also be placed there. In this case there is no need to escape user input as it is treated as a part of the environment and not the query. For example placing PATH=\u0026quot;/bin/*\u0026quot; into the environment, will allow the following query to run successfully:\nSELECT FullPath from glob(globs=PATH)  You should always try to write VQL queries referring to parameters in the environment because this makes them reusable - the scope parameters become inputs to your query and the query becomes a reusable function.\n "
},
{
        "uri": "/blog/html/2018/08/10/introducing_velociraptor.html",
        "title": "Introducing Velociraptor",
        "tags": [],
        "description": "At Velocidex we have been running open source endpoint monitoring\ntools for our clients in order to detect and respond to incidents.\nThis post introduces a new tool called Velociraptor - an advanced\nDFIR tool.\n",
        "content": "Hunting and responding like a raptor! At Velocidex we have been running open source endpoint monitoring tools for our clients in order to detect and respond to incidents. One of our favorite tools is GRR, developed by Google internally and then released as open source. GRR is a very powerful tool, with a polished UI and good documentation.\nUnfortunately the open source version released by Google suffers from some shortcomings and so we have decided to develop a new project, built on the shoulders of giants called Velociraptor.\nThese are Velociraptor's design goals:\n Focus on data collection. Velociraptor's primary use case is to collect data and export it to other systems. Velociraptor does no analysis itself and therefore has no need for a complex data model. Flexibility - Velociraptor can adapt easily to new requirements without needing to redeploy either clients or servers. Using VQL (Velocidex Query Language) provides flexibility in the type and number of queries that are used to rapidly adapt to changing requirements. VQL allows us to collect just the information needed and no more in an adaptive way. Remove abstractions. Velociraptor aims to be as simple to understand as possible. The default data store simply stores files in the file system which may be easily inspected by the user. No special tooling is required to script or manage Velociraptor. Reduce demand on the data store. Rather than increase the data store requirements, we want to simplify the design to the point that requirements on the data store are so low, one can run a medium to large sized deployment with very few resources (down to perhaps a single server machine). In fact the default data store does not even use a database, but simply uses flat files. Simplify everything! Velociraptor aims to be very simple to run and administer. We remove a lot of the GRR functionality that we dont find we use often. Velociraptor ships as a single, statically linked executable which can perform all actions necessary for deployers.   In short we really wanted something like this:\n Quick Start In this section we go through a typical deployment scenario of Velociraptor.\nDownload the binary Velociraptor ships as a single statically compiled binary. It has no external dependencies and does not require installing as a package. Simply download the binary from the release page for the OS you will be deploying on. The same binary is used for the client and the server.\n Generate a configuration file Velociraptor needs a configuration file to start serving requests. You can generate a new configuration file using the velociraptor config generate command. Note that this prints a new configuration into stdout so you might want to redirect it into a new file (NOTE: The file contains key material so ensure it has appropriate permissions). Below we highlight the parameters you will probably want to change:\n$ velociraptor config generate | tee /etc/velociraptor.config.yaml Client: server_urls: - http://localhost:8000/ writeback_linux: /etc/velociraptor.writeback.yaml writeback_windows: /Program Files/Velociraptor/velociraptor.writeback.yaml Datastore: filestore_directory: /tmp/velociraptor implementation: FileBaseDataStore location: /tmp/velociraptor Frontend: bind_address: 127.0.0.1 bind_port: 8000 GUI: bind_address: 127.0.0.1 bind_port: 8889  The configuration file contains default values for most settings and new keys for cryptographic material so it is expected that you edit the file to customize it for your local deployment. The config file is divided into sections. Here is a quick overview:\nThe Client section is used to configure clients. Server_urls is a list of URLs the client will attempt to connect to. If a connection to one fails it will try the others repeatedly. It is wise to include several URLs here or at least a DNS name so the server may be easily t just use a single IP address here). This will usually be different from Frontend bind address in a proper deployment but for testing it may be the same. Writeback: The writeback path is where clients will write their local persistent state (for example, their private keys). You can specify a different location for windows, osx and linux clients.   The Datastore section specifies where to store the data for the server. Be sure to update the location to a more permanent path. The filestore_location is a path that receives uploaded files which may be large. The Frontend is the server component which receives poll messages from clients. It should be exposed to the internet on a public interface so clients may reach it. This section also contains the server's certificate as signed by the CA (you can rotate server keys using the velociraptor config rotate_keys command. The GUI is the Velociraptor web admin UI. Do not expose this on a public interface without enabling TLS! We recommend to only enable it on the loopback interface and use SSH tunneling to access it over HTTP.server   Client configuration Clients receive a subset of the complete configuration which enables them to connect to the server. You can extract the client's configuration using the velociraptor config client command.\n$ velociraptor --config /etc/velociraptor.config.yaml config client Client: ca_certificate: | -----BEGIN CERTIFICATE----- MIIDIDCCAgigAwIBAgIQEPaF6CPMLOlixEmpgHhvsTANBgkqhkiG9w0BAQsFADAa abIwLMojhIxVFXZOZ0p2ZhYkeKJwNGbiA9rBJR2iKxeJOa0B -----END CERTIFICATE----- nonce: UwtTRfezXIU= server_urls: - http://localhost:8000/ writeback_linux: /etc/velociraptor.writeback.yaml writeback_windows: /Program Files/Velociraptor/velociraptor.writeback.yaml   Running the server Start the server using the frontend command:\n$ velociraptor --config velociraptor.config.yaml frontend INFO:2018/08/08 15:39:09 Launched gRPC API server on 127.0.0.1:8888 INFO:2018/08/08 15:39:09 GUI is ready to handle requests at 127.0.0.1:8889 INFO:2018/08/08 15:39:09 Frontend is ready to handle client requests at 127.0.0.1:8000  You can now verify the server is working by connecting to the GUI with a web browser:\n Running the client The client is run using the s configuration\n$ velociraptor --config /etc/velociraptor.client.config.yaml client velociraptor: error: Unable to load writeback file: open /etc/velociraptor.writeback.yaml: no such file or directory Genering new private key.... Wrote new config file /etc/velocirpator.writeback.yaml INFO:2018/08/08 16:02:22 Starting Crypto for client C.039f18494e6dae95 INFO:2018/08/08 16:02:22 Starting HTTPCommunicator: [http://localhost:8000/] INFO:2018/08/08 16:02:22 Sending unsolicited ping. INFO:2018/08/08 16:02:22 Updated server serial number in config file /etc/velociraptor.writeback.yaml to 1 INFO:2018/08/08 16:02:22 Received PEM for VelociraptorServer from http://localhost:8000/ INFO:2018/08/08 16:02:22 Received response with status: 406 Not Acceptable INFO:2018/08/08 16:02:22 Enrolling INFO:2018/08/08 16:02:23 Received response with status: 406 Not Acceptable INFO:2018/08/08 16:02:25 Sending unsolicited ping. INFO:2018/08/08 16:02:25 Received response with status: 200 OK INFO:2018/08/08 16:02:25 Checking foreman INFO:2018/08/08 16:02:26 Received response with status: 200 OK INFO:2018/08/08 16:02:27 Sending unsolicited ping. INFO:2018/08/08 16:02:27 Received response with status: 200 OK  We can see that when a new client starts for the first time it goes through a number of steps:\nThe writeback file is created with a new client private key (and a client ID). The client communicates with the server but receives a 406 status. This initiates the enrolment flow. The server schedules an Interrogate flow on the client, which issues a number of VQL queries. We can now search for the client using the GUI search box.   Introducing Velociraptor This post introduces Velociraptor - a new end point monitoring and IR tool built upon GRR's groundwork and experience. To be clear, we reused some of GRR's code and some design elements, but Velociraptor is a new project and is largely a rewrite of GRR's codebase. Like GRR, Velociraptor is released under an open source license and is a community project hosted on https://gitlab.com/velocidex/velociraptor.\nIt is still very early days and we would love to receive feedback and suggestions. This is the first technology preview release and we hope to make a more stable and comprehensive release in the coming months. As Velociraptor becomes more battle tested we hope the codebase will stabilize.\nThe near term roadmap is:\nImprove support for more operating systems. Especially Windows: Registry based VQL plugins. NTFS support for raw disk access. Memory scanning and rudimentary Memory analysis Design a more efficient client/server communication mechanism - long polling is problematic since clients only poll infrequently (e.g. every 10 minutes). We want to be able to control all clients quickly. Develop a library of VQL expressions which may be reusable. This should be similar to GRR's idea of Artifacts but be more geared towards VQL.  Please play with it and send feedback to velociraptor-discuss\u0026#64;googlegroups.com\n  "
},
{
        "uri": "/_header/",
        "title": "",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/golang/binparsergen/",
        "title": "Binparsergen",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/ci/",
        "title": "CI",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/discord/",
        "title": "Discord",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/golang/go-ntfs/",
        "title": "Go-NTFS",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/golang/go-pe/",
        "title": "Go-PE",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/golang/go-prefetch/",
        "title": "Go-Prefetch",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/docs/artifacts/linux/linux/",
        "title": "Linux Artifacts",
        "tags": [],
        "description": "Linux Artifacts",
        "content": " Linux.Applications.Chrome.Extensions Fetch Chrome extensions.\nChrome extensions are installed into the user\u0026rsquo;s home directory. We search for manifest.json files in a known path within each system user\u0026rsquo;s home directory. We then parse the manifest file as JSON.\nMany extensions use locale packs to resolve strings like name and description. In this case we detect the default locale and load those locale files. We then resolve the extension\u0026rsquo;s name and description from there.\n   Arg Default Description     extensionGlobs /.config/google-chrome//Extensions//*/manifest.json       View Artifact Source   name: Linux.Applications.Chrome.Extensions description: | Fetch Chrome extensions. Chrome extensions are installed into the user's home directory. We search for manifest.json files in a known path within each system user's home directory. We then parse the manifest file as JSON. Many extensions use locale packs to resolve strings like name and description. In this case we detect the default locale and load those locale files. We then resolve the extension's name and description from there. parameters: - name: extensionGlobs default: /.config/google-chrome/*/Extensions/*/*/manifest.json sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | /* For each user on the system, search for extension manifests in their home directory. */ LET extension_manifests = SELECT * from foreach( row={ SELECT Uid, User, Homedir from Artifact.Linux.Sys.Users() }, query={ SELECT FullPath, Mtime, Ctime, User, Uid from glob( globs=Homedir + '/' + extensionGlobs) }) - | /* If the Manifest declares a default_locale then we load and parse the messages file. In this case the messages are actually stored in the locale file instead of the main manifest.json file. */ LET maybe_read_locale_file = SELECT * from if( condition={ select * from scope() where Manifest.default_locale }, then={ SELECT Manifest, Uid, User, Filename as LocaleFilename, ManifestFilename, parse_json(data=Data) AS LocaleManifest FROM read_file( -- Munge the filename to get the messages.json path. filenames=regex_replace( source=ManifestFilename, replace=\u0026quot;/_locales/\u0026quot; + Manifest.default_locale + \u0026quot;/messages.json\u0026quot;, re=\u0026quot;/manifest.json$\u0026quot;)) }, else={ -- Just fill in empty Locale results. SELECT Manifest, Uid, User, \u0026quot;\u0026quot; AS LocaleFilename, \u0026quot;\u0026quot; AS ManifestFilename, \u0026quot;\u0026quot; AS LocaleManifest FROM scope() }) - | LET parse_json_files = SELECT * from foreach( row={ SELECT Filename as ManifestFilename, Uid, User, parse_json(data=Data) as Manifest FROM read_file(filenames=FullPath) }, query=maybe_read_locale_file) - | LET parsed_manifest_files = SELECT * from foreach( row=extension_manifests, query=parse_json_files) - | SELECT Uid, User, /* If the manifest name contains __MSG_ then the real name is stored in the locale manifest. This condition resolves the Name column either to the main manifest or the locale manifest. */ if(condition=\u0026quot;__MSG_\u0026quot; in Manifest.name, then=get(item=LocaleManifest, member=regex_replace( source=Manifest.name, replace=\u0026quot;$1\u0026quot;, re=\u0026quot;(?:__MSG_(.+)__)\u0026quot;)).message, else=Manifest.name) as Name, if(condition=\u0026quot;__MSG_\u0026quot; in Manifest.description, then=get(item=LocaleManifest, member=regex_replace( source=Manifest.description, replace=\u0026quot;$1\u0026quot;, re=\u0026quot;(?:__MSG_(.+)__)\u0026quot;)).message, else=Manifest.description) as Description, /* Get the Identifier and Version from the manifest filename */ regex_replace( source=ManifestFilename, replace=\u0026quot;$1\u0026quot;, re=\u0026quot;(?:.+Extensions/([^/]+)/([^/]+)/manifest.json)$\u0026quot;) AS Identifier, regex_replace( source=ManifestFilename, replace=\u0026quot;$2\u0026quot;, re=\u0026quot;(?:.+Extensions/([^/]+)/([^/]+)/manifest.json)$\u0026quot;) AS Version, Manifest.author as Author, Manifest.background.persistent AS Persistent, regex_replace( source=ManifestFilename, replace=\u0026quot;$1\u0026quot;, re=\u0026quot;(.+Extensions/.+/)manifest.json$\u0026quot;) AS Path, Manifest.oauth2.scopes as Scopes, Manifest.permissions as Permissions, Manifest.key as Key FROM parsed_manifest_files    Linux.Applications.Chrome.Extensions.Upload Upload all users chrome extension.\nWe dont bother actually parsing anything here, we just grab all the extension files in user\u0026rsquo;s home directory.\n   Arg Default Description     extensionGlobs /.config/google-chrome/*/Extensions/**       View Artifact Source   name: Linux.Applications.Chrome.Extensions.Upload description: | Upload all users chrome extension. We dont bother actually parsing anything here, we just grab all the extension files in user's home directory. parameters: - name: extensionGlobs default: /.config/google-chrome/*/Extensions/** sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | /* For each user on the system, search for extension files in their home directory and upload them. */ SELECT * from foreach( row={ SELECT Uid, User, Homedir from Artifact.Linux.Sys.Users() }, query={ SELECT FullPath, Mtime, Ctime, User, Uid, upload(file=FullPath) as Upload FROM glob(globs=Homedir + '/' + extensionGlobs) })    Linux.Applications.Docker.Info Get Dockers info by connecting to its socket.\n   Arg Default Description     dockerSocket /var/run/docker.sock Docker server socket. You will normally need to be root to connect.\\n      View Artifact Source   name: Linux.Applications.Docker.Info description: Get Dockers info by connecting to its socket. parameters: - name: dockerSocket description: | Docker server socket. You will normally need to be root to connect. default: /var/run/docker.sock sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | LET data = SELECT parse_json(data=Content) as JSON FROM http_client(url=dockerSocket + \u0026quot;:unix/info\u0026quot;) - | SELECT JSON.ID as ID, JSON.Containers as Containers, JSON.ContainersRunning as ContainersRunning, JSON.ContainersPaused as ContainersPaused, JSON.ContainersStopped as ContainersStopped, JSON.Images as Images, JSON.Driver as Driver, JSON.MemoryLimit as MemoryLimit, JSON.SwapLimit as SwapLimit, JSON.KernelMemory as KernelMemory, JSON.CpuCfsPeriod as CpuCfsPeriod, JSON.CpuCfsQuota as CpuCfsQuota, JSON.CPUShares as CPUShares, JSON.CPUSet as CPUSet, JSON.IPv4Forwarding as IPv4Forwarding, JSON.BridgeNfIptables as BridgeNfIptables, JSON.BridgeNfIp6tables as BridgeNfIp6tables, JSON.OomKillDisable as OomKillDisable, JSON.LoggingDriver as LoggingDriver, JSON.CgroupDriver as CgroupDriver, JSON.KernelVersion as KernelVersion, JSON.OperatingSystem as OperatingSystem, JSON.OSType as OSType, JSON.Architecture as Architecture, JSON.NCPU as NCPU, JSON.MemTotal as MemTotal, JSON.HttpProxy as HttpProxy, JSON.HttpsProxy as HttpsProxy, JSON.NoProxy as NoProxy, JSON.Name as Name, JSON.ServerVersion as ServerVersion, JSON.DockerRootDir as DockerRootDir FROM data    Linux.Applications.Docker.Version Get Dockers version by connecting to its socket.\n   Arg Default Description     dockerSocket /var/run/docker.sock Docker server socket. You will normally need to be root to connect.\\n      View Artifact Source   name: Linux.Applications.Docker.Version description: Get Dockers version by connecting to its socket. parameters: - name: dockerSocket description: | Docker server socket. You will normally need to be root to connect. default: /var/run/docker.sock sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | LET data = SELECT parse_json(data=Content) as JSON FROM http_client(url=dockerSocket + \u0026quot;:unix/version\u0026quot;) - | SELECT JSON.Version as Version, JSON.ApiVersion as ApiVersion, JSON.MinAPIVersion as MinAPIVersion, JSON.GitCommit as GitCommit, JSON.GoVersion as GoVersion, JSON.Os as Os, JSON.Arch as Arch, JSON.KernelVersion as KernelVersion, JSON.BuildTime as BuildTime FROM data    Linux.Debian.AptSources Parse Debian apt sources.\nWe first search for *.list files which contain lines of the form\n.. code:: console\ndeb http://us.archive.ubuntu.com/ubuntu/ bionic main restricted\nFor each line we construct the cache file by spliting off the section (last component) and replacing / and \u0026ldquo; \u0026rdquo; with _.\nWe then try to open the file. If the file exists we parse some metadata from it. If not we leave those columns empty.\n   Arg Default Description     linuxAptSourcesGlobs /etc/apt/sources.list,/etc/apt/sources.list.d/*.list Globs to find apt source *.list files.   aptCacheDirectory /var/lib/apt/lists/ Location of the apt cache directory.      View Artifact Source   name: Linux.Debian.AptSources description: | Parse Debian apt sources. We first search for \\*.list files which contain lines of the form .. code:: console deb http://us.archive.ubuntu.com/ubuntu/ bionic main restricted For each line we construct the cache file by spliting off the section (last component) and replacing / and \u0026quot; \u0026quot; with _. We then try to open the file. If the file exists we parse some metadata from it. If not we leave those columns empty. reference: - https://osquery.io/schema/3.2.6#apt_sources parameters: - name: linuxAptSourcesGlobs description: Globs to find apt source *.list files. default: /etc/apt/sources.list,/etc/apt/sources.list.d/*.list - name: aptCacheDirectory description: Location of the apt cache directory. default: /var/lib/apt/lists/ sources: - precondition: SELECT OS From info() where OS = 'linux' queries: - | /* Search for files which may contain apt sources. The user can pass new globs here. */ LET files = SELECT FullPath from glob( globs=split(string=linuxAptSourcesGlobs, sep=\u0026quot;,\u0026quot;)) - | /* Read each line in the sources which is not commented. Deb lines look like: deb [arch=amd64] http://dl.google.com/linux/chrome-remote-desktop/deb/ stable main Contains URL, base_uri and components. */ LET deb_sources = SELECT * FROM parse_records_with_regex( file=files.FullPath, regex=\u0026quot;(?m)^ *(?P\u0026lt;Type\u0026gt;deb(-src)?) (?:\\\\[arch=(?P\u0026lt;Arch\u0026gt;[^\\\\]]+)\\\\] )?\u0026quot; + \u0026quot;(?P\u0026lt;URL\u0026gt;https?://(?P\u0026lt;base_uri\u0026gt;[^ ]+))\u0026quot; + \u0026quot; +(?P\u0026lt;components\u0026gt;.+)\u0026quot;) - | /* We try to get at the Release file in /var/lib/apt/ by munging the components and URL. Strip the last component off, convert / and space to _ and add _Release to get the filename. */ LET parsed_apt_lines = SELECT Arch, URL, base_uri + \u0026quot; \u0026quot; + components as Name, Type, FullPath as Source, aptCacheDirectory + regex_replace( replace=\u0026quot;_\u0026quot;, re=\u0026quot;_+\u0026quot;, source=regex_replace( replace=\u0026quot;_\u0026quot;, re=\u0026quot;[ /]\u0026quot;, source=base_uri + \u0026quot;_dists_\u0026quot; + regex_replace( source=components, replace=\u0026quot;\u0026quot;, re=\u0026quot; +[^ ]+$\u0026quot;)) + \u0026quot;_Release\u0026quot; ) as cache_file FROM deb_sources - | /* This runs if the file was found. Read the entire file into memory and parse the same record using multiple RegExps. */ LET parsed_cache_files = SELECT Name, Arch, URL, Type, Source, parse_string_with_regex( string=Record, regex=[\u0026quot;Codename: (?P\u0026lt;Release\u0026gt;[^\\\\s]+)\u0026quot;, \u0026quot;Version: (?P\u0026lt;Version\u0026gt;[^\\\\s]+)\u0026quot;, \u0026quot;Origin: (?P\u0026lt;Maintainer\u0026gt;[^\\\\s]+)\u0026quot;, \u0026quot;Architectures: (?P\u0026lt;Architectures\u0026gt;[^\\\\s]+)\u0026quot;, \u0026quot;Components: (?P\u0026lt;Components\u0026gt;[^\\\\s]+)\u0026quot;]) as Record FROM parse_records_with_regex(file=cache_file, regex=\u0026quot;(?sm)(?P\u0026lt;Record\u0026gt;.+)\u0026quot;) - | // Foreach row in the parsed cache file, collect the FileInfo too. LET add_stat_to_parsed_cache_file = SELECT * from foreach( query={ SELECT FullPath, Mtime, Ctime, Atime, Record, Type, Name, Arch, URL, Source from stat(filename=cache_file) }, row=parsed_cache_files) - | /* For each row in the parsed file, run the appropriate query depending on if the cache file exists. If the cache file is not found, we just copy the lines we parsed from the source file and fill in empty values for stat. */ LET parse_cache_or_pass = SELECT * from if( condition={ SELECT * from stat(filename=cache_file) }, then=add_stat_to_parsed_cache_file, else={ SELECT Source, Null as Mtime, Null as Ctime, Null as Atime, Type, Null as Record, Arch, URL, Name from scope() }) - | -- For each parsed apt .list file line produce some output. SELECT * from foreach( row=parsed_apt_lines, query=parse_cache_or_pass)    Linux.Debian.Packages Parse dpkg status file.\n   Arg Default Description     linuxDpkgStatus /var/lib/dpkg/status       View Artifact Source   name: Linux.Debian.Packages description: Parse dpkg status file. parameters: - name: linuxDpkgStatus default: /var/lib/dpkg/status sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | /* First pass - split file into records start with Package and end with \\n\\n. Then parse each record using multiple RegExs. */ LET packages = SELECT parse_string_with_regex( string=Record, regex=['Package:\\\\s(?P\u0026lt;Package\u0026gt;.+)', 'Installed-Size:\\\\s(?P\u0026lt;InstalledSize\u0026gt;.+)', 'Version:\\\\s(?P\u0026lt;Version\u0026gt;.+)', 'Source:\\\\s(?P\u0026lt;Source\u0026gt;.+)', 'Architecture:\\\\s(?P\u0026lt;Architecture\u0026gt;.+)']) as Record FROM parse_records_with_regex( file=linuxDpkgStatus, regex='(?sm)^(?P\u0026lt;Record\u0026gt;Package:.+?)\\\\n\\\\n') - | SELECT Record.Package as Package, atoi(string=Record.InstalledSize) as InstalledSize, Record.Version as Version, Record.Source as Source, Record.Architecture as Architecture from packages    Linux.Mounts List mounted filesystems by reading /proc/mounts\n   Arg Default Description     ProcMounts /proc/mounts       View Artifact Source   name: Linux.Mounts description: List mounted filesystems by reading /proc/mounts parameters: - name: ProcMounts default: /proc/mounts sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | SELECT Device, Mount, FSType, split(string=Opts, sep=\u0026quot;,\u0026quot;) As Options FROM parse_records_with_regex( file=ProcMounts, regex='(?m)^(?P\u0026lt;Device\u0026gt;[^ ]+) (?P\u0026lt;Mount\u0026gt;[^ ]+) (?P\u0026lt;FSType\u0026gt;[^ ]+) '+ '(?P\u0026lt;Opts\u0026gt;[^ ]+)')    Linux.Proc.Arp ARP table via /proc/net/arp.\n   Arg Default Description     ProcNetArp /proc/net/arp       View Artifact Source   name: Linux.Proc.Arp description: ARP table via /proc/net/arp. parameters: - name: ProcNetArp default: /proc/net/arp sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | SELECT * from split_records( filenames=ProcNetArp, regex='\\\\s{3,20}', first_row_is_headers=true)    Linux.Proc.Modules Module listing via /proc/modules.\n   Arg Default Description     ProcModules /proc/modules       View Artifact Source   name: Linux.Proc.Modules description: Module listing via /proc/modules. parameters: - name: ProcModules default: /proc/modules sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | SELECT Name, atoi(string=Size) As Size, atoi(string=UseCount) As UseCount, Status, Address FROM split_records( filenames=ProcModules, regex='\\\\s+', columns=['Name', 'Size', 'UseCount', 'UsedBy', 'Status', 'Address'])    Linux.Ssh.AuthorizedKeys Find and parse ssh authorized keys files.\n   Arg Default Description     sshKeyFiles .ssh/authorized_keys*       View Artifact Source   name: Linux.Ssh.AuthorizedKeys description: Find and parse ssh authorized keys files. parameters: - name: sshKeyFiles default: '.ssh/authorized_keys*' sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | // For each user on the system, search for authorized_keys files. LET authorized_keys = SELECT * from foreach( row={ SELECT Uid, User, Homedir from Artifact.Linux.Sys.Users() }, query={ SELECT FullPath, Mtime, Ctime, User, Uid from glob( globs=Homedir + '/' + sshKeyFiles) }) - | // For each authorized keys file, extract each line on a different row. // Note: This duplicates the path, user and uid on each key line. SELECT * from foreach( row=authorized_keys, query={ SELECT Uid, User, FullPath, Key from split_records( filenames=FullPath, regex=\u0026quot;\\n\u0026quot;, columns=[\u0026quot;Key\u0026quot;]) })    Linux.Ssh.KnownHosts Find and parse ssh known hosts files.\n   Arg Default Description     sshKnownHostsFiles .ssh/known_hosts*       View Artifact Source   name: Linux.Ssh.KnownHosts description: Find and parse ssh known hosts files. parameters: - name: sshKnownHostsFiles default: '.ssh/known_hosts*' sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | // For each user on the system, search for known_hosts files. LET authorized_keys = SELECT * from foreach( row={ SELECT Uid, User, Homedir from Artifact.Linux.Sys.Users() }, query={ SELECT FullPath, Mtime, Ctime, User, Uid from glob( globs=Homedir + '/' + sshKnownHostsFiles) }) - | // For each known_hosts file, extract each line on a different row. SELECT * from foreach( row=authorized_keys, query={ SELECT Uid, User, FullPath, Line from split_records( filenames=FullPath, regex=\u0026quot;\\n\u0026quot;, columns=[\u0026quot;Line\u0026quot;]) /* Ignore comment lines. */ WHERE not Line =~ \u0026quot;^[^#]+#\u0026quot; })    Linux.Sys.ACPITables Firmware ACPI functional table common metadata and content.\n   Arg Default Description     kLinuxACPIPath /sys/firmware/acpi/tables       View Artifact Source   name: Linux.Sys.ACPITables description: Firmware ACPI functional table common metadata and content. reference: - https://osquery.io/schema/3.2.6#acpi_tables parameters: - name: kLinuxACPIPath default: /sys/firmware/acpi/tables sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | LET hashes = SELECT Name, Size, hash(path=FullPath) as Hash FROM glob(globs=kLinuxACPIPath + '/*') - | SELECT Name, Size, Hash.MD5, Hash.SHA1, Hash.SHA256 from hashes    Linux.Sys.CPUTime Displays information from /proc/stat file about the time the cpu cores spent in different parts of the system.\n   Arg Default Description     procStat /proc/stat       View Artifact Source   name: Linux.Sys.CPUTime description: | Displays information from /proc/stat file about the time the cpu cores spent in different parts of the system. parameters: - name: procStat default: /proc/stat sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | LET raw = SELECT * FROM split_records( filenames=procStat, regex=' +', columns=['core', 'user', 'nice', 'system', 'idle', 'iowait', 'irq', 'softirq', 'steal', 'guest', 'guest_nice']) WHERE core =~ 'cpu.+' - | SELECT core AS Core, atoi(string=user) as User, atoi(string=nice) as Nice, atoi(string=system) as System, atoi(string=idle) as Idle, atoi(string=iowait) as IOWait, atoi(string=irq) as IRQ, atoi(string=softirq) as SoftIRQ, atoi(string=steal) as Steal, atoi(string=guest) as Guest, atoi(string=guest_nice) as GuestNice FROM raw    Linux.Sys.Crontab Displays parsed information from crontab.\n   Arg Default Description     cronTabGlob /etc/crontab,/etc/cron.d/,/var/at/tabs/,/var/spool/cron/,/var/spool/cron/crontabs/       View Artifact Source   name: Linux.Sys.Crontab description: | Displays parsed information from crontab. parameters: - name: cronTabGlob default: /etc/crontab,/etc/cron.d/**,/var/at/tabs/**,/var/spool/cron/**,/var/spool/cron/crontabs/** sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | LET raw = SELECT * FROM foreach( row={ SELECT FullPath from glob(globs=split(string=cronTabGlob, sep=\u0026quot;,\u0026quot;)) }, query={ SELECT FullPath, data, parse_string_with_regex( string=data, regex=[ /* Regex for event (Starts with @) */ \u0026quot;^(?P\u0026lt;Event\u0026gt;@[a-zA-Z]+)\\\\s+(?P\u0026lt;Command\u0026gt;.+)\u0026quot;, /* Regex for regular command. */ \u0026quot;^(?P\u0026lt;Minute\u0026gt;[^\\\\s]+)\\\\s+\u0026quot;+ \u0026quot;(?P\u0026lt;Hour\u0026gt;[^\\\\s]+)\\\\s+\u0026quot;+ \u0026quot;(?P\u0026lt;DayOfMonth\u0026gt;[^\\\\s]+)\\\\s+\u0026quot;+ \u0026quot;(?P\u0026lt;Month\u0026gt;[^\\\\s]+)\\\\s+\u0026quot;+ \u0026quot;(?P\u0026lt;DayOfWeek\u0026gt;[^\\\\s]+)\\\\s+\u0026quot;+ \u0026quot;(?P\u0026lt;Command\u0026gt;.+)$\u0026quot;]) as Record /* Read lines from the file and filter ones that start with \u0026quot;#\u0026quot; */ FROM split_records( filenames=FullPath, regex=\u0026quot;\\n\u0026quot;, columns=[\u0026quot;data\u0026quot;]) WHERE not data =~ \u0026quot;^\\\\s*#\u0026quot; }) WHERE Record.Command - | SELECT Record.Event AS Event, Record.Minute AS Minute, Record.Hour AS Hour, Record.DayOfMonth AS DayOfMonth, Record.Month AS Month, Record.DayOfWeek AS DayOfWeek, Record.Command AS Command, FullPath AS Path FROM raw    Linux.Sys.LastUserLogin Find and parse system wtmp files. This indicate when the user last logged in.\n   Arg Default Description     wtmpGlobs /var/log/wtmp*    wtmpProfile {\\n \u0026ldquo;timeval\u0026rdquo;: [8, {\\n \u0026ldquo;tv_sec\u0026rdquo;: [0, [\u0026ldquo;int\u0026rdquo;]],\\n \u0026ldquo;tv_usec\u0026rdquo;: [4, [\u0026ldquo;int\u0026rdquo;]]\\n }],\\n \u0026ldquo;exit_status\u0026rdquo;: [4, {\\n \u0026ldquo;e_exit\u0026rdquo;: [2, [\u0026ldquo;short int\u0026rdquo;]],\\n \u0026ldquo;e_termination\u0026rdquo;: [0, [\u0026ldquo;short int\u0026rdquo;]]\\n }],\\n \u0026ldquo;timezone\u0026rdquo;: [8, {\\n \u0026ldquo;tz_dsttime\u0026rdquo;: [4, [\u0026ldquo;int\u0026rdquo;]],\\n \u0026ldquo;tz_minuteswest\u0026rdquo;: [0, [\u0026ldquo;int\u0026rdquo;]]\\n }],\\n \u0026ldquo;utmp\u0026rdquo;: [384, {\\n \u0026ldquo;__glibc_reserved\u0026rdquo;: [364, [\u0026ldquo;Array\u0026rdquo;, {\\n \u0026ldquo;count\u0026rdquo;: 20,\\n \u0026ldquo;target\u0026rdquo;: \u0026ldquo;char\u0026rdquo;,\\n \u0026ldquo;target_args\u0026rdquo;: null\\n }]],\\n \u0026ldquo;ut_addr_v6\u0026rdquo;: [348, [\u0026ldquo;Array\u0026rdquo;, {\\n \u0026ldquo;count\u0026rdquo;: 4,\\n \u0026ldquo;target\u0026rdquo;: \u0026ldquo;int\u0026rdquo;,\\n \u0026ldquo;target_args\u0026rdquo;: null\\n }]],\\n \u0026ldquo;ut_exit\u0026rdquo;: [332, [\u0026ldquo;exit_status\u0026rdquo;]],\\n \u0026ldquo;ut_host\u0026rdquo;: [76, [\u0026ldquo;String\u0026rdquo;, {\\n \u0026ldquo;length\u0026rdquo;: 256\\n }]],\\n \u0026ldquo;ut_id\u0026rdquo;: [40, [\u0026ldquo;String\u0026rdquo;, {\\n \u0026ldquo;length\u0026rdquo;: 4\\n }]],\\n \u0026ldquo;ut_line\u0026rdquo;: [8, [\u0026ldquo;String\u0026rdquo;, {\\n \u0026ldquo;length\u0026rdquo;: 32\\n }]],\\n \u0026ldquo;ut_pid\u0026rdquo;: [4, [\u0026ldquo;int\u0026rdquo;]],\\n \u0026ldquo;ut_session\u0026rdquo;: [336, [\u0026ldquo;int\u0026rdquo;]],\\n \u0026ldquo;ut_tv\u0026rdquo;: [340, [\u0026ldquo;timeval\u0026rdquo;]],\\n \u0026ldquo;ut_type\u0026rdquo;: [0, [\u0026ldquo;Enumeration\u0026rdquo;, {\\n \u0026ldquo;target\u0026rdquo;: \u0026ldquo;short int\u0026rdquo;,\\n \u0026ldquo;choices\u0026rdquo;: {\\n \u0026ldquo;0\u0026rdquo;: \u0026ldquo;EMPTY\u0026rdquo;,\\n \u0026ldquo;1\u0026rdquo;: \u0026ldquo;RUN_LVL\u0026rdquo;,\\n \u0026ldquo;2\u0026rdquo;: \u0026ldquo;BOOT_TIME\u0026rdquo;,\\n \u0026ldquo;5\u0026rdquo;: \u0026ldquo;INIT_PROCESS\u0026rdquo;,\\n \u0026ldquo;6\u0026rdquo;: \u0026ldquo;LOGIN_PROCESS\u0026rdquo;,\\n \u0026ldquo;7\u0026rdquo;: \u0026ldquo;USER_PROCESS\u0026rdquo;,\\n \u0026ldquo;8\u0026rdquo;: \u0026ldquo;DEAD_PROCESS\u0026rdquo;\\n }\\n }]],\\n \u0026ldquo;ut_user\u0026rdquo;: [44, [\u0026ldquo;String\u0026rdquo;, {\\n \u0026ldquo;length\u0026rdquo;: 32\\n }]]\\n }]\\n}\\n       View Artifact Source   name: Linux.Sys.LastUserLogin description: Find and parse system wtmp files. This indicate when the user last logged in. parameters: - name: wtmpGlobs default: /var/log/wtmp* # This is automatically generated from dwarf symbols by Rekall: # gcc -c -g -o /tmp/test.o /tmp/1.c # rekall dwarfparser /tmp/test.o # And 1.c is: # #include \u0026quot;utmp.h\u0026quot; # struct utmp x; - name: wtmpProfile default: | { \u0026quot;timeval\u0026quot;: [8, { \u0026quot;tv_sec\u0026quot;: [0, [\u0026quot;int\u0026quot;]], \u0026quot;tv_usec\u0026quot;: [4, [\u0026quot;int\u0026quot;]] }], \u0026quot;exit_status\u0026quot;: [4, { \u0026quot;e_exit\u0026quot;: [2, [\u0026quot;short int\u0026quot;]], \u0026quot;e_termination\u0026quot;: [0, [\u0026quot;short int\u0026quot;]] }], \u0026quot;timezone\u0026quot;: [8, { \u0026quot;tz_dsttime\u0026quot;: [4, [\u0026quot;int\u0026quot;]], \u0026quot;tz_minuteswest\u0026quot;: [0, [\u0026quot;int\u0026quot;]] }], \u0026quot;utmp\u0026quot;: [384, { \u0026quot;__glibc_reserved\u0026quot;: [364, [\u0026quot;Array\u0026quot;, { \u0026quot;count\u0026quot;: 20, \u0026quot;target\u0026quot;: \u0026quot;char\u0026quot;, \u0026quot;target_args\u0026quot;: null }]], \u0026quot;ut_addr_v6\u0026quot;: [348, [\u0026quot;Array\u0026quot;, { \u0026quot;count\u0026quot;: 4, \u0026quot;target\u0026quot;: \u0026quot;int\u0026quot;, \u0026quot;target_args\u0026quot;: null }]], \u0026quot;ut_exit\u0026quot;: [332, [\u0026quot;exit_status\u0026quot;]], \u0026quot;ut_host\u0026quot;: [76, [\u0026quot;String\u0026quot;, { \u0026quot;length\u0026quot;: 256 }]], \u0026quot;ut_id\u0026quot;: [40, [\u0026quot;String\u0026quot;, { \u0026quot;length\u0026quot;: 4 }]], \u0026quot;ut_line\u0026quot;: [8, [\u0026quot;String\u0026quot;, { \u0026quot;length\u0026quot;: 32 }]], \u0026quot;ut_pid\u0026quot;: [4, [\u0026quot;int\u0026quot;]], \u0026quot;ut_session\u0026quot;: [336, [\u0026quot;int\u0026quot;]], \u0026quot;ut_tv\u0026quot;: [340, [\u0026quot;timeval\u0026quot;]], \u0026quot;ut_type\u0026quot;: [0, [\u0026quot;Enumeration\u0026quot;, { \u0026quot;target\u0026quot;: \u0026quot;short int\u0026quot;, \u0026quot;choices\u0026quot;: { \u0026quot;0\u0026quot;: \u0026quot;EMPTY\u0026quot;, \u0026quot;1\u0026quot;: \u0026quot;RUN_LVL\u0026quot;, \u0026quot;2\u0026quot;: \u0026quot;BOOT_TIME\u0026quot;, \u0026quot;5\u0026quot;: \u0026quot;INIT_PROCESS\u0026quot;, \u0026quot;6\u0026quot;: \u0026quot;LOGIN_PROCESS\u0026quot;, \u0026quot;7\u0026quot;: \u0026quot;USER_PROCESS\u0026quot;, \u0026quot;8\u0026quot;: \u0026quot;DEAD_PROCESS\u0026quot; } }]], \u0026quot;ut_user\u0026quot;: [44, [\u0026quot;String\u0026quot;, { \u0026quot;length\u0026quot;: 32 }]] }] } sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | SELECT * from foreach( row={ SELECT FullPath from glob(globs=split(string=wtmpGlobs, sep=\u0026quot;,\u0026quot;)) }, query={ SELECT ut_type, ut_id, ut_host.AsString as Host, ut_user.AsString as User, timestamp(epoch=ut_tv.tv_sec.AsInteger) as login_time FROM binary_parse( file=FullPath, profile=wtmpProfile, target=\u0026quot;Array\u0026quot;, args=dict(Target=\u0026quot;utmp\u0026quot;) ) })    Linux.Sys.Users Get User specific information like homedir, group etc from /etc/passwd.\n   Arg Default Description     PasswordFile /etc/passwd The location of the password file.      View Artifact Source   name: Linux.Sys.Users description: Get User specific information like homedir, group etc from /etc/passwd. parameters: - name: PasswordFile default: /etc/passwd description: The location of the password file. sources: - precondition: | SELECT OS From info() where OS = 'linux' queries: - | SELECT User, Description, Uid, Gid, Homedir, Shell FROM parse_records_with_regex( file=PasswordFile, regex='(?m)^(?P\u0026lt;User\u0026gt;[^:]+):([^:]+):' + '(?P\u0026lt;Uid\u0026gt;[^:]+):(?P\u0026lt;Gid\u0026gt;[^:]+):(?P\u0026lt;Description\u0026gt;[^:]*):' + '(?P\u0026lt;Homedir\u0026gt;[^:]+):(?P\u0026lt;Shell\u0026gt;[^:\\\\s]+)')    "
},
{
        "uri": "/golang/oleparser/",
        "title": "OleParser",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/golang/",
        "title": "Other Velocidex Projects",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/golang/regparser/",
        "title": "Regparser",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/tags/",
        "title": "Tags",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/golang/timetracker/",
        "title": "Timetracker",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/docs/getting-started/triaging/",
        "title": "Triaging",
        "tags": [],
        "description": "Velociraptor can also be used as a triage collection tool.",
        "content": " We can use Velociraptor to hunt for many artifacts across the network. However ultimately, the Velociraptor agent simply runs VQL to collect its artifacts. What if we can just collect the artifacts interactive?\nWe certainly can do this!\nTriaging a system. When triaging a system our goal is to collect and preserve as much data from the system as possible, as quickly as possible.\nFirst lets see what artifacts come built in with Velociraptor:\n$ velociraptor artifact list Admin.Client.Upgrade Admin.Events.PostProcessUploads Admin.System.CompressUploads Demo.Plugins.Fifo Generic.Applications.Office.Keywords ...  Now we just select which artifact to collect and specify an output zip file to store the results to:\nF:\\\u0026gt;velociraptor.exe artifacts collect -v Windows.Triage.WebBrowsers --output f:\\output\\test.zip [INFO] 2019-04-01T03:49:38-07:00 Loaded 99 built in artifacts [INFO] 2019-04-01T03:49:38-07:00 Collecting file \\C:\\Users\\test\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Cookies [INFO] 2019-04-01T03:49:38-07:00 Collecting file \\C:\\Users\\test\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Cookies-journal [INFO] 2019-04-01T03:49:38-07:00 Collecting file \\C:\\Users\\test\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Current Session ...  "
},
{
        "uri": "/golang/velociraptor/",
        "title": "Velociraptor",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/contact/",
        "title": "Velociraptor - Contact Us",
        "tags": [],
        "description": "Contact Us page.",
        "content": ""
},
{
        "uri": "/faq/",
        "title": "Velociraptor / Dig deeper",
        "tags": [],
        "description": "Velociraptor's homepage",
        "content": ""
},
{
        "uri": "/golang/vfilter/",
        "title": "Vfilter",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/golang/vtypes/",
        "title": "Vtypes",
        "tags": [],
        "description": "",
        "content": ""
},
{
        "uri": "/categories/getting-started/",
        "title": "getting started",
        "tags": [],
        "description": "",
        "content": ""
}]
